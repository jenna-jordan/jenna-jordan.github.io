<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jennajordan.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jennajordan.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-16T00:26:27+00:00</updated><id>https://jennajordan.me/feed.xml</id><title type="html">blank</title><subtitle>I do data stuff with python + SQL. Learned to code at library school. Knitter. Dog mom. Nerdy for sci-fi/fantasy, board games, and well-wrangled data. </subtitle><entry><title type="html">Analytical Data Warehouses - an introduction</title><link href="https://jennajordan.me/blog/analytical-data-warehouses" rel="alternate" type="text/html" title="Analytical Data Warehouses - an introduction"/><published>2023-08-13T00:00:00+00:00</published><updated>2023-08-13T00:00:00+00:00</updated><id>https://jennajordan.me/blog/analytical-data-warehouses</id><content type="html" xml:base="https://jennajordan.me/blog/analytical-data-warehouses"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_data_warehouses-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_data_warehouses-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_data_warehouses-1400.webp"/> <img src="/assets/img/header_data_warehouses.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When I first started working on the City of Boston Analytics Team, I had never really worked with an analytical data warehouse before. I had experience with databases, and with the data flow in analytics projects (mostly using pandas, in jupyter notebooks), and I had used databases as a starting point for analytics projects, but I had not yet put all of those pieces together. Then I started working as a data engineer whose primary role was to operate within an analytical data warehouse, and I took a 4-week crash course in dbt… and my brain went into overdrive trying to sythesize everything I already knew and was learning in order to come up with a mental model of what an analytical data warehouse was, how it should operate, and why. Now, almost two years later, I’m pretty sure I get it - and I want to share how I conceptualize a data warehouse in hopes that it makes the journey easier for anyone else who may be in the same position I was.</p> <p>But first, a couple caveats:</p> <ol> <li>I’m still learning, and I know my understanding will continue to evolve as I continue to learn</li> <li>This is largely influenced by how my team uses our data warehouse, and it may be different for other teams</li> </ol> <h2 id="databases-vs-data-warehouses-transactional-vs-analytical-the-what-and-why">Databases vs data warehouses, transactional vs analytical (the what, and why)</h2> <p>An analytical data warehouse is the data source that powers analytical outputs - dashboards, reports, one-off statistical analyses, machine learning models, etc. Having a data warehouse makes sense when you are working in a team - you often need to share data, pick up other people’s projects, and know that an analytical output will continue to function even after a project is wrapped up. Having a reliable, managed, and shared infrastructure is a huge productivity boost. Also, pretty much every BI tool is built to at least connect to a database.</p> <p>But what makes a data warehouse different from a (relational) database? In large part, this is simply a matter of semantics. Any database can be a data warehouse, it all comes down to how you use it. If you are using a database for analytics, you are likely doing a lot more and more complicated queries to read from the database. If you are using a database to power an application, you are doing a lot more (and need faster) writes to the database. Transactional databases are optimized for writes, and analytical databases are optimized for reads. Some DBMSs are balanced and can be used for both (i.e. PostgreSQL). Others optimize heavily for one use case (Snowflake is pretty much only used for analytics). How you are using a database also dictates how you organize it. If you are using a transactional database to power an application, there is a very good chance your database is normalized (most typically into 3rd or Boyce-Codd Normal Form).</p> <p>Normalizing your data is the best way to organize your data when you want to reduce the duplication of data, avoid data anomolies for inserts/deletes/updates, and ensure relational integrity. But the normalized structure can make it difficult to answer seemingly simple questions - you often have to write complex queries with many joins. And analytics is all about asking questions of your data.</p> <p>If you are using your database to answer many different analytical questions, you probably have many different data sources - and many of those data sources may be completely different from each other and never intended to be joined together. This is when a database starts to become a data warehouse - rather than one cohesive place to store data for one purpose (and therefore the data is all related), the data warehouse is more like a data repository. It is stored in a single place because of the convenience, not because the data is necessarily related (though some if it certainly will be).</p> <p>In a well-designed relational database, it is usually pretty easy to understand how data is organized and linked through primary and foreign keys. You don’t necessarily need to know the data to query it so long as you know the structure. But an analytical data warehouse is not so easy to comprehend. There are almost never foreign keys, because they introduce complications to the ETL processes that update the data. If you are lucky, there are primary keys on some of the tables. To understand how an analytical data warehouse is organized, you instead need to look at the transformation flow.</p> <h2 id="the-transformation-flow-the-how-and-why">The transformation flow (the how, and why)</h2> <p>If you have worked on enough code-based analytics projects, you have probably learned the value of maintaining a proper data transformation flow. This may show up in how you organize your data folder: you have your “raw” data that is never to be altered, your cleaned/processed data that is an intermediate output, and your final data. You learn the value of using code to transform data - it means all transformations are documented, replicable, and easy to alter. Your notebooks/scripts are a mix of exploration, small cleaning tasks, and big transformations. You may have some functions in there for transformations that need to be performed repeatedly or to encapsulate more complicated code. You are displaying chunks of dataframes in order to visually examine the results and decide what needs to be done next. You are also saving dataframes to CSVs to be used elsewhere.</p> <p>All of these elements show up in the analytical data warehouse as well - they just look a bit different. No matter where the data lives, analytics data work should follow a set of core principles that cement data management best practices in the analytical output.</p> <h3 id="principles-of-data-management">Principles of data management</h3> <ol> <li>Preserved raw data: raw data should be a direct copy of the source as much as possible, and should not be altered (besides being updated with new data)</li> <li>Replicable transformations: all transformations of the raw data should be replicable/documented (the SQL/python code is saved, preferably in a git-tracked central repository) and automated if needed (transformations performed on a schedule in line with data imports)</li> <li>Tested production data: all data considered “production” (ready for analysis) should be able to pass tests of data quality. These tests should be documented and automated if needed, like the transforms</li> </ol> <h3 id="preserved-raw-data">Preserved raw data</h3> <p>When raw data is loaded into a data warehouse via an ETL process, it is loaded in a “source” table. Different teams have different names for this set of tables - “base”, “staging”, etc - but I will use “source” for the sake of simplicity/consistency. These source tables are often isolated or otherwise identifiable in the data warehouse. Some teams have a separate database only for source data, others have a separate schema or set of schemas for source tables, and some may use a naming convention to identify these tables (e.g. a suffix of “_stg”).</p> <p>These source tables reflect the actual source data as closely as possible. This means that they may have the wrong datatypes, need to be cleaned (e.g. trimming whitespace), split out into multiple columns (especially if it is JSON text), the column names may need to be changed, or some other transformation is needed before it can be considered “production ready”. But it is important that these raw source tables remain exactly the same as they were when first loaded into the database - you want to avoid alter tables statements.</p> <p>Sometimes, these raw sources may come from a relational database. In this case, you may want to implement primary keys on the source tables - because if the primary key constraint is violated, that means something went wrong in the ETL process. However, implementing foreign keys is generally not recommended, as it can severely complicate the ETL process. Instead, document these foreign keys (in tests, in text, etc) and use them to inform downstream transformations.</p> <h3 id="replicable-transformations">Replicable transformations</h3> <p>The next step is to transform these source tables into “production” tables. There may be many transformations before producing the table that will be used for analysis, but there should always be at least one. In a data warehouse, a “transform” usually means a SQL select query that is used to create either a table or a view. For the purpose of this post, “production” refers to tables or views in the database that are ready to be used for any downstream analytical purposes (dashboards, analysis, further downstream views, etc), have been through a development and approval process, and have been designated as being “production” based on its name, schema, database, or some other indication.</p> <p>The first transform is the simplist - it should select from the source table and apply any basic cleanup that is needed. Let’s call the result of this first transformation the “base production” table. It should correspond to the source table closely, and should have a 1:1 relationship to the source table (so, no joins). It should also always be materalized as a table (not as a view). The base production tables - and all downstream transforms - should live in a separate schema/database from the source tables, or otherwise be easily distinguished from the source tables.</p> <p>Even if no cleanup or alteration from the base table is needed, this first transformation is still a necessary step. Why? Sometimes there is an error in the ETL process, and the source table may be wiped. If you only have one table (the source table), then you have lost that data, and it will effect any downstream uses on that table. However, if you have a base table (which selects from the source table), and the source table is wiped, then you can stop the transformation before the corresponding base production table is wiped. So, the base production table still has data (even if it is outdated - which is preferable to no data at all). Note: this is only possible if the base production table is materialized as a table, not a view.</p> <p>Even if a source is static (not being updated by an ETL process), you still want to have this source -&gt; base production table transform, and preserve both this separation and the tranformation applied. Why? You may discover at a later point that the base production table needs to be altered in some way - a column should be renamed, a data type needs to be corrected, whitespace needs to be removed, etc. Or, you may discover that a transform you thought you needed is actually incorrect, and you need to alter the transform. Once you alter the source table, there is no going back (besides dropping the table and re-creating it from the source). Whereas if you have a transformation query, you can simply re-run the query to re-create the altered production table. This is only possible if the raw data in the source table remains the same - and if the SQL used to do the transform is saved (preferably in a git-tracked repo).</p> <h4 id="further-transformations">Further transformations</h4> <p>While base production tables should be materialized as a table and should have a 1:1 relationship with its source table, all further downstream transformations can be materialized as views and query multiple tables. The primary reason that a downstream transformation would be materialized as a table instead of a view is for performance reasons - if the query is computationally expensive and takes a long time to run. Otherwise, views are preferred.</p> <p>Note: further downstream transformations should never query a source table - only the production tables/views. These tables/views generally live in the same schemas/databases as the base production tables.</p> <p>There may be many, many downstream transformations for a particular data source, and the structure of these can vary widely. There are many different design philophies around this area of the data warehouse, but I will leave that for another post. Suffice to say, a large portion of the “thinking” work in a data warehouse is in writing these downstream transforms.</p> <h3 id="tested-production-data">Tested production data</h3> <p>When new data is imported and transformed on a regular cadence, it is important to have Data Unit Tests (DUTs) to automatically check that previous assumptions about the data still hold true. DUTs can be performed on any table/view in the data warehouse - source, production, or downstream views. For example, you might want to check that the source table contains some minimum number of rows before doing the transform to production - and if the test fails, then the transform will not happen. You may want to check that a field you assume is your primary key is actually unique and has no null values. You may want to check that a categorical field only has a limited list of values - and if it fails the test, you only want it to warn you, but not prevent the transformation. The most important tests (the ones you want to stop downstream transforms if they fail) should be performed as early in the transformation flow as possible (ideally, the source tables), while the nice-to-have tests that should only warn you about failures (and not prevent transformations) can be implemented further downstream.</p> <p>DUTs are a way to productionize data testing, but can also be useful during the exploratory phase when first designing transformations. You can test your data through a formal DUT structure, or through a series of queries. For example, you may want to <code class="language-plaintext highlighter-rouge">select distinct</code> on a field to see all possible values, or <code class="language-plaintext highlighter-rouge">select count(distinct __)</code> on a field compared to <code class="language-plaintext highlighter-rouge">select count(__)</code> to see whether there are duplicates. Based on this exploratory data testing, you may wish to revise your transform. And while it is important to document and preserve any SQL transform queries so they can be re-run, it is less essential to preserve these ad-hoc data tests - their primary purpose for static data is to inform your SQL query for transforming source data into the production table. However, if the data source may be updated, then formalizing them into DUTs is a good idea.</p> <h2 id="the-implications-or-why-dbt">The implications (or, why dbt)</h2> <p>Imagine you are an analyst, staring at this data warehouse and all of its many tables, trying to figure out which tables are going to have the answer to your question. Or, imagine you are an engineer, informed that something has gone wrong in a dashboard and you need to figure out what (and where) in the data flow something went wrong. What is the one thing that you are really going to want (need!) in order to do your job effectively and efficiently?</p> <p>Documentation.</p> <p>You are going to want documentation for your tables, the columns in those tables, the relationships and dependencies between those tables, the tests performed on those tables, and any external dependencies (dashboards relying on those tables, ETL processes powering the source tables). But documentation has a dirty secret - (almost) nobody wants to or has time to write documentation. Unless you make documentation fast and easy to produce (or you pay for it by making documentation someone’s job or primary responsiblity), it isn’t going to happen (at least not consistently).</p> <p>dbt may be advertised as a tool to transform (and test!) your data, but its real superpower is in the documentation that is produced as a side effect of how those transformations and test are implemented. If you do not currently have an infrastructure to support transformations and data unit tests within your data warehouse, then implementing dbt is a no brainer because it is the easiest out-of-the-box way to accomplish those fundamental tasks. But even if you do have a (likely custom/house built) infrastructure to do those transforms &amp; tests, making the switch to dbt is worth it, because of the documentation that is produced (and the culture of documentation that is encouraged/supported). Once you see a DAG (directed acyclic graph of table nodes) that visualizes every step from source to dashboard, there is no going back.</p> <p>Why? Because given enough time, your team will accumulate enough data sources (and the people most knowledgable about those sources will leave at some point), and enough complicated transforms and table structures, that working within the data warehouse will become a snakes nest of tangled dependencies, table rot, and dangerous assumptions. And if you can’t rely on your data warehouse, then your stakeholders can’t rely on your analytical outputs. Documentation is not just a nice to have - its essential. Document your data, your transforms, your tests, your analytical outputs, everything - and then put that documentation in one easily accessible and searchable place. dbt makes this easy, and that is the real reason it has become the darling of the data community - and the reason I’ve wanted to implement it for our team ever since I learned enough about the tool and our current data infrastructure.</p> <p>Which bring us back to our principles of good data management for analytics. There was, in fact, one principle missing from the list, so let’s round it out.</p> <h3 id="principles-of-data-management-final-version">Principles of data management (final version)</h3> <ol> <li>Preserved raw data: raw data should be a direct copy of the source as much as possible, and should not be altered (besides being updated with new data)</li> <li>Replicable transformations: all transformations of the raw data should be replicable/documented (the SQL/python code is saved, preferably in a git-tracked central repository) and automated if needed (transformations performed on a schedule in line with data imports)</li> <li>Tested production data: all data considered “production” (ready for analysis) should be able to pass tests of data quality. These tests should be documented and automated if needed, like the transforms</li> <li>Documented data: the data (each column, each table) should be documented, as well as the data lineage (how the final data was produced from the raw source data)</li> </ol> <h2 id="bonus-the-development-cycle">Bonus: the development cycle</h2> <p>Now that we’ve brought dbt into the picture, we can talk about one final piece of the (modern) analytical data warehouse: the development cycle. In the principles above (and the blocks of text further above), note the use of the term “production”. The presence of “production” data implies the presence of data that is not “production” - something distinct from the raw source data already described. This something missing is the “development” version of the data - basically, the data/transform that has not yet been finalized.</p> <p>Tables that are still in development should be distinguished from production tables in the data warehouse. Dev tables may live in a separate database or schema, or be indicated through a table naming convention. Teams may also have less formal (read: social) ways of indicating that a table is in development, though this introduces opportunity for error. A table in development is not finalized - column names, order, and content may change, the table name itself may change, and the logic has not yet been reviewed and approved by the team. Essentially, it’s very risky (and certainly inefficient) to build downstream dependencies on dev tables, though there will always be cases where speedy delivery is a necessity and this rule has to be broken.</p> <p>One great feature of dbt is that this development phase of tables/views (dbt refers to them all as “models”) is built into how dbt works through the use of profiles. It’s very easy to designate a specific schema or database as a dev schema/database, and to build models agnostic of the dev/prod schema. It’s also why git is a core component of any dbt project - the “main” branch should have models in “production”, while feature branches should have models in “development”. You have to be familiar with the use of version control in a software development cycle in order to use dbt effectively, and the fact that dbt brings these practices into the modern analytical workflow is considered a major selling point of dbt.</p> <h2 id="in-conclusion">In conclusion</h2> <p>Unless you are already part of a data team that uses a data warehouse, it can be hard to understand what an analytical data warehouse is, how to use it properly, and why it is such an important part of working on a data team. If you’re in school or otherwise studying for a career in data, knowing how to work within a data warehouse will give you a big leg up over other entry level candidates. My hope is that this blog post can (1) convince you that you already know the most important pieces of the puzzle, (2) fill in some of the missing pieces of the puzzle, and (3) help you synthesize and put all of the pieces together to form a cohesive picture.</p> <p>In future posts I would like to continue to flesh out how to work in a data warehouse, but this concludes the introduction. If you have any questions or future topics you’d like me to focus on, let me know!</p>]]></content><author><name></name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[what? how? why? data warehousing for analytics, explained (mostly)]]></summary></entry><entry><title type="html">Life Since Grad School</title><link href="https://jennajordan.me/blog/life-since-grad-school" rel="alternate" type="text/html" title="Life Since Grad School"/><published>2023-07-30T00:00:00+00:00</published><updated>2023-07-30T00:00:00+00:00</updated><id>https://jennajordan.me/blog/life-since-grad-school</id><content type="html" xml:base="https://jennajordan.me/blog/life-since-grad-school"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_lifesincegradschool-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_lifesincegradschool-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_lifesincegradschool-1400.webp"/> <img src="/assets/img/header_lifesincegradschool.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This website, and therefore this blog, has fallen by the wayside since I graduated from library school with my MSLIS in the spring of 2020… in the midst of a global pandemic. I wasn’t really sure what to do with the site, beyond letting it exist and adding some projects here and there, but I recently decided that it was time for a revamp (and rebrand/redesign). I wanted my personal website to say “working professional”, not “overly enthusiastic grad student” - though I am keeping all the old posts so the “overly enthusiastic grad student” is not going away. But it has been 3 years (wow!) since I was in grad school, and I have managed to get some professional experience under my belt - though with the pandemic raging as I entered the work force I had a bit of a rocky start.</p> <h2 id="2020">2020</h2> <p>My last semester of grad school was not the best. There was before spring break, and there was after spring break. Before spring break, everything was continuing on as usual, with some masks starting to appear. After spring break, everything shut down. All classes and work went remote. There was no graduation ceremony. I had no idea that the last time I saw my professors, classmates, coworkers, and firends, it would be the last time I saw them ever. While in undergrad I could not care less about the graduation ceremony, for grad school - a place where I really found my academic home - I was looking forward to the chance to see everyone one last time and say goodbye. Instead, there was just endless isolation.</p> <p>After graduating into a pandemic with no job offer yet in hand, I did what everyone seemed to be doing - I moved back home. My mom was living alone, I couldn’t afford to pay rent without a job, and after the isolation of the previous months I was looking forward to forming a little family bubble. I put most of my stuff in storage, sure that I would be moving soon to wherever I found a job. And sure enough, within a few months of graduating I got a job offer… with one big catch. I would need to get my security clearance. I was so excited to get a job offer, I ignored the perfectly reasonable advice every entry level professional is given (and ignores): don’t accept the first offer you get. I was told that the process would take about 6-12 months, and I hoped that my previous security clearance (for my internship with the State Department in undergrad) would help speed things up. In the meantime, I could find some short-term contracting gigs to start advancing my data skills and developing experience. So that’s exactly what I did.</p> <p>Pretty much as soon as I had some income coming in, I kept a promise that I had made to myself years ago at the end of undergrad (when I knew I was going to be doing a lot of traveling). I finally adopted a puppy. I adopted my sweet girl from Saving Grace, a rescue in Raleigh. At the rescue, the name on her tag was “Genuine Risk”. I named her <a href="/bella/">Bella</a>, after the loyal pony Bela in Wheel of Time. She was about 14 weeks old, the last of her litter, and shy as could be, but after 5 minutes of me sitting cross legged in her pen she crawled into my lap and fell asleep. That’s when I knew - and my mom really knew - that she would be coming home with us.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bella_blanket-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bella_blanket-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bella_blanket-1400.webp"/> <img src="/assets/img/bella_blanket.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I also took up knitting (I know, such a classic pandemic hobby). The very first thing I knit was a blanket for Bella. My reasoning was: (1) she won’t care what it looks like, so it’s fine if it turns out awful and ugly, and (2) it’s getting cold, she’d probably appreciate a blanket that smells like me. (Actually I lie, the first 4 blanket squares got combined into a blanket for Molly, our much smaller and much older family dog). Sadly, that blanket no longer exists - it got slowly torn apart and consumed because… I gave it to a puppy. C’est la vie. But while the blanket slowly got smaller, I kept on <a href="/knitting/">knitting</a>, and eventually started knitting shawls as wedding presents. (6 shawls and 5 weddings later, I’m finally now starting on a shawl for myself)</p> <h2 id="2021">2021</h2> <p>A year came… and went. Still no word on the security clearance. I had done all of the steps (including going in person for the drug test, polygraph, etc) and I got no updates, no estimated timeline, nothing. I knew that I wanted to kick off my career properly - in a full-time position doing data work as a part of a larger team focused on public service - and the contracting gigs had only been meant as a temporary stopgap until I could join such a team. I had been applying for data librarian jobs and even got to the final round of interviews a couple times, but no dice (actually, <a href="/projects/carpentries-dataviz-workshop">my Carpentries lesson</a> was a side product of one of those interviews!).</p> <p>Finally, in late fall of 2021, I hit gold. A mutual on twitter pointed me towards a posting for a data engineer at the City of Boston’s Analytics Team. It was perfect - data work, in public service, and a genuine way to really start my career. I applied, made a <a href="/projects/streamlit-apps">streamlit app</a> for extra credit, and by the end of the year I had accepted a job offer.</p> <p>I kid you not, a week after accepting that job offer, my security clearance came through. But I knew that the data engineer position in Boston was a better fit for me, so I stuck with it and moved to Boston in January of 2022. Almost a year and a half of waiting on that security clearance… well, sometimes, timing really is everything.</p> <h2 id="2022">2022</h2> <p>My first year as a Data Engineer on the Analytics Team was a true learning experience - exactly what I was hoping for from my first job to kick off this new career. In grad school, I had learned SQL and how to design databases in 3rd Normal Form. At my job, I learned about analytical data warehouses and how SQL was used in production code, and I leveled up my SQL skills substantially (can you believe we learned about subqueries but not CTEs?). In grad school, I had learned about automated workflows as related to improving data quality. At my job, I learned about data orchestration (hello, YAML files and YAQL) and the pros and cons of different workflow design paradigms. In grad school, I mostly wrote my own python code from scratch to finish. At my job, I learned how to work with a team of engineers and contribute to an existing codebase. I learned how to work and contribute within a larger analytics team - rather than doing every part of the analytics flow myself.</p> <p>I also made sure to focus on my professional development in other ways. In March I took CoRise’s course <a href="https://corise.com/course/analytics-engineering-with-dbt">“Analytics Engineering with dbt”</a>, taught by Emily Hawkins. In August I took GovEx’s course on Data Governance. Both were valuable and what I learned in them I could immediately apply in my work.</p> <p>During my first year, I focused on learning how the Analytics Team, and the Engineering Team especially, worked, and how I could best contribute within the existing framework. I learned so much from my coworkers, and I found the experience of having fellow engineers that I could lean on and collaborate with invaluable. But in addition to learning how to contribute to what was already there, I was also starting to identify new ideas that I wanted to add to the mix. Primarily: <a href="https://docs.getdbt.com/docs/introduction">dbt</a>.</p> <h2 id="2023">2023</h2> <p>For my second year on the Analytics Team, I wanted to make a substantial impact to improve how the engineering team worked. This involved a lot of thinking and iterating through designs and strategies before I brought my proposals to the team. I wanted to make sure that when it came to the actual implementation of these plans, that it would go as quickly and smoothly as possible. It also meant communicating with my team and building interest and agreement in my proposals. I had been talking about dbt almost since I joined the team, but in a “this is a cool tool” way, not in a “let’s do it” way. If I was going to ask everyone to learn and use this new tool, I wanted to make sure they believed it was worth it too.</p> <p>I don’t want to go into too much detail about what dbt is and how we are implementing it - I’ll save that for a future blog post. But suffice to say that the project got started early in 2023, and after 6 months we are finally in a sprint of building models and adding to the project, with almost all engineers onboarded and starting to onboard analysts. I’m very excited to see what further progress we can make for the rest of the year - and I’m especially excited to be able to share this experience of implementing dbt for a city analytics team in <a href="https://coalesce.getdbt.com/agenda/from-coast-to-coast-implementing-dbt-in-the-public-sector">a talk at this year’s Coalesce</a> (dbt Lab’s annual conference).</p> <p>Besides the dbt project, this year I’ve also been working on larger and more complex projects that have involved a lot of data architecture/design work. Without going into too much detail, the city has a lot of legacy systems that don’t talk to each other, when city workers really need for information to be passed between those systems in order to do their work well and efficiently. I have been involved in a couple projects this year that have been focused on how to integrate these systems (or at least have all of the data in one place and able to be joined together), particularly for housing and development work. I love building ETL pipelines, but I know a project will be special when I get to do system design and data architecture work before building the pipeline. These projects are always bigger, longer, and trickier, involve working with more stakeholders and teams, and are just so rewarding because you can see the difference your contribution is making and you can build relationships outside of your team.</p> <p>Another high point for this year on the team is that I’ve had the opportunity to start teaching Carpentries workshops again. I taught a <a href="https://carpentries-incubator.github.io/git-novice-branch-pr/">workshop on git</a> because the analysts on the team have been writing more code and wanted to preserve and collaborate on their code in a GitHub repository, and I taught some <a href="https://swcarpentry.github.io/python-novice-gapminder/07-reading-tabular.html">sessions on python and pandas</a> as a part of a Data Culture pilot program focused on python. Later this year I’m planning on teaching a [SQL workshop](https://swcarpentry.github.io/sql-novice-survey/ and a session on querying APIs with python. Teaching computational skills is something I really enjoy, and it was the one thing I was really missing in this job last year.</p> <p>Finally, if you think that working on a city analytics team sounds interesting, I will plug the fact that we are <a href="https://www.boston.gov/news/join-city-bostons-department-innovation-and-technology">currently hiring for several positions</a> - if you live in Boston or want to move to Boston, <a href="https://www.boston.gov/career-center">apply</a>!</p>]]></content><author><name></name></author><category term="life-updates"/><category term="musings"/><summary type="html"><![CDATA[from grad student to working data professional]]></summary></entry><entry><title type="html">Carpentries Workshops</title><link href="https://jennajordan.me/projects/carpentries-workshops" rel="alternate" type="text/html" title="Carpentries Workshops"/><published>2021-09-28T00:00:00+00:00</published><updated>2021-09-28T00:00:00+00:00</updated><id>https://jennajordan.me/projects/carpentries-workshops</id><content type="html" xml:base="https://jennajordan.me/projects/carpentries-workshops"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_carpentries-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_carpentries-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_carpentries-1400.webp"/> <img src="/assets/img/header_carpentries.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My Carpentries teaching journey started out slow - I taught a couple of times that first year, and stuck to the lesson I was most comfortable with (SQL &amp; Databases). However, at the start of the summer in 2021 my work put me in charge (or I volunteered to be in charge) of a group of undergraduate interns who wanted to learn how to use computational methods for open source intelligence analysis. So I put together a curriculum of Carpentries lessons to take my interns from zero-assumed knowledge to the ability to complete a computational analysis in Python. I’m teaching the same curriculum in the Fall (slightly expanded and refined) to another group of undergraduate interns. This allowed me to gain experience in teaching more of the core Carpentries lessons… and also inspired me to develop <a href="/projects/carpentries-dataviz-workshop">my own lesson focused on interactive data visualizations</a>!</p> <hr/> <h1 id="a-semester-long-curriculum-of-carpentries-workshops">A semester-long curriculum of Carpentries workshops</h1> <p>Each week I teach for 2 hours, and then learners can practice what they’ve learned and give feedback on the workshop in an assignment (delivered via Google Forms). I’ve included the curriculum schedule below:</p> <h2 id="week-0-self-assessment">Week 0: Self Assessment</h2> <p><strong>Goals:</strong></p> <ul> <li>Learners assess their current skill level</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/QsB6fFpn9VWiVjRz9">Computational Skills Self Assessment form</a></li> </ul> <h2 id="week-1-github">Week 1: GitHub</h2> <p><strong>Goals:</strong></p> <ul> <li>Instructor &amp; learners introduce themselves</li> <li>Overview of the curriculum</li> <li>All learners have a GitHub account and know the basics of creating and modifying a personal repository on GitHub (with GitHub Desktop).</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/SPUiKxH5gtc1q5o47">Assignment 1: GitHub</a></li> </ul> <p><strong>Resources:</strong></p> <ul> <li><a href="https://github.com/elliewix/github-training-brain-dumps/blob/master/github_directions.md">Getting Started with GitHub - Training by Elizabeth Wickes</a></li> <li><a href="https://docs.github.com/en/github/getting-started-with-github/signing-up-for-github">GitHub’s docs on creating a new account</a></li> <li><a href="https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/overview/getting-started-with-github-desktop#introduction">GitHub’s docs on GitHub Desktop</a></li> <li><a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/creating-a-new-repository">GitHub’s docs on creating a new repo</a></li> <li><a href="https://www.makeareadme.com/">Writing a good README</a></li> <li><a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github">GitHub’s docs on cloning a repo</a></li> </ul> <h2 id="week-2-the-shell-part-1">Week 2: The Shell (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/shell-novice/">Shell workshop lesson plan from Software Carpentries</a> (Lessons 1-5)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>become comfortable navigating your computer via the command line</li> <li>learn the basic set of Bash commands (pwd, ls, man, cd, mkdir, nano, touch, mv, cp, rm, wc, cat, less, sort, head, tail, echo)</li> <li>learn how to chain commands using the pipe, redirect, and append</li> <li>learn how to use loops</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/CqZ1zkhdAmdBju8c9">Assignment 2: The Shell</a></li> <li><a href="https://docs.google.com/document/d/1uvkMieUHK2JM4XPx8mAeEmqvfdNiDSAQc2mUHshrAvE/edit?usp=sharing">Answer Key for Assignment 2</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="http://linuxcommand.org/lc3_learning_the_shell.php#contents">The Linux Command Line - web tutorials</a></li> <li><a href="https://phoenixnap.dl.sourceforge.net/project/linuxcommand/TLCL/19.01/TLCL-19.01.pdf">The Linux Command Line - full free textbook</a></li> <li><a href="https://explainshell.com/">Explain Shell</a></li> </ul> <h2 id="week-3-the-shell-part-2">Week 3: The Shell (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/shell-novice/">Shell workshop lesson plan from Software Carpentries</a> (Lessons 6 &amp; 7)</li> <li><a href="http://carpentries-incubator.github.io/shell-extras/">Shell Extras workshop lesson plan from Software Carpentries</a> (Lesson 2)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to save a series of commands as a shell script</li> <li>Learn how to search files from the command line <ul> <li>grep, find</li> </ul> </li> <li>Learn how to generate and use your own SSH key pair</li> <li>Learn how to access a computer remotely <ul> <li>ssh, scp</li> </ul> </li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/hVq8VgEBWPyVXC4S8">Assignment 3: The Shell cont</a></li> <li><a href="https://docs.google.com/document/d/1B0qwQVUyNudAQ7jGPIAIONmU32FBedkNo4wkyPEsX74/edit?usp=sharing">Answer Key for Assignment 3</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://linuxize.com/post/using-the-ssh-config-file/">Setting up and using the SSH config file</a></li> </ul> <h2 id="week-4-git">Week 4: Git</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/git-novice/">Git workshop lesson plan from Software Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to use git from the command line</li> <li>Set up git on the remote sandbox server <ul> <li>git config</li> </ul> </li> <li>Learn how to create repositories <ul> <li>git init</li> </ul> </li> <li>Learn how to track changes <ul> <li>git add, git commit, git status, git log, git diff</li> </ul> </li> <li>Learn how to use the history <ul> <li>HEAD, git show, git checkout, git revert</li> </ul> </li> <li>Learn how to not track things <ul> <li>.gitignore</li> </ul> </li> <li>Set up a remote repository <ul> <li>git remote, git push, git pull</li> </ul> </li> <li>Learn how to collaborate &amp; resolve conflicts <ul> <li>git clone</li> </ul> </li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/XyEa9RmgznnRuLPx7">Assignment 4: Git</a></li> <li><a href="https://docs.google.com/document/d/1VUVdRvK9YPIEqiVrWX_3pL1DeSr9e-brajbPsx0XgeY/edit?usp=sharing">Answer Key for Assignment 4</a></li> </ul> <h2 id="week-5-python-part-1">Week 5: Python (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 1-7)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Everyone has Anaconda (and thus Python &amp; Jupyter Lab) installed and is familiar with how to work within Jupyter Lab and use a Jupyter Notebook</li> <li>Learn about data types and type conversion in python</li> <li>Learn how to use built in functions</li> <li>Learn how to get help, read the built-in docs, and read errors</li> <li>Learn how to import and use libraries</li> <li>Learn how to read data into a dataframe and interact with the dataframe (pandas)</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/oJYrV2aPVtnVPHJ56">Assignment 5: Python</a></li> <li><a href="https://docs.google.com/document/d/1FGN-ZHiPPu_dW73kC7o3ks9kWwXP8VtvXcPXe--2B2E/edit?usp=sharing">Answer Key for Assignment 5</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://carpentries-incubator.github.io/introduction-to-conda-for-data-scientists/">Intro to Anaconda from Software Carpentries</a></li> </ul> <h2 id="week-6-python-part-2">Week 6: Python (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 8-12)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to select data from a dataframe (pandas)</li> <li>Learn how to plot data (matplotlib.pyplot)</li> <li>Learn how to work with lists</li> <li>Learn about for loops and the accumulator pattern</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/XPxxRaBubMaDaptD9">Assignment 6: Python &amp; pandas</a></li> <li><a href="https://github.com/NCRI-io/nclabs_pandaspractice">GitHub repo for the pandas practice</a></li> </ul> <h2 id="week-7-python-part-3--requests--rest-apis">Week 7: Python (part 3) + requests &amp; REST APIs</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 13-18)</li> <li><a href="https://cac-staff.github.io/summer-school-2016-Python/11-dicts.html">Storing Information with Dictionaries from Software Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn about conditionals (if, else, elif)</li> <li>Looping over files (glob)</li> <li>Learn how to write functions</li> <li>Learn about the dictionary data type</li> <li>Learn how use python’s requests library to query a REST API</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/GGDeBWAmg5Ck7WRA7">Assignment 7: Python</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://realpython.com/python-api/">Python &amp; APIs: A Winning Combo for Reading Public Data from Real Python</a></li> <li><a href="https://www.dataquest.io/blog/python-api-tutorial/">Python API Tutorial: Getting Started with APIs from DataQuest</a></li> </ul> <h2 id="week-8-interactive-data-visualizations">Week 8: Interactive Data Visualizations</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="https://carpentries-incubator.github.io/python-interactive-data-visualizations/">Interactive Data Visualizations in Python workshop lesson plan</a></li> </ul> <p><strong>Goals</strong></p> <ul> <li>Learn how to create a new conda environment</li> <li>Learn how to wrangle data into a tidy shape</li> <li>Learn how to create line plots with plotly</li> <li>Learn how to create and run a streamlit app</li> <li>Learn how to add widgets to the streamlit app</li> </ul> <h2 id="week-9-databases--sql-part-1">Week 9: Databases &amp; SQL (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/sql-novice-survey/">SQL workshop lesson plan from Software Carpentries</a> (Lessons 1-5)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to select data from a table</li> <li>Learn how to sort results and remove duplicates</li> <li>Learn how to filter, or select subsets of data, based on boolean conditions</li> <li>Learn how to calculate new values to be returned in the results</li> <li>Learn about missing data, and incorporating NULL into queries</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/wWQFYSREEXLkQQMA6">Assignment 8: SQL</a></li> </ul> <h2 id="week-10-databases--sql-part-2">Week 10: Databases &amp; SQL (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/sql-novice-survey/">SQL workshop lesson plan from Software Carpentries</a> (Lessons 6-10)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to aggregate data in queries (calculate sums, averages, etc)</li> <li>Learn how to combine tables in a query</li> <li>Learn about data hygiene, and principles of database design like primary &amp; foreign key constraints</li> <li>Learn how to create, modify, and delete data</li> <li>Learn how to use SQLite databases within a python script</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/hdgDb7KsQU2DhVCw9">Assignment 9: SQL</a></li> <li>Review <a href="https://swcarpentry.github.io/sql-novice-survey/10-prog/index.html">Lesson 10</a> (Programming with Databases - Python) on your own</li> </ul> <h2 id="week-11-regular-expressions">Week 11: Regular Expressions</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="https://librarycarpentry.org/lc-data-intro/">Regular Expressions workshop lesson plan from Library Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to use Regular Expressions (regex)</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li>Practice using regular expressions by completing some <a href="https://regexcrossword.com/">Regex Crosswords</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://youtube.com/playlist?list=PL7C1EB31127AB8A0B">Regular Expressions workshop videos from Software Carpentries</a></li> </ul>]]></content><author><name></name></author><category term="software-carpentries"/><summary type="html"><![CDATA[An overview of the workshops I've taught, including a semester-long syllabus.]]></summary></entry><entry><title type="html">About The Carpentries</title><link href="https://jennajordan.me/blog/about-the-carpentries" rel="alternate" type="text/html" title="About The Carpentries"/><published>2021-09-27T00:00:00+00:00</published><updated>2021-09-27T00:00:00+00:00</updated><id>https://jennajordan.me/blog/about-the-carpentries</id><content type="html" xml:base="https://jennajordan.me/blog/about-the-carpentries"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_carpentries-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_carpentries-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_carpentries-1400.webp"/> <img src="/assets/img/header_carpentries.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If you’ve never heard of <a href="https://carpentries.org">The Carpentries</a> before, you may be confused - have I gotten into woodworking?! (No… but I have gotten into knitting - the real kind, not the RMarkdown kind).</p> <p>From the horse’s mouth, here’s what The Carpentries is all about:</p> <blockquote> <p><strong>Vision:</strong> Our vision is to be the leading inclusive community teaching data and coding skills.</p> <p><strong>Mission:</strong> The Carpentries builds global capacity in essential data and computational skills for conducting efficient, open, and reproducible research. We train and foster an active, inclusive, diverse community of learners and instructors that promotes and models the importance of software and data in research. We collaboratively develop openly-available lessons and deliver these lessons using evidence-based teaching practices. We focus on people conducting and supporting research.</p> <p>-– <a href="https://carpentries.org/about/">The Carpentries - About Us</a></p> </blockquote> <h2 id="how-i-got-involved-with-the-carpentries">How I got involved with The Carpentries</h2> <p>My first semester of my MSLIS, I took an Intro to Python class with Elizabeth Wickes. After completing the class, Elizabeth (an Instructor Trainer with The Carpentries and an elected member of the Executive Council since 2018) encouraged me to take the Carpentries instructor training class and get involved with the local UIUC chapter of The Carpentries… but it took me another year before I felt comfortable enough with my technical skills to feel like I could teach them. In December 2019, I finally took the instructor training class with Elizabeth Wickes &amp; Neal Davis. It was a 2-day class that prepared new instructors to teach Carpentries workshop - with a special emphasis on how to teach while live-coding. As with all things Carpentries, the <a href="https://carpentries.github.io/instructor-training/">Instructor Training course</a> is also freely available.</p> <p>I had the chance to be a helper for one in-person workshop in early 2020 before… the pandemic happened. Then, everything went remote. I was a helper for another workshop series online, before I finally felt comfortable enough to teach a workshop for myself in Summer of 2020 - the SQL lesson, which remains my favorite lesson to teach!</p> <h2 id="why-i-love-the-carpentries">Why I love The Carpentries</h2> <p>The Carpentries is something special. For one, all of the workshop lessons are freely available - so anyone can use the lesson plans to teach or learn for themselves. The lessons live in GitHub repos, so they can be collaboratively developed. Anyone can develop their own Carpentries lesson using the provided template and submit the lesson to the Carpentries Incubator, so there are a ton of lessons out there beyond the core set. All of the lessons follow a similar pedagogical philosophy live coding throughout the workshop, getting learners to a place of being able to <em>use</em> their new skills as quickly as possible, and being as inclusive and supportive as possible. <a href="https://carpentries.org/blog/2019/07/alex-ttt-reflection/">This reflection blog post</a> details some of the pedagogy that is the foundation of all Carpentries lessons.</p> <p>The Carpentries is also all about community. Some instructors are based at a university and have the support of their local chapter, but even if instructors don’t have a local chapter, they can be supported by the global online community. There are community discussions via Zoom and a Slack workspace so instructors can stay connected and learn from each other, and many instructors are active on social media, like Twitter. There’s also <a href="https://carpentrycon.org">CarpentryCon</a>!</p> <p>So if you’re interested in getting involved with The Carpentries, <a href="https://carpentries.org/volunteer/">go for it!</a> I cannot emphasize enough how much I have learned and benefited from being a part of The Carpentries community.</p>]]></content><author><name></name></author><category term="software-carpentries"/><summary type="html"><![CDATA[All about The Carpentries, how I got involved, and what I love about the organization.]]></summary></entry><entry><title type="html">My Classes for Spring 2020</title><link href="https://jennajordan.me/blog/my-classes-for-spring-2020" rel="alternate" type="text/html" title="My Classes for Spring 2020"/><published>2020-02-23T00:00:00+00:00</published><updated>2020-02-23T00:00:00+00:00</updated><id>https://jennajordan.me/blog/my-classes-for-spring-2020</id><content type="html" xml:base="https://jennajordan.me/blog/my-classes-for-spring-2020"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_fall2018classes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_fall2018classes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_fall2018classes-1400.webp"/> <img src="/assets/img/header_fall2018classes.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is it - the last semester of my MS/LIS degree. As I am currently job searching, I decided to take a slightly lighter class load for this final semester: 2 regular classes, 1 short class, and an independent study. Here they are:</p> <ul> <li>IS532: Theory &amp; Practice of Data Cleaning</li> <li>IS590PZ: Data Structures &amp; Algorithms - Puzzles &amp; Games</li> <li>IS590SC: Introduction to Command Line Tools</li> <li>IS592: Independent Study</li> </ul> <p>Note that the course descriptions &amp; learning outcomes are pulled from the syllabi for each respective course.</p> <h2 id="is532-theory--practice-of-data-cleaning">IS532: Theory &amp; Practice of Data Cleaning</h2> <p>While I have encountered a lot of the topics covered in this class in previous classes (e.g. Regular Expressions, the Relational Model), this is still a must-take class at the iSchool. I have actually been meaning to take this class for a while, but because it is reliably offered every semester, I kept putting it of to take other classes. This class teaches several concepts important to data curation (e.g. data provenance, reproducibility), and includes some new topics that are quite exciting to me - like logic programming (Datalog) and workflow automation.</p> <h3 id="course-description">Course Description</h3> <blockquote> <p>Data cleaning (also: cleansing) is the process of assessing and improving data quality for later analysis and use, and is a crucial part of data curation and analysis. This course identifies data quality issues throughout the data lifecycle, and reviews specific techniques and approaches for checking and improving data quality. Techniques are drawn primarily from the database community, using schema-level and instance-level information, and from different scientific communities, which are developing practical tools for data pre-processing and cleaning.</p> </blockquote> <h3 id="learning-objectives">Learning Objectives</h3> <ul> <li>Understand how to detect and flag data quality problems.</li> <li>Understand principles of data and information modeling.</li> <li>Understand techniques that support automated data curation and cleaning.</li> </ul> <h3 id="key-topics">Key Topics</h3> <ul> <li>Introduction to Data Quality and Data Cleaning</li> <li>Syntactic Issues and Regular Expressions</li> <li>Hands-on Data Cleaning with OpenRefine… ​and optionally with R or Python</li> <li>Logic-Based Integrity Constraints in ​Datalog &amp; ​SQL</li> <li>Workflow Automation and Modeling</li> <li>From Workflows to Data Provenance and Reproducibility</li> </ul> <h2 id="is590pz-data-structures--algorithms---puzzles--games">IS590PZ: Data Structures &amp; Algorithms - Puzzles &amp; Games</h2> <p>This is a new class at the iSchool (this semester is the 2nd time it has been offered). The goal of the class is to teach students about data structures and algorithms in a fun and interesting way… through puzzles and games. Each week we are given a new puzzle or game that we have to represent in some kind of data structure and then solve based on a specific algorithmic approach. Many of the projects are done in groups, although some can also be completed individually. This class is challenging and fast-paced, but also highly rewarding because it pushes me to advance my python problem solving skills. Also, as a huge fan of puzzles and games, this class is just intrinsically interesting.</p> <h3 id="course-description-1">Course Description</h3> <blockquote> <p>Learn, experiment, code with, and compare performance of common data structures and algorithms in a fun, collaborative, and challenging context. In class, students will discuss and solve logic puzzles and play several types of strategy games. In small teams they will explore the deductive, strategic, and tactical decisions involved, select appropriate data structures &amp; algorithms to develop efficient program solutions to automate playing, solving, generating, or analyzing puzzles &amp; games. Techniques and tools used include analysis of efficiency (Big-O), recursion, minimax, Monte Carlo Tree Search, client/server network communications, deterministic vs non-deterministic algorithms. Structures used include arrays, matrices, hash tables, stacks, various trees, network graphs, and custom structures. For some projects, students will have competitions pitting their solutions against other teams</p> </blockquote> <h3 id="learning-objectives-1">Learning Objectives</h3> <p>Though the contextual focus of the course is on strategic games and logic puzzles, the underlying purpose is for students to learn and practice the following critical and broadly-useful skills:</p> <ul> <li>logical and analytical thinking and problem solving</li> <li>use of performance analysis techniques as one aspect of comparing similar solutions/algorithms</li> <li>understand how data structures and algorithms are interdependent and make suitable choices accordingly</li> <li>practical coding ability using data structures and concepts listed in the course description above.</li> </ul> <h2 id="is590sc-introduction-to-command-line-tools">IS590SC: Introduction to Command Line Tools</h2> <p>This is a one-month short course offered at the end of the semester. It covers many of the vital skills often glossed over in typical programming classes - like version control, shell scripting, and computer cluster/cloud tools.</p> <h3 id="course-description-2">Course Description</h3> <p>This class will provide an overview of the history and commonly offered command line interfaces and essential shell scripting tools. These approaches will be extended to cover common version control tools, including git and GitHub, their value, and how to appropriately organize a project within them. We will also review how to submit projects to the Illinois Campus Cluster tool, and touch on situations where it may be valuable to do so.</p> <h3 id="textbook">Textbook</h3> <p class="small"><br/> <strong>“The Linux Command Line”</strong> (2nd ed), by William Shotts</p> <p><a href="http://linuxcommand.org/tlcl.php" class="btn btn--primary">Available online here</a></p> <h2 id="is592-independent-study">IS592: Independent Study</h2> <p>I decided to do an independent study this semester because I wanted to learn how to analyze data using python packages that are typically used for data science projects. Last semester, I learned how to perform statistical analyses in R (IS542) and how to use various machine learning algorithms for data mining in WEKA (IS590DT). However, the language that I am most comfortable with - and use for data wrangling - is python, and I have yet to learn how to apply the concepts I learned in those previous classes to the python data science ecosystem. I also want to learn how to do time-series analysis - something that neither of those previous classes covered.</p> <p>This independent study is also functioning as a follow up to another class I took last semester, Open Data Mashups (IS590OM). The goal of that class was to produce a new analysis-ready dataset that combined data from other open data sources. I created a country-year time-series dataset that combined data from the Correlates of War project, the UCDP/PRIO Armed Conflict dataset, the World Bank’s World Development Indicators, and the Polity IV project. For my independent study, I will be analyzing this dataset in a variety of ways, implementing the statistical learning techniques I learned last semester in python.</p> <h3 id="resources">Resources</h3> <p class="small"><br/> <strong>“Introduction to Machine Learning with Python”</strong> (2016), by Andreas Müller and Sarah Guido</p> <p><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/" class="btn btn--primary">Available here</a></p> <p class="small"><strong>“Python for Data Analysis”</strong> (2017), by Wes McKinney</p> <p><a href="https://learning.oreilly.com/library/view/python-for-data/9781491957653/" class="btn btn--primary">Available here</a></p> <p class="small"><strong>“Python Data Science Handbook”</strong> (2017), by Jake VanderPlas</p> <p><a href="https://jakevdp.github.io/PythonDataScienceHandbook/" class="btn btn--primary">Available here</a></p> <p class="small"><strong>“Machine Learning”</strong> MOOC by Andrew Ng</p> <p><a href="https://www.coursera.org/learn/machine-learning/home/info" class="btn btn--primary">Available here</a></p>]]></content><author><name></name></author><category term="MSLIS"/><summary type="html"><![CDATA[what classes I took for my 4th (and final) semester of grad school]]></summary></entry><entry><title type="html">My Classes for Fall 2019</title><link href="https://jennajordan.me/blog/my-classes-for-fall-2019" rel="alternate" type="text/html" title="My Classes for Fall 2019"/><published>2019-09-29T00:00:00+00:00</published><updated>2019-09-29T00:00:00+00:00</updated><id>https://jennajordan.me/blog/my-classes-for-fall-2019</id><content type="html" xml:base="https://jennajordan.me/blog/my-classes-for-fall-2019"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_fall2018classes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_fall2018classes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_fall2018classes-1400.webp"/> <img src="/assets/img/header_fall2018classes.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>One year down, one year to go! During the first year of my MSLIS degree, I focused on the fundamentals: how to organize and process data. This year, I am shifting my focus to analysis. This semester I am taking three classes focused on different ways to analyze data, as well as one project-oriented class that will allow me to collect together the data I want to analyze.</p> <ul> <li>IS542: Data, Statistical Models, and Information</li> <li>IS590DV: Data Visualization</li> <li>IS590DT: Data Mining</li> <li>IS590OM: Open Data Mashups</li> </ul> <p>Note that the course descriptions, themes, learning outcomes and such are pulled from the syllabi for each respective course.</p> <h2 id="is542-data-stats--info">IS542: Data, Stats, &amp; Info</h2> <p>This is an introduction to statistics for information professionals - it covers the fundamental concepts of statistical analysis and how to perform those analyses in base R using RStudio.</p> <h3 id="course-description">Course Description</h3> <blockquote> <p>An introduction to statistical and probabilistic models that are used to quantify information, assess information quality, and make decisions in a principled way. The increasing prevalence of massive data sets and falling computational barriers have rendered statistical modeling an integral part of contemporary information management. With this in mind, this class prepares students to select and properly undertake common modeling tasks. The course reviews relevant results from probability theory, emphasizing the merits and limitations of familiar probability distributions as vehicles for modeling information. Subsequent consideration includes parametric and non-parametric predictive models, as well as extensions of these models for unsupervised learning. Throughout these discussions, the course focuses on model selection and gauging model quality. Applications of statistical and probabilistic models to tasks in information management (e.g. prediction, ranking, and data reduction) are emphasized.</p> </blockquote> <h3 id="learning-outcomes">Learning Outcomes</h3> <ul> <li>Explain the role of marginal, joint, and conditional probability in modeling processes involving information.</li> <li>Select, parameterize, and compare probability distributions as vehicles for modeling information.</li> <li>Specify, estimate and evaluate elementary parametric and non-parametric statistical models.</li> <li>Articulate the responsibilities of a professional who creates, describe, or uses models of data.</li> </ul> <h3 id="textbooks">Textbooks</h3> <p class="small"><br/> <strong>“An Introduction to Statistical Learning: with Applications in R”</strong> (2013), by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani</p> <p><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/" class="btn btn--primary">Author’s Site</a></p> <p class="small"><strong>“OpenIntro Statistics”</strong> (3rd ed)</p> <p><a href="https://www.openintro.org/stat/textbook.php?stat_book=os" class="btn btn--primary">Publisher’s Site, free download</a></p> <p class="small"><strong>“An Introduction to R”</strong> (3rd ed), Venables, W.N., Smith, D.M and the R Core Team</p> <p><a href="http://cran.r-project.org/doc/manuals/R-intro.pdf" class="btn btn--primary">Full Document</a></p> <h2 id="is590dv-data-visualization">IS590DV: Data Visualization</h2> <p>One of the most popular classes in the iSchool, this class is all about data visualization. The class mostly focuses on python libraries, and each class has both a lecture that deep-dives on one particular aspect of visualization as well as live code-alongs.</p> <h3 id="course-description-1">Course Description</h3> <blockquote> <p>Data visualization is crucial to conveying information drawn from models, observations or investigations. This course will provide an overview of historical and modern techniques for visualizing data, drawing on quantitative, statistical, and network-focused datasets. Topics will include construction of communicative visualizations, the modern software ecosystem of visualization, and techniques for aggregation and interpretation of data through visualization. Particular attention will be paid to the Python ecosystem and multi-dimensional quantitative datasets.</p> </blockquote> <h3 id="central-themes">Central Themes</h3> <ol> <li>What are the components of an effective visualization of quantitative data?</li> <li>What tools and ecosystems are available for visualizing data?</li> <li>What systems can be put in place to generate visualizations rapidly and with high-fidelity representation?</li> </ol> <h3 id="course-website">Course Website</h3> <p>Lecture slides and Jupyter notebooks from the code-alongs are available on the course’s GitHub website.</p> <p><a href="https://uiuc-ischool-dataviz.github.io/fall2019/" class="btn btn--primary">IS590DV Fall 2019 Website</a></p> <h2 id="is590dt-data-mining">IS590DT: Data Mining</h2> <p>While IS542 focuses on the more traditional methods of statistical analysis, this class focuses on modern methods of extracting information from large data. Techniques for classification, clustering, and prediction are taught using <a href="https://www.cs.waikato.ac.nz/ml/weka/">WEKA</a>, a free software for machine learning.</p> <h3 id="course-description-2">Course Description</h3> <blockquote> <p>Data mining refers to the process of exploring large datasets with the goal of uncovering interesting patterns. This process usually involves a number of tasks such as data collection, pre-processing, and characterization; model fitting, selection, and evaluation; classification, clustering, and prediction. Although data mining has its roots in database management, it has grown into a discipline that focuses on algorithm design (to ensure computational feasibility) and statistical modeling (to separate the signal from the noise). As such, it draws heavily upon a variety of other disciplines including statistics, machine learning, operations research, and information retrieval. This course will cover the major data mining concepts, principles, and techniques that <em>every information scientist should know about.</em> Lectures will introduce and discuss the major approaches to data mining, computer lab sessions coupled with assignments will provide hands-on experience with these approaches, and term projects offer the opportunity to use data mining in a novel way. Mathematical detail will be left to the students who are so inclined.</p> </blockquote> <h3 id="course-objectives">Course Objectives</h3> <ol> <li>To gain a broad exposure to data mining concepts and principles through lectures and discussion.</li> <li>To develop a working proficiency in selected data mining techniques through lab sessions, hands-on assignments, and office hours.</li> <li>To nurture the ability to detect opportunities to apply data mining concepts, principles and techniques in new scenarios by independent exploration of resources beyond the course materials, and, optionally, through a course project.</li> </ol> <h3 id="textbook">Textbook</h3> <p class="small"><br/> <strong>“Data Mining: Practical Machine Learning Tools and Techniques”</strong> (4th ed), by I.H. Witten, E. Frank, M.A. Hall, and C.J. Pal</p> <p><a href="https://doi.org/10.1016/C2015-0-02071-8" class="btn btn--primary">Book via ScienceDirect</a></p> <h4 id="data">Data</h4> <p>Data used for the course is available through the professor’s website.</p> <p><a href="http://abel.lis.illinois.edu/data/" class="btn btn--primary">Data</a></p> <h2 id="is590om-open-data-mashups">IS590OM: Open Data Mashups</h2> <p>The entire point of this class is to produce a new dataset that combines multiple other open data sources. The project is the class - with some career planning and development sprinkled in along the way. The end goal is to have a project for your portfolio, and the tools to use that project when applying for jobs - think elevator pitch, cover letter, resume, presentation, etc. As an added bonus, you have an interesting dataset ready and waiting for analysis.</p> <h3 id="course-description-3">Course Description</h3> <blockquote> <p>Data sharing and modern open data standards have been creating large repositories of data that remain disconnected. Many data science and machine learning techniques are boosted by incorporating data representing a variety of domains and granularities. Topics on data curation, data cleaning, copyright, web scraping, storage, processing, and automation will be reviewed. This course seeks to explore techniques and perspectives of combining various data sources to create a dataset ready for analysis, but in a project oriented space so that each topic is synthesized with practice and experienced in context. Students will select a project area and explore the technical and conceptual requirements of that project space, eventually producing a proof of concept around it. All project domains and areas are open, with the only requirement being that they combine several data sources into a new dataset. This course is meant for students who have completed at least two semesters of coursework, are comfortable with programming in Python (the project can be completed in any language, but instruction will be in Python), and desire a space to explore and develop a capstone or independent study project. However, further work on the project is not a requirement. Guest speakers and field experts from the University Library will be invited. Students will be encouraged to share and publish their datasets at the end of the semester.</p> </blockquote> <h3 id="learning-goals">Learning Goals</h3> <ol> <li>Identify and assess relevant data sources for a research project</li> <li>Process, clean, and quality check datasets</li> <li>Combine multiple datasets into one for an analysis process</li> <li>Produce quality dataset and project documentation</li> <li>Effectively deliver various types of project pitches</li> <li>Write effective conference talk proposals</li> <li>Provide constructive peer feedback and peer reviews</li> <li>Receive and interpret peer feedback</li> </ol>]]></content><author><name></name></author><category term="MSLIS"/><summary type="html"><![CDATA[what classes I took for my 3rd semester of grad school]]></summary></entry><entry><title type="html">My Class and Internship for Summer 2019</title><link href="https://jennajordan.me/blog/my-class-and-internship-for-summer-2019" rel="alternate" type="text/html" title="My Class and Internship for Summer 2019"/><published>2019-07-31T00:00:00+00:00</published><updated>2019-07-31T00:00:00+00:00</updated><id>https://jennajordan.me/blog/my-class-and-internship-summer-2019</id><content type="html" xml:base="https://jennajordan.me/blog/my-class-and-internship-for-summer-2019"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_fall2018classes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_fall2018classes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_fall2018classes-1400.webp"/> <img src="/assets/img/header_fall2018classes.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Today was the last day of both my class and my internship for the summer! I have been taking a class online, IS562: Metadata in Theory &amp; Practice, as well as working as a Data Science Intern at the Program on Governance and Local Development in Gothenburg, Sweden. First I’ll introduce the class, and then share a reflection on my internship experience.</p> <h2 id="is562-metadata-in-theory--practice">IS562: Metadata in Theory &amp; Practice</h2> <p>Rather than focusing on learning a specific metadata schema (some classes focus just on using those common in libraries, like MARC and MODS), this class focused on teaching the overarching concepts and technical traits that all metadata schemas share, in order to prepare students to work with any of the wide variety of metadata schemas and standards used in the real world (libraries, companies, academia, government, archives, etc). Along the way we got practice using a wide variety of specific metadata schemas (Dublin Core, CDWALite, PREMIS, and CIDOC-CRM just to name a few). While most metadata schemas are based in XML, we also learned about schemas that use RDF and how the semantic web and linked open data are becoming more prominent in the metadata world. PREMIS is a schema that currently uses XML but is in the process of transitioning to RDF, which makes it a very interesting schema to study.</p> <h3 id="course-description">Course Description</h3> <blockquote> <p>Metadata plays an increasingly critical role in the creation, distribution, management and use of electronic materials. This course will combine theoretical examination of the design of metadata schema with their practical application in a variety of settings. Hands-on experience in the creation of descriptive, administrative, technical, provenance and structural metadata in XML- and RDF-based schema, along with their application in systems such as harvesting and digital repositories, will help students develop a thorough understanding of current practices in metadata and metadata schema creation.</p> </blockquote> <blockquote> <p>Combines theoretical examination of the design of metadata schema with their practical application in a variety of settings. Hands-on experience in the creation of descriptive, administrative, and structural metadata, along with their application in systems such as OAI harvesting, OpenURL resolution systems, metasearch systems and digital repositories, will help students develop a thorough understanding of current metadata standards as well as such issues as crosswalking, metadata schema, metadata’s use in information retrieval and data management applications, and the role of standards bodies in metadata schema development.</p> </blockquote> <h3 id="main-topics">Main Topics</h3> <ul> <li>Identifiers</li> <li>Descriptive Metadata - Dublin Core, CDWALite</li> <li>Administrative Metadata - PBCore</li> <li>Technical Metadata, Provenance &amp; Preservation - PREMIS, MIX,</li> <li>Structural Metadata - METS</li> <li>XML, Oxygen (XML Editor), Designing schemas with XSDs</li> <li>RDF, the Semantic Web, Serialization, and Linked Open Data - CIDOC-CRM</li> <li>Crosswalking, Quality Control, XSLTs</li> <li>OAI-PMH, SPARQL</li> <li>Workflows and Application Profiles</li> </ul> <h2 id="internship">Internship</h2> <p>I have spent the past 10 weeks working as a Data Science Intern at the Program on Governance and Local Development (GLD). The internship was kicked off in the best way possible - attending GLD’s annual conference to learn about accountability in local governance and enjoying the wonderful hospitality only a Swedish seaside spa hotel can offer. The conference was a great way to learn about the ongoing academic work in local governance and development, and I was able to meet many interesting professionals from both academia and development agencies. But after the conference, it was time to get to work. This summer GLD has been conducting a survey in Kenya, Zambia, and (later) Malawi to gather data for their Local Governance Performance Index. GLD had just hired a new Data Scientist, and it was my job to help her with monitoring the data incoming daily from the survey. In addition to learning new tools like Survey To Go and dipping my toes into R, I was able to use the Python skills I’ve spent the last year developing almost every day. My skillset of information processing and organization was a perfect complement to the Data Scientist’s skillset in statistical analysis (she had just completed her PhD in Statistics), and we worked really well together. Data science typically has two sides - preparing the data, and analyzing the data. I happen to enjoy (and have more experience in) preparing the data, while her expertise was in analysis. Along the way, I got a few ad-hoc lessons in various statistical concepts (she loves to teach statistics, and I was a very willing audience), and I did my best to make her job easier and allow her to spend more time on analyzing the data.</p> <p>The vast majority of my python work was in processing XML documents using XPath (extracting needed information and sometimes putting it into tabular form), reorganizing tabular data using Pandas (with my design choices informed by relational databases), and writing extracted data into whatever format was needed for analysis (such as WKT files for use in QGIS). Every week - sometimes every day - there was some new information puzzle to solve… and it was fun. This was messy, real-world data that would eventually be used to try and improve people’s lives. While using my newly developed technical skills was a major priority for me, it was just as important that my work, my time and effort, would go toward improving the world.</p> <p>This internship was also a perfect transition between my first year at the iSchool - which was focused on information organization and processing - and my second year at the iSchool - which will be more focused on data analysis. I got to use my python skills prepare data for analysis, while getting an introduction to analysis through the osmosis of office conversations. I’ve even started working my way through Hadley Wickham’s <a href="https://r4ds.had.co.nz">R For Data Science</a>, in anticipation of learning R in one my classes next semester. I have learned so much over the past year… I can only hope that I will learn just as much over the next!</p>]]></content><author><name></name></author><category term="MSLIS"/><summary type="html"><![CDATA[the class I took during the summer semester of grad school and my summer internship]]></summary></entry><entry><title type="html">Defining Unique Identifiers</title><link href="https://jennajordan.me/blog/defining-unique-identifiers" rel="alternate" type="text/html" title="Defining Unique Identifiers"/><published>2019-07-11T00:00:00+00:00</published><updated>2019-07-11T00:00:00+00:00</updated><id>https://jennajordan.me/blog/defining-unique-identifiers</id><content type="html" xml:base="https://jennajordan.me/blog/defining-unique-identifiers"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_identifiers-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_identifiers-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_identifiers-1400.webp"/> <img src="/assets/img/header_identifiers.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Note</strong>: This was originally written as a final paper for IS590TH: Theories of Information, in May 2019. I’ve edited it slightly for this blog post.</p> <h2 id="introduction">Introduction</h2> <p>Generally speaking, a Unique Identifier (UID) is an inscription that represents (no more than) one entity within a given system. UIDs are essential to the functioning of modern information systems, so it is important to understand and define what a UID is and how it should be used.</p> <p>In <a href="https://doi.org/10.1371/journal.pbio.2001414">“Identifiers for the 21st Century”</a>, McMurry et al. provides guidelines for creating good unique identifiers. However, while they outline the desirable characteristics of a UID, they refrain from stating which characteristics are necessary for an identifier to truly qualify as a UID, and which are merely good practice. In <a href="http://dx.doi.org/10.1007/s11023-011-9230-6">“Sense and Reference on the Web”</a>, Halpin examines a specific type of identifier, the URI (uniform resource identifier), but also refrains from defining unique identifiers more generally outside of the context of the web.</p> <p>In this post, I will define unique identifiers by deducing their essential properties.</p> <h2 id="what-is-the-purpose-of-a-uid">What is the purpose of a UID?</h2> <p>Information systems exist to represent or describe some aspect of the real world. Entities are the specific things that we are describing through their attributes and relationships. For example, think of a person. You may wish to record specific attributes of this person, such as their name, age, eye color, hair color, height, etc. You may wish to describe their role within the system by establishing relationships with other entities - what company they are employed by, who their parents are, or what objects they own. But how can you identify this person? In natural language, you would probably use their name. But what if there are multiple people with the same name? You might add other attributes - “the Jane with the brown hair”, or “the Jane who works for B Corp.” You may need to use multiple attributes to identify the intended person. However, the combination of these attributes (which can uniquely identify Jane) is not a UID.</p> <p>Think about what is happening in your mind. You have a mental concept of the person, but you cannot telepathically transmit this mental concept to another person. So instead, you describe the person using attributes - things that you associate with your concept of the person. This person’s name is Jane. This person is a woman. This person has brown hair, and so on. You know that the other person you are conversing with also has a mental concept of Jane. If you settle on the right combination of attributes (which uniquely identify Jane for both of parties), then you will know that saying “Jane” refers to that specific person.</p> <p>Just as you cannot telepathically transmit your mental concept of Jane to another person, you cannot transmit this concept to a computer. However, you still want to create a record of Jane in the information system, including all of her attributes. Unlike a person, however, a computer is unable to form a mental concept. Instead, the concept of the person named Jane is represented in the information system by a UID. Let’s say Jane’s UID is <code class="language-plaintext highlighter-rouge">CH4TW1N</code>. You can tell the computer that <code class="language-plaintext highlighter-rouge">CH4TW1N</code>’s name is Jane, that <code class="language-plaintext highlighter-rouge">CH4TW1N</code> is a woman, that <code class="language-plaintext highlighter-rouge">CH4TW1N</code> has brown hair, and so on. Thus, a UID functions for a computer like a mental concept functions for a person. In <a href="https://en.wikipedia.org/wiki/Gottlob_Frege">Fregean</a> terms, a UID is a <a href="https://en.wikipedia.org/wiki/Proper_name_(philosophy)">proper Name</a>, with the entity it picks out as its referent. This is the UID’s fundamental purpose in information systems.</p> <h2 id="what-qualifies-as-a-uid">What qualifies as a UID?</h2> <p>Let’s first begin by describing an identifier.</p> <p>An identifier is the <strong>physical, inscribable representation of a concept</strong>. The most common form of a UID is an alphanumeric string stored on a computer. For example, take the alphanumeric string ‘CH4TW1N’. On a computer, this alphanumeric string has a specific encoding, and could be translated to some combination of 1s and 0s, which themselves are physically encoded in the computer’s hardware. A person could also physically write down ‘CH4TW1N’ with pen and paper, and it would still be an identifier. However, I could not simply say out loud the phonetic pronunciation of this string and expect it to be an identifier - it cannot be referenced at a later time (unless it was recorded, in which case it is still being physically encoded on some medium). So, the first fundamental trait of an identifier is that it must be an inscription in some form that can be referenced at a later time.</p> <p>Furthermore this inscription must pick out an entity. An entity could be a concept, a person, a physical object, a place, etc. Anything that has an independent concept in your mind is an entity; more commonly, in English a good rule of thumb is that entities are nouns (person, place, thing, or idea). When two entities are connected in some way, that connection is referred to as a relationship. Relationships are not entities; however, certain relationships may be instances of a concept, which may be an entity. For example, Jane’s brother is Martin. The Jane-Martin relationship is not an independent entity, because it cannot mentally exist without either Jane or Martin. However, the concept of being siblings can exist independently, and could be an entity. While translating relationships into entities can be quite complex, attributes are simpler. If Jane has brown hair, then ‘brown hair’ is an attribute of Jane. However, ‘brown hair’ is not exclusive to Jane - we can picture brown hair independently of Jane. Attributes can often be entities themselves.</p> <p>So, we understand what an identifier is. However, unique identifiers have two core additional properties: UIDs must be both <strong>unique</strong> and <strong>unambiguous</strong>.</p> <p>If a UID is <strong>unambiguous</strong>, any given UID points to exactly one entity (<a href="https://doi.org/10.1371/journal.pbio.2001414">McMurry et al</a>). For example, if there are two Janes at B Corp, then the identifier ‘Jane’ will pick out two separate entities. The identifier ‘Jane’ is ambiguous, and therefore not a UID. If a UID is <strong>unique</strong>, any given entity is assigned exactly one identifier (<a href="https://doi.org/10.1371/journal.pbio.2001414">McMurry et al</a>). If Jane is assigned the identifier <code class="language-plaintext highlighter-rouge">CH4TW1N</code> at B Corp, then finds a new job a F Corp where she has the identifier <code class="language-plaintext highlighter-rouge">W4TCH3R</code>. She later moves back to B Corp, and is assigned the new identifier <code class="language-plaintext highlighter-rouge">3L1Z4PM</code>. Jane now has two identifiers at B Corp. Thus, Jane does not have a unique identifier, by virtue of the fact that two identifiers will pick her out.</p> <p>The two core properties of uniqueness and unambiguousness imply two further properties - <strong>identifier stability</strong> and <strong>entity stability</strong>. An identifier is stable if it does not change over time (<a href="https://doi.org/10.1371/journal.pbio.2001414">McMurry et al</a>). Jane’s identifier at B Corp changed over time from <code class="language-plaintext highlighter-rouge">CH4TW1N</code> to <code class="language-plaintext highlighter-rouge">3L1Z4PM</code>, so it was not stable. An entity is stable if the entity picked out by an identifier does not change over time (<a href="https://doi.org/10.1371/journal.pbio.2001414">McMurry et al</a>). If, after Jane left B Corp, her <code class="language-plaintext highlighter-rouge">CH4TW1N</code> identifier was reassigned to Julia, then it is not stable. In other words, <em>an identifier’s uniqueness and unambiguousness must persist over time</em>.</p> <p>These properties of uniqueness, unambiguity, identifier stability, and entity stability can only be enforced if the UIDs exist within a system. Theoretically, this system would contain every possible UID, whether it is currently mapped to an entity or not. In this system, a UID could exist in one of three possible states: not yet assigned to an entity, currently assigned to a single entity, or retired (was assigned to an entity, which for some reason was removed from the system). A UID can only travel one way along this path - it cannot go backwards from being removed from the system to being assigned to entity. In this system, a UID isn’t really created or deleted, merely assigned. And <em>a UID cannot be reassigned</em>, because that would result in the entire system failing to meet the required properties of unambiguity and identifier stability.</p> <h2 id="how-is-a-uid-created">How is a UID created?</h2> <p>The fact that a UID must exist within a system of all possible UIDs implies that there are a finite number of possible UIDs within the system - and furthermore, that UIDs must obey certain rules.</p> <p>When a system is created in order to record information about entities (which will have UIDs), the set of all possible UIDs must be defined. In practical terms, this means that a UID can be defined by a regular expression; in linguistic terms, this means that a UID must be describable by a formal grammar.</p> <p>For example, let’s say that B Corp’s system defines a UID as a string of alphanumeric character of length 7. There are 26 letters and 10 digits, and 36^7 is 78,364,164,096. This system of UIDs can identify over 78 billion entities - more than sufficient. Most of these UIDs will never be assigned, and thus never need to enter the database. They will never leave the first stage. Some UIDs will be used to identify employees, and thus enter the second stage. The UID will be entered into a database and be assigned attributes with specific values. Some of these UIDs may be retired - for example, if an employee passes away. However, these retired UIDs must never be used again - which means that the physical system must still keep track of them, if only to prevent their reassignment.</p> <p>How these UIDs are actually generated is not an issue, so long as they follow these simple rules: <em>they must be formed from a formal grammar’s production rules, and once assigned to an entity they must never be reassigned</em>.</p> <h2 id="the-question-of-meaning">The question of meaning</h2> <p>It is generally regarded as bad practice to embed meaning within UIDs. When meaning is embedded in a UID, it may become easy to confuse a UID with an attribute. However, while a UID’s value could be considered an attribute of the entity, the UID’s purpose is to stand in for the entity itself - and thus is not an attribute. There is one fundamental distinction between UIDs and attributes: while an attribute’s value may change, a UID may not. Thus, if a UID’s value is based on an attribute’s value, and that attribute’s value changes, confusion may ensue. The UID cannot be changed, or the entire UID system will be invalidated. But a person looking at the UID may make the wrong assumption. Suppose, for example, that the first character ‘C’ in Jane’s UID means that she is a level C employee. But Jane recently got promoted to level B. Her UID cannot be changed and still be valid, but her UID also incorrectly implies that she is a level C employee.</p> <p>Encoding meaning in a UID is dangerous - but is it invalid? Even though someone might make the wrong assumption by only looking at Jane’s UID, so long as the UID remains unique, unambiguous, and stable over time, it is still a valid UID. Rather than a person making an erroneous assumption, the reverse should be true - individuals must disassociate meaning from the UID since it cannot be guaranteed to be correct. As <a href="https://doi.org/10.1371/journal.pbio.2001414">McMurry et al</a> recommend, “Meaning should only be embedded if it is indisputable, unchangeable and also useful to the data consumer.”</p> <h2 id="uids-can-only-be-unique-within-a-local-context">UIDs can only be unique within a local context</h2> <p>When a UID system is created, it is usually created to work within a specific context - a company, a university, a government database, etc. Outside of the system, a UID’s uniqueness cannot be guaranteed. However, data systems frequently need to be integrated. And so this system, this context, needs to be defined. This is frequently accomplished by assigning the context itself to a unique identifier (commonly denoted as a prefix followed by a colon). The prefix designates which system a UID belongs to, and thus becomes a part of the identifier itself. Together, all of the contexts and their prefixes form a new system - a new context that is the union of all sub-contexts contained within. And so, the prefix becomes a part of this new system’s UID formal grammar.</p> <p>If the prefix is to become part of the UID, then it too must follow the rules outlined thus far: to be unambiguous (one prefix designates exactly one context), to be unique (a context has exactly one prefix), and to be stable (prefixes do not change over time or refer to different contexts over time). This stability over time is what necessitates the use of prefixes - for even if the local UIDs happen to still be unique and unambiguous in the merged system, there is no guarantee that this will still be the case for future states of the system, as each local UID’s uniqueness is only guaranteed within its local system. Whenever one system is merged with another, namespaces and prefixes must be designated, or the identifier cannot qualify as a unique identifier.</p> <p>Unique context identifiers can ensure the new UID system is unambiguous (each UID picks out exactly one entity); however, ensuring that the new UID system is unique is more complicated. For example, suppose B Corp and F Corp are both acquired by L Corp, which combines all employee UIDs into one system using context identifiers. Suppose B Corp’s prefix is <code class="language-plaintext highlighter-rouge">B</code> and F Corp’s prefix is <code class="language-plaintext highlighter-rouge">F</code>. Suddenly, we have a problem. <code class="language-plaintext highlighter-rouge">B:CH4TW1N</code> and <code class="language-plaintext highlighter-rouge">F:W4TCH3R</code> both refer to Jane, and so the entire UID system no longer fulfills the uniqueness requirement.</p> <p>To solve this problem, L Corp should find the combination of stored attributes which can uniquely identify a person. For example, B Corp and F Corp will likely store attributes such as her first name, last name, and birthdate. Together, these attributes may uniquely identify an employee (though there is no guarantee). Or they may have stored other attributes for security purposes, such as fingerprints or a retinal scan. These are much more likely to guarantee uniqueness. Regardless, having a a combination of attributes that can uniquely identify an entity can help a system enforce its UID uniqueness. In database terms, this is known as a compound primary key. It’s also possible that a single attribute may uniquely identify Jane, such as her social security number - a UID from a much broader system. L Corp can now create a mapping from one context to another. However, this is only the first step in the solution.</p> <p>The real problem is that the two local contexts identify entities of the same type (person), thus creating the danger of overlap (non-uniqueness). It can be generalized that in a UID system that contains multiple contexts, besides the prefix itself being unique, the entity type that the prefix describes must also be unique. This means that in order for L Corp to have a UID system, they must create a formal grammar and then assign a new UID to each employee. L Corp may wish preserve the old B Corp and F Corp UIDs for historical purposes, and so may create mappings between the UID systems, but only by establishing a completely new set of UIDs can L Corp ensure that each employee has an unambiguous and unique identifier.</p> <p>Suppose that a UID system identified multiple types of entities - people, locations, objects, etc. If any one of these entity types overlaps with another UID system, the same process of (1) finding the combination of attributes that can uniquely identify an entity, (2) mapping between the systems, and then using that mapping when (3) creating a completely new UID system must be followed. (This also implies that it is good practice for a local UID system to identify one entity type, but this is not a necessary condition for UIDs.)</p> <p>It is important to note that there is no such thing as a truly global UID - nor is it desirable for there to be a global UID system (though that discussion is outside the scope of this post). For there to be a truly global UID, all entities - all information - would have to exist within a single information system. Even with the existence of the internet, this is not feasible. Therefore, the issue of context identifiers, namespaces, prefixes, and mappings between systems is unavoidable - any definition of a UID must discuss both local identifiers and context identifiers as components of an overall UID.</p> <h2 id="once-again-the-question-of-meaning">Once again, the question of meaning</h2> <p>For local (single context) UIDs, embedding meaning is strongly discouraged (though not disqualifying). However, when context identifiers and prefixes are added to the mix, embedded meaning quickly becomes impossible to escape. Many prefixes are acronyms or short words, describing either the organization that produced the UID or the type of entity that the UID set describes (or a combination of the two). Should meaning be a disqualifying characteristic of UIDs, no non-local UID with a context identifier could be considered a UID - and as I have already established, any definition of UIDs must consider context identifiers as an essential aspect of a UID. Therefore, while opacity is a recommended characteristic for UIDs, it is not a necessary property. So long as the UID still follows the production rules of its formal grammar, any meaning, purposefully embedded or not, is irrelevant.</p> <h2 id="contexts-may-change-over-time-and-other-attributes">Contexts may change over time, and other attributes</h2> <p>Despite the best of intentions, it is inevitable that a UID system will change over time. A UID may need to be reassigned, or the formal grammar may need to change. When this happens, a completely new UID system is created. UIDs cannot be reassigned, and the set of all possible UIDs have already been defined. By definition, should either of these things happen, it necessitates a distinct system and context, and thus a distinct prefix as well.</p> <p>This most frequently happens as a result of versioning. A common convention is to append the version number at the end of the original prefix (again embedding meaning in the context identifier). It is important to document this association, so as to avoid both contexts being used in a larger system (as this would introduce a uniqueness problem), as well as to provide a mapping from the old system to the new.</p> <p>This implies that in addition to context identifiers having a specify entity type (or set) that their local UIDs reference as an attribute, context identifiers also have a specific time period as an attribute. If B Corp changed their production rules (for example, they wish to make their UIDs 5 characters long instead of 7) on January 1, 2018, then the context identifier <code class="language-plaintext highlighter-rouge">bcorp</code> has the attribute of identifying entities of the type person and the attribute of identifying entities prior to 2018. The context identifier <code class="language-plaintext highlighter-rouge">bcorp_v2</code> has the attribute of identifying entities during and after 2018. Another property of context identifiers is the formal grammar used to generate all local UIDs. If this set of UIDs are stored on the web, another attribute of the context identifier may be its web address (URL).</p> <p>However, it is important to distinguish between a context’s attributes and a context’s identifier. An online location is an attribute, because it is not guaranteed to be stable over time. Within the semantic web community, a web address (URL) is often treated as a unique identifier. This is erroneous, as web addresses have the potential to change from one moment to the next. For a web address to be a UID, the entire system would have to be created anew every second. Perhaps this is feasible - for the web’s system to incorporate a timestamp into each context identifier, but it is certainly not practical or desirable. Furthermore, the semantic web community does not argue that this is the case - and so, we are left with the conclusion that web addresses (URLs) are not UIDs, both because they are not stable over time, nor can uniqueness be guaranteed (there is nothing restricting me from copying one webpage exactly at a different web address). I will repeat my point: location is an attribute, not a unique identifier.</p> <h2 id="the-issue-of-identity">The issue of identity</h2> <p>There is a larger issue at play here, which I will touch on only briefly - the issue of identity. This is absolutely core to the concept of unique identifiers, as each UID must pick out an entity with a distinct identity. However, it is generally accepted that entities may change - Jane can dye her hair blonde, change employers, move to a new location… even change her name. But you would not consider blonde Eliza of P Corp to be a different person. You could argue that she still has the same fingerprints, the same DNA, but even these can change over time. Eliza and Jane are still the same person. When it comes to physical entities like people, identity is fairly easy to grasp, if hard to define. However, identity is not nearly so easily traced when it comes to imaginary entities - countries, corporations, books, etc. When I say imaginary entities, I mean those things that would not exist without people to think them up. A tiger is a tiger, whether human beings ever evolved from great apes or not. But a corporation only really exists in society’s collective consciousness. It may be described on paper, have physical offices, and pay employees, but the moment everybody stops believing that the corporation exists, it will cease to exist as an entity (Yuval Noah Harari explores this concept in <a href="https://www.ynharari.com/book/sapiens/">“Sapiens”</a>). For these types of entities, identity is much harder to trace. What if the corporation changes its name? What if it fires all of its employees, sells all of its offices, and finds new ones? What if it sells a different product? Is it still the same corporation? What is the line that an entity must cross in order to assume a new identity, and thus a new UID? These are questions that must still be considered in order to have a full understanding of unique identifiers - for the meaning of the entity that a UID identifies is essential to the meaning of the UID itself.</p> <h2 id="key-takeaways">Key Takeaways</h2> <p>Definition of a UID:</p> <ol> <li>an inscription produced by a formal grammar</li> <li>refers to exactly 1 entity (unambiguous) at any given time</li> <li>its referred entity has exactly one UID (unique) at any given time</li> <li>the inscription persists over time (identifier stability)</li> <li>the entity to which it refers persists over time (entity stability)</li> </ol> <p><strong>Remember</strong></p> <ul> <li> <p>A UID may be decomposed into a context identifier and a local identifier. In this case, all context identifiers must also follow this definition, wherein the entity referred to is the set of local identifiers.</p> </li> <li> <p>When a local identifier violates any of the rules in the definition, a new context is created and a new context identifier must be defined.</p> </li> <li> <p>Opacity (lack of inherent or inferred meaning), while highly recommended for UIDs, is not necessary.</p> </li> </ul> <h2 id="citations">Citations</h2> <p>McMurry JA, Juty N, Blomberg N, Burdett T, Conlin T, Conte N, et al. (2017) Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data. PLoS Biol 15(6): e2001414. https://doi.org/10.1371/journal.pbio.2001414</p> <p>Harry Halpin. (2011) Sense and Reference on the Web. Minds Mach. 21, 2 (May 2011), 153-178. http://dx.doi.org/10.1007/s11023-011-9230-6</p>]]></content><author><name></name></author><category term="MSLIS"/><category term="musings"/><category term="essays"/><category term="data"/><summary type="html"><![CDATA[a conceptual analysis]]></summary></entry><entry><title type="html">MSLIS Year 1 - A Reflection</title><link href="https://jennajordan.me/blog/year-1-reflection" rel="alternate" type="text/html" title="MSLIS Year 1 - A Reflection"/><published>2019-06-04T00:00:00+00:00</published><updated>2019-06-04T00:00:00+00:00</updated><id>https://jennajordan.me/blog/year-one-reflection</id><content type="html" xml:base="https://jennajordan.me/blog/year-1-reflection"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_reflection-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_reflection-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_reflection-1400.webp"/> <img src="/assets/img/header_reflection.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It’s summer, and I have officially completed half of my Master’s in Library &amp; Information Science degree. This past semester has been extremely busy for me - not only was I taking more classes, but I was working 20 hours/week as a Graduate Research Assistant at the Cline Center on campus. And while that meant that I had essentially no free time (hence the utter lack of blog posts), there was nothing that I wanted to drop… so I can only blame myself. And next year looks to be at least as busy! But before I look ahead, I want to take some time to reflect on what I have learned and accomplished during my first year of library school.</p> <h2 id="degree-requirements">Degree Requirements</h2> <p>The MSLIS degree at UIUC has only two required classes (IS501 &amp; IS502), and I have now completed both. The degree also requires 40 credit hours, and I have now completed 24 credit hours - I’ll definitely be going well-over the credit hour requirement by the last semester. My cumulative GPA for the first year is 3.94 - despite an overloaded semester, I was able to earn straight A’s. Check, check, check - from an academic standpoint, my MSLIS degree is going very well.</p> <h2 id="technical-skills">Technical Skills</h2> <p>I chose UIUC and planned my classes with the express purpose of gaining and advancing my technical skills. I had a fairly good idea of what fundamentals were important to learn, and I could hold conversations about various technical concepts. I was familiar enough with the tech world to not be overly intimidated by the concepts we were learning about in class, but not enough to actually be able to accomplish something on my own. Now, just a year later, I have confidence in my ability to solve information problems. I feel like an information magician - in the same way that the magicians in my favorite fantasy novels can manipulate the physical world around them with spells, I can manipulate data in an information world with code - a very empowering feeling in our Information Age. I was able to reach this point by focusing my learning on 3 core concepts:</p> <h3 id="an-information-oriented-headspace">An information-oriented headspace</h3> <p>Philosophy has always been extremely interesting to me. I love having philosophical debates and learning about new philosophical ideas. There is nothing more important to learning than being able to think critically - and nothing teaches critical thinking skills like philosophy. I have a whole rant about how severely undervalued and underutilized philosophy is in the modern era - but I will save that for another time. The point is, philosophy is important because it is foundational to every field of study… whether you realize it or not.</p> <p>In the fall I audited a practical philosophy class, PHIL103: Logic &amp; Reasoning. I was introduced to propositional logic - propositions, validity, truth tables, translating arguments into the logical language, probability and the Bayes theorem. Essentially, I learned how to think in a purely logical way - the same way that computer “think”. I was cultivating a logical headspace that proved to be extremely useful in both my database and python classes.</p> <p>In the spring I took IS590TH: Theories of Information. This was a one-time, small seminar-style class taught by the iSchool Dean, Dr. Renear. The goal of the class was to answer one seemingly simple question: what is information? We learned about the technique of conceptual analysis - essentially defining an abstract concept - and slowly accumulated the building blocks we would need in order to define information. We read original works by Gottlob Frege, Alonzo Church, Bertrand Russell, Saul Kripke, Edmund Gettier, H. P. Grice, and Jonathan Furner. While the Logic class had introduced me to the concept of a proposition, in this class we dissected it - to a degree that I had not thought possible. We dissected a lot of concepts, and as a result gained a much deeper understanding of all the fundamental building blocks that make up the concept of information. This was a class that really got my brain fired up - what can I say? I like thinking deeply about abstract and obscure things. But as a result, the headspace I have been cultivating for working with information deepened, broadened, and became more connected. I am able to think critically about information and data in a way that I could not have imagined a year ago, and I feel prepared to tackle more advanced subjects because I have such a solid foundation in the study of information science.</p> <h3 id="organizing-information">Organizing Information</h3> <p>I knew from the beginning that databases were one of those fundamental technologies that I needed to learn how to use, simply because of how prevalent they are in storing information. But I had yet to learn just how important it is to properly organize and structure data so that it can be used properly.</p> <p>For my first semester, I took IS490DB: Introduction to Databases. My main goal was to learn just what databases are and how to access the information in them using SQL. The course, however, placed a lot of emphasis on database design - and in the end, that was far more fascinating to me than learning how to write SQL queries (though I certainly learned that as well). We learned how to design a database from scratch - modeling the information in terms of entities, attributes, and relationships and drafting a Chen-style ER diagram (and later EER diagrams), mapping to the relational model, building the relational model with a GUI and then finally with SQL. At every stage there are design decisions to make that fundamentally effect how users will interact with the information. Throughout the course we were taught the principles of normalization, but we weren’t formally taught about normalization and functional dependencies until the very end of the course… and at that point it just felt natural and instinctual to use those good database design principles. I was fully converted - relational databases are da bomb!</p> <p>During the spring I took IS561: Information Modeling. In this class, relational databases were just one technique of many for organizing information. And before you can organize information for use with a particular technology, you need to create a model of that information. Different types of information lend themselves well to different models and technology. We learned how to model documents using XML and how to craft the schema in a DTD; how to model networks using graphs and how the semantic web uses knowledge graphs formed from a combination of RDF syntax and OWL ontologies; we studied logic through phrase-structured formal grammars (BNF) and learned how to use first-order predicate logic (an expansion of propositional logic) and it’s applications in developing ontologies. Only at the end of the class did we learn about relational databases and how to map between the relational model and RDF. As it turns out, relational databases are just one of many ways to structure data - and the same information can be modeled using many different techniques and technologies, each with their advantages and disadvantages. And besides adding a bunch of modeling/organizational tools to my information magician toolbelt, I was also able to advance my understanding of formal logic. This was also a very project-oriented class, so I got a lot of hands-on practice using all of these tools.</p> <h3 id="processing-information">Processing Information</h3> <p>I knew from the start that learning how to program would be very important - but it was also something I was kind of dreading. I was afraid I would find it boring, or too difficult and complicated. You can’t really say you have technical skills in this age and not know how to program - and my whole goal for this degree was to gain and advance my technical skills. What if I simply didn’t enjoy it?</p> <p>As you can probably tell at this point, those fears were not realized. My first semester, I began learning how to program using the python language in IS452: Foundations of Information Processing. In this class, the emphasis was on using python to process information (vs building a software program). I had never considered software development to be very interesting, and I hadn’t really thought about other uses of programming languages outside of the realm of software development. But using it to process and transform data? That I found myself enjoying. I had already encountered the problem of having the data I needed but not in the right form for analysis (during my undergrad honors thesis). Having the power manipulate data structures and get the information I actually needed in a form that I could analyze? That was some powerful stuff. Besides learning how to use all of the basic data structures/techniques in python (lists, dictionaries, tuples, loops, functions, decision structures) we also learned how to read/write files (text, csv, json), navigate XML documents using XPath, and craft Regular Expressions. By the end of the course, I felt like I had a decent grasp of the base python language, and could use it to do some simple data wrangling tasks. But I did not yet have full confidence in my ability to handle any real information processing task independently - which is why the next class was so important.</p> <p>In the spring I took IS590PR: Programming for Analytics and Data Processing. This class was designed as a follow-up to IS452, and it was all about manipulating data structures to get the target information out of various public datasets. This class allowed me to build confidence in my ability to use python for practical data wrangling projects. Every assignment I started out unsure whether I would even be able to complete it, but by the end I had a solution that I was proud of - and by repeating that process over and over again, I gained confidence that even if I did not know immediately how to solve a problem, I could figure it out. Along the way I learned how to use several important python libraries (Pandas, Numpy), how to properly document code using docstrings and test code using doctests, and how to use classes to create object-oriented programs. We also covered several other topics, including XPath, regex, requesting data from an API, graphs and the NetworkX library, and efficiency/optimization using the Cython and Numba packages. The tool that was most heavily used in this class, however, was Pandas - a library I have come to appreciate greatly.</p> <h2 id="portfolio-projects">Portfolio Projects</h2> <p>One of the things that I most appreciate about this degree is how project oriented that classes are. Furthermore, many of my technical classes have very open-ended final projects - allowing me to work within the domain that I am interested in (political science, international relations). There are two major projects that I am especially proud of.</p> <h3 id="correlates-of-war-database">Correlates of War Database</h3> <p>During my first semester, I was able to combine the final projects for both my python and database classes. My goal was to create a relational database version of the various datasets that make up the Correlates of War project. I had previously used some of the CoW datasets for my undergraduate thesis - and had to go to the statistical consulting center to transform the data into the right form so it could be integrated with other datasets. The CoW project is widely used by political scientists studying international conflict, and I wanted to create something that would be useful for them. I knew that the CoW datasets could work so much better together if they existed within one cohesive database - but I had no idea just how difficult it would be to bring them back together. Over time, the datasets had been split apart and maintained by different parties, who made different design decisions. While the datasets could work together because they shared unique IDs for the various entities the project tracked (countries, territories, wars, etc), it was no longer possible to simply merge the datasets and move on.</p> <p>So on the database side, I had to design and create a database schema that would integrate all of the various datasets. On the python side, I had to transform the publicly available datasets into the format required for my database design. And since there is no better library for wrangling tabular data than Pandas, that meant that I had to start teaching myself Pandas before formally learning it in the Spring semester. It was a much more ambitious undertaking than I had expected - and I continued to work on the project even after turning in the components required by my classes. In fact, I plan to continue working on this project throughout my degree, expanding it to incorporate other datasets that use the CoW identifier system.</p> <p>If you’re interested in exploring this project more, please refer to its GitHub repo:</p> <p><a href="https://github.com/jenna-jordan/international-relations-database" class="btn btn--primary">International Relations Database GitHub Repo</a></p> <h3 id="prisoners-dilemma-monte-carlo-simulation">Prisoner’s Dilemma: Monte Carlo Simulation</h3> <p>This was the final project for my spring programming class, IS590PR. This project is significant to me for a few reasons - the amount of work I put into it, the game theory + poli-sci element, the potential for future academic work using this program… but mostly because it is the first time I have written my own classes. Up till now, including for all course assignments, I did everything procedurally. I did not foresee just how difficult it would be to wrap my head around programming with classes. They are just another data structure, yes, but they function in a fundamentally different way, and it took a lot of time and experimentation for me to be able to use classes properly (at a beginner level) and feel comfortable with the data flow.</p> <p>So what does this project do? Essentially, it runs an iterated prisoner’s dilemma tournament (you will find umpteen-million poli-sci journal articles on this topic) many times with several randomized elements in order to rank different strategies on their game score. The twist that makes this prisoner’s dilemma tournament unique is “reactive noise”: in addition to having a starting noise environmental variable, the noise level is incremented up for defecting plays and down for cooperative plays.</p> <p>If you’re interested in trying to run your own simulations using my code, look for the Quick Start Guide in the GitHub repo:</p> <p><a href="https://github.com/jenna-jordan/Prisoners-Dilemma" class="btn btn--primary">Prisoner’s Dilemma GitHub Repo</a></p> <h3 id="bonus-coming-soon">Bonus: coming soon</h3> <p>So the Correlates of War database project is a good showcase of “organizing information”, and the Prisoner’s Dilemma project is a good showcase of “processing information” - it’s only proper that I should also have a final project to showcase “an information-oriented headspace.” Well, it’s not really a project, or even really a proper academic paper, but I do have something for you.</p> <p>For my spring philosophy class (IS590TH), our single/final assignment was to write our own (or critique someone else’s) conceptual analysis on a topic relevant to library/information science. I chose to write a conceptual analysis (a.k.a. definition) on unique identifiers. For some reason, identifiers fascinate me - and I had devoted a lot of thought throughout the semester to understanding identifiers on a deeper level.</p> <p>So what are unique identifiers? Stay tuned to my next blog post to find out! I’ll be posting my paper in its full, rambling glory.</p> <h2 id="professional-development">Professional Development</h2> <p>As much fun as learning for the sake of learning is, I’m earning this degree so that I can get a job doing what I enjoy. Taking classes is only half the battle - I also need to gain practical on-the-job experience.</p> <p>During the fall semester, I had an hourly job and volunteered my time with different research projects in the iSchool. I talked with professors and told them what I wanted to do - work with data in the field of political science. Fortunately, many of those professors knew exactly where I had to go: the Cline Center for Advanced Social Research. The Cline Center is based in UIUC’s Research Park, and works on computational social science research with partners at the University and other organizations focused on providing data for political scientists. The Cline Center was the one place on campus that would provide me the opportunity to combine information science with political science. At the end of the fall semester, I found out that they were hiring a graduate research assistant. I owe it to several amazing professors at the iSchool who advocated for me and connected me with the folks at the Cline Center. The GRA-ship turned out to be a perfect melding of my various skillsets: I would be creating the documentation for the Cline Center’s Global News Index - a massive database of metadata and extracted features for international news articles.</p> <p>I was hired, and in January I got started writing the documentation that would allow the Cline Center to open up the database to the campus at large. Researching for and writing the documentation used my journalism skills (investigating algorithms and distilling complicated information into a concise and readable form) my political science and journalism domain knowledge, and my newly acquired understanding of information science (knowing what information about the variables and corpora was important to include for researchers from a wide variety of domains). I got to use InDesign to layout the documentation (and even learned some new tricks!), and learned that technical writing was definitely in my wheelhouse. By the end of the semester, after spending 20 hours every week working on this documentation, I had produced documentation for the global news index itself, as well as user guides for the applications used to access the database.</p> <p>I’m happy to say that next semester I will be continuing to work at the Cline Center as a Graduate Research Assistant, with my duties shifting from creating documentation to more directly assisting with the research conducted by the Cline Center and its partners. This GRA-ship was important to me for many reasons - not only did I gain experience in a new skill (technical writing), but I gained confidence in my ability to operate as a professional. I didn’t feel like a student, just there to help with menial tasks or shadow the real experts; I felt like a part of the team, contributing something valuable on my own merits. What’s more - I enjoyed my work. I even had a couple small opportunities to use my coding skills (which I enjoyed the most!). I’m so excited to see how I can further develop in my next semester.</p> <p>But before the fall semester arrives, there is summer - and what an exciting summer it will be! This summer I am in Gothenburg, Sweden, as a Data Science Intern for the Program on Governance and Local Development at the University of Gothenburg. I am working directly with GLD’s Data Scientist on a major project - a massive survey on local governance issues in multiple Southeast African countries. My internship started off in the best way possible… attending GLD’s annual conference (at an amazing spa hotel) to get a crash course on accountability research in international development. I got to meet both academics and practitioners from international aid organizations and enjoyed many interesting discussions. As for the internship itself, I’ve been able to use my python skills to process information and put it into the form our Data Scientist needs in order to run her analyses. I’m very glad that both of my programming instructors taught a unit on XPath - most of the information I’ve been processing comes from XML documents! I’m enjoying myself immensely, because every day has a new coding challenge - and it’s all real problems (not class assignments) that need to be solved. I’m so excited to be at an internship that uses the information processing and organization skills I’ve spent the past year acquiring, in the domain of international development. Plus… it’s Sweden! Living and working in a foreign country feels pretty normal at this point, but I am beyond stoked to experience life in a Scandinavian nation (especially considering the cool summers). Here’s to another year of professional development - hopefully, it leads to a job!</p>]]></content><author><name></name></author><category term="MSLIS"/><category term="life-updates"/><category term="musings"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">My Classes for Spring 2019</title><link href="https://jennajordan.me/blog/my-classes-for-spring-2019" rel="alternate" type="text/html" title="My Classes for Spring 2019"/><published>2019-01-29T00:00:00+00:00</published><updated>2019-01-29T00:00:00+00:00</updated><id>https://jennajordan.me/blog/my-classes-for-spring-2019</id><content type="html" xml:base="https://jennajordan.me/blog/my-classes-for-spring-2019"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_fall2018classes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_fall2018classes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_fall2018classes-1400.webp"/> <img src="/assets/img/header_fall2018classes.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It’s my second semester at UIUC! I learned a lot last semester, and I’m ready to learn even more. I also have to be prepared to be a whole lot busier… in addition to taking four courses this semester (4! 14 total credit hours! …That’s a lot in a master’s program!) I’m also working 20 hours a week as a Graduate Research Assistant (GRA) at the Cline Center. I think it’s a good problem to have that I want to be taking all of my classes and I got my dream assistantship, so there’s absolutely nothing I want to drop to make my life a bit easier this semester, but… boy have I been busy. I’m only starting week three and I don’t know how I’m going to get everything done.</p> <p>Speaking of classes, this semester the four courses I’m taking are:</p> <ul> <li>IS502: Libraries, Information &amp; Society</li> <li>IS561: Information Modeling</li> <li>IS590PR: Programming for Analytics and Data Processing</li> <li>IS590TH: Theories of Information</li> </ul> <p>Time to introduce these classes and what they’re all about!</p> <h2 id="is502-libraries-information--society">IS502: Libraries, Information &amp; Society</h2> <p>IS502 serves as a forum for critical discussions about issues in the profession of Library Science, and is one of two required courses for the MSLIS degree (the other being IS501, which I took in the Fall). Like IS501, IS502 introduces LIS students to the many different jobs that information professionals can have, but focuses on the commonalities critical to the profession: ethics, social responsibility, intellectual freedom, literacy, etc. The class takes the general format of a lecture followed by a class discussion of the assigned readings.</p> <h3 id="course-description">Course Description</h3> <blockquote> <p>Explores major issues in the library and information science professions as they involve their communities of users and sponsors. Analyzes specific situations that reflect the professional agenda of these fields, including intellectual freedom, community service, professional ethics, social responsibilities, intellectual property, literacy, historical and international models, the socio-cultural role of libraries and information agencies and professionalism in general, focusing in particular on the interrelationships among these issues.</p> </blockquote> <h3 id="central-themes--topics">Central Themes &amp; Topics</h3> <ol> <li>The Fields of Librarianship and Information Science</li> <li>Historical &amp; International Models</li> <li>Intellectual Freedom</li> <li>Professional Ethics</li> <li>Social Responsibilities &amp; Community Service</li> <li>Intellectual Property</li> <li>Literacy</li> <li>Social-Cultural Role of Libraries and Information Providers</li> <li>Future of Librarianship and Information Science</li> </ol> <h3 id="learning-objectives">Learning Objectives</h3> <ol> <li>The variety of library and information agencies that exist and their missions and historical connections.</li> <li>The definitions of what is an information professional and the role of differing information professionals in meeting the missions of information institutions</li> <li>Major issues in librarianship and other information professions, including, but not limited to intellectual freedom, community service, social responsibility, ethics, intellectual property, and literacy.</li> </ol> <h2 id="is561-information-modeling">IS561: Information Modeling</h2> <p>In order to best store information/data for future use, you need to understand how to best organize it. And to understand how to organize it, you need to know how to model it. This is a required course for the MSIM (Master’s in Information Management) program and the CAS in Digital Libraries, so I knew that it would be a good idea for me to take this class because I’m most interested in the more technical/digital side of things. But unlike the programming or database classes (which focused on hard skills), this class focuses on teaching the philosophical/conceptual foundations of how information is modeled/organized in today’s digital world, with the side-effect of teaching useful technologies like XML, RDF and OWL, UML diagrams, etc.</p> <h3 id="course-description-1">Course Description</h3> <blockquote> <p>An introduction to the foundations of information modeling methods used in current digital library applications. The specific methods considered include relational database design, conceptual modeling, markup systems, and ontologies. The basic concepts underlying these methods are, respectively, relations, entities, grammars, and logic. Implementations include relational database design, FR/EER/UML diagrams, XML markup languages, and RDF/OWL semantic web languages. First order logic is emphasized throughout as the foundational framework for information modeling in general, and for contemporary web-based information management and delivery systems (including semantic web technologies) in particular.</p> </blockquote> <h3 id="learning-objectives-1">Learning Objectives</h3> <p>Content Objectives</p> <ol> <li>Develop fluency in reading and understanding formal definitions.</li> <li>Understand the role of abstraction in making systems design choices.</li> <li>Contrast deep vs. superficial differences in modeling languages.</li> <li>Recognize practical implications of trading expressive power for tractability.</li> <li>Appreciate the fundamental role of a very small set of inter-related concepts.</li> </ol> <p>Teamwork and Communication Objectives</p> <ol> <li>Develop and practice strong teamwork skills.</li> <li>Develop and practice strong oral and written communication skills</li> </ol> <h2 id="is590pr-programming-for-analytics-and-data-processing">IS590PR: Programming for Analytics and Data Processing</h2> <p>IS590PR is the follow-up course to IS452 (which I took last semester), so it assumes that you have either a basic working knowledge of Python or experience with another programming language (as many of the MSIM students do). Whereas IS452 introduced all of the basic concepts of Python like working with strings &amp; integers/floats, for loops, lists, file reading &amp; writing, functions, booleans &amp; if/elif/else statements, dictionaries, and some extra stuff (working with csv, json, SQL, Xpath, regex)… IS590PR takes it a step further. You can see all of the stuff we’ll be learning down below under Course Topics (copied from the syllabus), but the basic format seems to be: read background in the textbook, listen to lecture on a specific topic, and complete a data processing programming exercise using real-world data. This class is definitely going to be challenging for me, but I expect to learn a lot and gain confidence in my ability to use Python to process data.</p> <h3 id="course-description-2">Course Description</h3> <blockquote> <p>Building on the fundamentals introduced in IS 452, this course adds skills, data structures, tools, and patterns needed for developing and modifying software to solve more complex problems and to improve code maintainability and reliability. These skills are relevant to many types of programming, but many scenarios used will involve data analysis, conversion, validation, and processing pipelines. The course helps prepare students for work on larger projects with multiple developers. Includes test-driven design, more OOP design concepts, refactoring, profiling, introductory parallel processing, and more. Primarily uses the Python language.</p> </blockquote> <h3 id="course-topics--learning-objectives">Course Topics &amp; Learning Objectives</h3> <p>Programming Concepts:</p> <ol> <li>Expand study of data structures and libraries beyond 452 (e.g. sets, queues, graphs, numpy ndarray, pandas DataFrame &amp; Series, etc.)</li> <li>Write functions including those that can accept a variable number of parameters</li> <li>Design object oriented classes, including operator overloading, encapsulation, abstraction, static methods, inheritance, and polymorphism</li> <li>Functional programming concepts: first-class functions; lambda functions; map, reduce, and filter functions; recursion; closures</li> <li>Algorithmic complexity and performance measurements; Big-O/Big-Q/Big-W notation, profiling, selective compilation</li> <li>Multi-threaded vs. multi-process design, locking, messaging</li> <li>Monte Carlo simulation techniques</li> <li>Designing and using multi-stage data pipelines; incorporating non-Python phases</li> </ol> <p>Software Development Practices &amp; Tools:</p> <ol> <li>Program quality considerations and code reviews; Refactoring</li> <li>Programming Teamwork</li> <li>Test-Driven Development: Using &amp; writing automated tests, the TDD process and continuous integration.</li> <li>Version control/source code mgmt.: using git and GitHub. Do checkout, commits, versioning, branching, and merging; Compare distributed vs. centralized VCS tools for context.</li> <li>Debugging and Tracing Tools</li> <li>Code profiling and timing for performance and bottleneck analysis</li> <li>Full-featured Integrated Development Environments (IDE)</li> <li>Jupyter Notebook (common in data science)</li> </ol> <h3 id="textbooks">Textbooks</h3> <p class="small"><br/> <strong>“Programming in Python 3”</strong> (2nd Ed), by Mark Summerfield</p> <p><a href="https://www.oreilly.com/library/view/programming-in-python/9780137155149/" class="btn btn--primary">Publisher’s Site</a></p> <p class="small"><strong>“Python for Data Analysis”</strong> (2nd Ed), by Wes McKinney</p> <p><a href="http://shop.oreilly.com/product/0636920050896.do" class="btn btn--primary">Publisher’s Site</a></p> <h2 id="is590th-theories-of-information">IS590TH: Theories of Information</h2> <p>Maybe I shouldn’t play favorites with classes but… so far this class is definitely my favorite. It’s essentially a philosophy course, but with the topic of philosophical inquiry being information - and shouldn’t all information professionals know what “information” is? This is not your typical iSchool class: it’s small (about 10 people), very participatory, and it’s taught by the iSchool’s Dean - who has his Ph.D. in philosophy. As far as I know this is the first time this class has been offered (though I hope it becomes a regular part of the course catalog). The readings for the class (Frege, Church, Grice, Russell, Kripke, Gettier, Searle, Furner) are all works of philosophy, but with a direct relevance to the concept of information, and thus also to how we organize information (and thus how we can use information). I can say that I am taking this class because I firmly believe in having a strong philosophical grounding in your profession (which I do) but honestly… I’m taking this class because it’s <em>fun</em>.</p> <h3 id="course-description-3">Course Description</h3> <blockquote> <p>A theory of information attempts to articulate clearly and precisely what information is, and what it means to become informed. Theories of information can contribute to the scientific foundations for many important research and practice activities in LIS, including data curation, information modeling, information access, digital preservation, and informatics support for science and scholarship. This course, Theories of Information – A, takes a logic-based approach to investigating the nature of information. Methodologically we draw from a family of methods that might be called formal methods, in contrast to the empirical methods of social and nature science. Formal methods typically make use of concepts from logic, set theory, and discrete mathematics to construct and explore formal systems. Formal methods are widely used in linguistics, mathematics, philosophy, and computer science. Within the general area of formal methods our approach in this course might be more specifically referred to as conceptual analysis, as it takes the form of a systematic analysis of a concept, namely information. Most of the prior work that is relevant to our analysis is from analytic philosophy, linguistics (especially formal semantics), and computer science (especially knowledge representation and AI).</p> </blockquote> <h3 id="topics-to-be-covered">Topics to be covered</h3> <ol> <li>Conceptual Analysis</li> <li>Ontology</li> <li>Propositions</li> <li>Communication</li> <li>Semantics</li> <li>Names</li> <li>Epistemology</li> <li>FRBR (Functional Requirements for Bibliographic Records)</li> </ol> <h3 id="readings-a-sampling">Readings (a sampling)</h3> <p class="small"><br/> <strong>“Sense and Nominatum”</strong> (1872) and <strong>“The Thought: A Logical Inquiry”</strong> (1918) by Gottlob Frege</p> <p class="small"><strong>“Propositions and Sentences”</strong> by Alonzo Church (1956)</p> <p class="small"><strong>“Meaning”</strong> by H.P. Grice (1957)</p> <p class="small"><strong>“On Denoting”</strong> by Bertrand Russell (1905)</p> <p class="small"><strong>“Naming and Necessity”</strong> by Saul Kripke (1872)</p> <p class="small"><strong>“Is Justified True Belief Knowledge”</strong> by Edmund Gettier (1963)</p> <p class="small"><strong>“Information Studies without Information”</strong> by Jonathan Furner (2004)</p>]]></content><author><name></name></author><category term="MSLIS"/><summary type="html"><![CDATA[what classes I took for my 2nd semester of grad school]]></summary></entry></feed>