<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jennajordan.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jennajordan.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-31T23:28:53+00:00</updated><id>https://jennajordan.me/feed.xml</id><title type="html">blank</title><subtitle>I do data stuff with python + SQL. Learned to code at library school. Knitter. Dog mom. Nerdy for sci-fi/fantasy, board games, and well-wrangled data. </subtitle><entry><title type="html">So you want to build a data mesh</title><link href="https://jennajordan.me/blog/data-mesh-dbt" rel="alternate" type="text/html" title="So you want to build a data mesh"/><published>2025-10-15T00:00:00+00:00</published><updated>2025-10-15T00:00:00+00:00</updated><id>https://jennajordan.me/blog/data-mesh-dbt</id><content type="html" xml:base="https://jennajordan.me/blog/data-mesh-dbt"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_data-mesh-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_data-mesh-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_data-mesh-1400.webp"/> <img src="/assets/img/header_data-mesh.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>The Coalesce session recording is now available! See the <a href="#appendix">Appendix</a></p> </blockquote> <p>During my time as a consultant with Analytics8, I worked with two different companies who were both implementing dbt Mesh. I learned so much from both of those engagements, both from my very smart colleagues at both Analytics8 and the clients, and from the problems I had to solve while implementing dbt Mesh. I also started learning more about data mesh, and added the Data Mesh bible to my bookshelf. I saw the chaos that could result from implementing dbt Mesh simply as a convenient higher level of organization, and I became convinced that one of the best ways to implement dbt Mesh was with data mesh as a grounding, opinionated philosophy.</p> <p>Data Mesh exploded into the data scene in 2022 with Zhamak Dehghani’s book “Data Mesh: Delivering Data-Driven Value at Scale”.<d-footnote>Every quote throughout this blog post, unless otherwise noted, is from this book.</d-footnote> Zhamak characterized data mesh as a “decentralized sociotechnical approach to share, access, and manage analytical data in complex and large-scale environments – within or across organizations.”<d-cite key="Dehghani_2022"></d-cite></p> <p>She clarifies that data mesh is about analytical data, not operational data, and that it is an approach to data management, not an architecture. It belongs properly as part of an organization’s data strategy, and provides a target state to iterate towards for both the organizational operating model (socio) and enterprise architecture (technical).</p> <p>Data mesh is defined by its 4 core principles, which when applied can shift how data is managed organizationally, architecturally, technically, operationally, principally, and infrastructurally. And perhaps most importantly, data mesh is not for every organization - it is only intended for organizations that have reached a certain scale and maturity (and therefore complexity) in their data practices.</p> <p>Zhamak is purposefully tech &amp; tool agnostic in her book, though she went on to found her company Nextdata to create the tech to best implement her vision of a data mesh. This means that as long as you are following the 4 principles of data mesh, you can use any set of tools you want to build your own data mesh - including dbt.</p> <p>We are going to explore these 4 principles through the lens of dbt, with the goal of outlining an opinionated approach to how you should architect your dbt Mesh of project(s). While dbt is extremely flexible in how it can be implemented, often a strong structure and grounding philosophy can be more helpful than endless possibilities.</p> <p>Much like data mesh, dbt Mesh should really only be considered if your organization has reached a certain scale and maturity in your data practices. Like data mesh, dbt Mesh was designed to solve problems encountered by organizations maintaining very large and very mature dbt projects. If this is not you, congratulations: you don’t have to deal with all of the additional complications data/dbt mesh introduces for a while yet. Enjoy it while you can!</p> <h3 id="a-brief-history-of-data-mesh-and-further-resources">A brief history of data mesh (and further resources)</h3> <p>Zhamak first started writing about Data Mesh in 2019, when she published “<a href="https://martinfowler.com/articles/data-monolith-to-mesh.html">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>” on her ThoughtWorks colleague Martin Fowler’s blog. She followed this up with “<a href="https://martinfowler.com/articles/data-mesh-principles.html">Data Mesh Principles and Logical Architecture</a>” in 2020, and then fully articulated the data mesh approach in her 2022 book “<a href="https://www.thoughtworks.com/en-us/insights/books/data-mesh">Data Mesh: Delivering Data-Driven Value at Scale</a>”.</p> <p>Data mesh made it onto Gartner’s 2022 Hype Cycle for Data Management in the “Innovation Trigger” phase (the first phase), but was predicted to become obsolete before reaching the final phase, the “Plateau of Productivity”.</p> <p>Data mesh quickly gained a following among data practitioners who believed this approach could solve many of the pain points they had been experiencing in managing data at very large organizations. The <a href="https://datameshlearning.com/">Data Mesh Learning Community</a> was founded in 2021 to help practitioners connect and learn about data mesh from each other.</p> <p>After Zhamak’s book was released in 2022, she quit her role at ThoughtWorks to found her own company, Nextdata, to create her ideal implementation of a data mesh architecture. For 3 years they were quietly building, and meanwhile many in the data community declared that data mesh was dead (as they are often wont to do) - that while it sounds great in theory, it is impossible to actually implement, and data teams should instead adopt pieces of the data mesh strategy and then move on. But in April 2025 Nextdata publicly launched their product <a href="https://www.nextdata.com/product">Nextdata OS</a>, an operating platform purpose-built for autonomous data products. Data mesh is once again in the public dialogue - it seems to have survived the “trough of disillusionment” and is now climbing the “slope of enlightenment” (though I suppose we’ll have to wait for next year’s hype cycle report to see if Gartner agrees).</p> <p>Data mesh as a concept has also evolved as the thought leadership on data mesh has expanded beyond Zhamak to include others like Jean-Georges Perrin (JGP), who co-authored “<a href="https://www.oreilly.com/library/view/implementing-data-mesh/9781098156213/">Implementing Data Mesh</a>” with Eric Broda in 2024 and authored the soon-to-be-published “<a href="https://www.oreilly.com/library/view/building-data-products/9798341629394/">Building Data Products</a>”. JGP’s focus is primarily on the principle of data as a product, and you can read more about his approach in the blog post “<a href="https://dataintelligenceplatform.substack.com/p/the-next-generation-of-data-platforms">The Next Generation of Data Platforms is Data Mesh</a>”. JGP is also the chair of the Linux Foundation’s <a href="https://bitol.io/">Bitol</a> project, which has published the <a href="https://bitol-io.github.io/open-data-product-standard/latest/">Open Data Product Standard (ODPS)</a> and <a href="https://bitol-io.github.io/open-data-contract-standard/latest/">Open Data Contract Standard (ODCS)</a>.</p> <h3 id="why-data-mesh-and-dbt">Why data mesh and dbt</h3> <p>Data mesh and dbt both share a single fundamental mission: bringing software engineering best practices to data engineering. With <a href="https://www.getdbt.com/blog/data-engineering">dbt</a>, analytics engineers got version control, CI/CD, isolated development and production environments, testing, and documentation out of the box. Similarly, Zhamak positions data mesh as bringing the decentralization revolution in software engineering (microservices architecture) to the data world. Notably, both data mesh &amp; microservices architecture were deeply influenced by Eric Evans’ book <a href="https://domainlanguage.com/ddd/">Domain-Driven Design</a>.</p> <p>While the product name choice of “dbt Mesh” may make you think that it is dbt’s solution to data mesh, there have actually only been a few explicit lines drawn between dbt Mesh and data mesh. For the most part, <a href="https://docs.getdbt.com/best-practices/how-we-mesh/mesh-5-faqs">dbt Labs</a> maintains that dbt Mesh is a natural evolution of dbt as enterprises scaled up from a single dbt project into needing multiple interconnected projects. However, <a href="https://www.linkedin.com/posts/activity-7284688805645697024-cQFA?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAiK2FABKHqWAaEnq7CdATQwY_GSauRe0Xg">Hope Watson</a> (a dbt Labs Resident Architect) made the connection between data mesh and dbt Mesh explicit in her training course “<a href="https://www.loom.com/share/ea99434045ab4d1da4f794cee9e28500?sid=134d190f-c84e-420c-9ed9-1b185b1bed29">Data Mesh with dbt Cloud for dbt Practitioners</a>.”</p> <p>dbt Mesh’s product history reflects this natural evolution, and I could not have always recommended dbt as a way to implement data mesh. There are still several serious limitations that will influence your dbt Mesh architecture, but I also hope that the perspective in this blog post - dbt Mesh as a way to implement a version of data mesh - can influence the dbt Mesh product roadmap enough that these shortcomings will eventually be resolved (read: dbt product and developer experience teams, this is for you too!).</p> <p>dbt Lab’s product strategy has also shifted to better enable a data mesh implementation - beyond support for Iceberg and Cross-Platform Mesh, the metadata capabilities unlocked by dbt Fusion are hugely important for enabling key features needed in a data mesh. We’ll cover the most important feature, state-aware orchestration, in the section on the principle of domain ownership.</p> <p>One final note: for the most part, folks implementing data mesh using dbt will be dbt Labs Enterprise customers. Most dbt Mesh features require an Enterprise or Enterprise+ plan. However, if you are really set on a data mesh with dbt core, it is not an impossibility - you will just need to work a bit harder and figure out your own setup. I’ll point you to the <a href="https://github.com/nicholasyager/dbt-loom">dbt-loom plugin</a> for cross-project dependency management and <a href="https://dagster.io/">Dagster</a> for <a href="https://github.com/cnolanminich/dbt-loom-example/tree/bi-directional-data-mesh">multi-project orchestration</a> (see a <a href="https://www.loom.com/share/059ce1e196834fc6a427de071fb9cc24">demo</a> of how it works!).</p> <h2 id="the-4-principles-of-data-mesh">The 4 principles of data mesh</h2> <p>Data mesh is defined by 4 principles that work in concert:</p> <ol> <li>Domain Ownership</li> <li>Data as a Product</li> <li>Self-serve Data Platform</li> <li>Federated Computational Governance</li> </ol> <p>Each of these 4 principles can guide how you should implement dbt Mesh for distributed data management at scale. Let’s take them one at a time.</p> <h3 id="domain-ownership">Domain Ownership</h3> <p>The principle of Domain Ownership is the first stumbling block for organizations trying to implement data mesh, because it requires organizational change. While other principles can be (mostly) accomplished by technologists, this principle requires buy-in from leadership and long-term investment in data mesh as the data strategy.</p> <p>Rather than having a central data team that serves all departments, organizations should decentralize the ownership of analytical data to the business domains, and ensure that team structure is aligned by domain. Each business domain (already its own organizational unit) should be responsible for curating, maintaining, and sharing the data that it is most familiar with.</p> <p>The implication here is: if your organization currently has a single centralized data team, and cannot or does not wish to functionally have many small domain-oriented data teams, then your organization is not ready for data mesh (and that’s okay!). But if your organization is already organized by domain with data practitioners embedded, they are probably itching for ownership over their own data and the support to do so successfully.</p> <h4 id="applied-to-dbt">Applied to dbt</h4> <p>Simply put, the recommendation here is simple: align your dbt projects to your domains. Each business domain, with its own set of self-sufficient data practitioners, should have ownership over their own dbt project.</p> <p>Of course, nothing can be simple when you are playing in the waters of distributed systems. Up until very recently, it was not possible (and then not recommended) to completely align your dbt projects to your domain teams due to complications around bidirectional project dependencies.</p> <p>When dbt Mesh was first announced at Coalesce in 2023, dbt Labs advocated a hub-and-spoke model, where a central data team still maintained the “hub” project, but now downstream domain teams could use models from the “hub” while developing their own custom models in isolated “spoke” projects. This architecture was necessitated by the fact that project dependencies could only go one way - the “hub” project could never depend on models from one of the “spoke” projects. Not very meshy, was it? In a data mesh, domain teams should be able to serve and consume data products to and from each other.</p> <p>This meant that, functionally, all dbt projects had to be a DAG (directed acyclic graph) as well. As a result, organizations were either forced into a hub-and-spoke strategy (though this likely was not an issue for organizations wanting to continue with having a single central data team), or domain-oriented projects would get split up into “producer” and “consumer” projects.</p> <p>Fortunately, after product feedback from customers (including yours truly), dbt Labs made bidirectional project dependencies possible in 2024, with the feature generally available in October along with a suite of other mesh-related product announcements at Coalesce (notably cross-platform Mesh). While all of the models across all of the projects still had to adhere to the “acyclic” rule of DAGs, the projects themselves no longer had to form a DAG.</p> <p>But - and this is a very big but - there was still no elegant way to orchestrate for these bidirectional dependencies. dbt Labs knew that they would need to figure this out, and pre-emptively <a href="https://youtu.be/KB3Xrhvg-_0?si=0-ZZmh3x7U6c5QYY&amp;t=829">announced adaptive jobs</a> as a feature that would be coming soon, but until then it would be inadvisable to architect a dbt Mesh with bidirectional project dependencies.</p> <p>We finally got resolution for this story in May at dbt’s Launch Showcase with “<a href="https://docs.getdbt.com/docs/deploy/state-aware-about">State Aware Orchestration</a>”, a feature of the new dbt Fusion engine… which as of this week at Coalesce 2025 is now in public preview! This means that there is now a way to orchestrate for dbt Meshes with bidirectional project dependencies, but only if you are also able to transition over to the dbt Fusion engine. dbt Fusion and state-aware orchestration are also not yet generally available, so implement with caution (and report bugs).</p> <p>Now that we’ve covered some of the major dbt Mesh product developments that have resulted in the fact that I can finally now recommend organizing your dbt projects according to the first data mesh principle of domain ownership, let’s talk about the other complicating factor: your organization. There are two different organizational profiles that are likely to implement dbt mesh:</p> <ol> <li>The organization that has been using dbt for years, with model counts ballooning into the thousands and DAG that looks like the Cthulu version of a spaghetti monster, finally needing to split their single project up into many for the sake of everyone’s sanity</li> <li>The organization that is new to adopting dbt but knows they either want a data mesh or are already operating with a data mesh strategy, and therefore want many domain-aligned dbt projects</li> </ol> <p>Both types of organizations may read my recommendation of “align your dbt projects to your domains” and say, “yes, let’s do exactly that!” and immediately proceed to create many domain-aligned dbt projects. To which I must say… “wait! Not yet.”</p> <p>Cue the disgruntled groans and disappointed sighs. To the organizations of the former type - those with old and overgrown dbt projects - you will only need to take a short pause. To the organizations of the latter type - those new to dbt - you will likely need a much longer pause (but only a pause! I promise you can get to multiple projects soon enough). Both are on the same journey, but one is much further along, while the other can get to the final goal (true mesh with many projects) much faster but still needs to go on the journey of adopting dbt in the first place.</p> <p>I know that dbt Mesh sounds very exciting and promising (and really, it is), but it also introduces a lot of complications that you really should not want to deal with until you absolutely have to. As dbt Labs DevEx advocate Benoit so elegantly <a href="https://getdbt.slack.com/archives/C04FP5LQA15/p1756225114294919?thread_ts=1756198154.211049&amp;cid=C04FP5LQA15">stated</a>, “dbt mesh is about adding <strong>good</strong> friction to the data transformation process.” Adopting dbt introduces enough friction in the first place, and jumping straight to dbt Mesh may introduce too much friction - enough that the transition may fail entirely.</p> <p>All of this to say: if your organization is new to dbt, start in a single project. You can just architect this single project with the goal in mind that you will eventually transition to multiple projects. The adage of “slow is smooth, and smooth is fast” comes to mind - I promise that you will have a smoother time adopting dbt if you start with one project, and you will get to a <em>successful</em> dbt Mesh implementation faster than if you had started with multiple projects.</p> <p>So, let’s talk about how to design your single dbt project such that the transition to multiple dbt projects will be as painless as possible. Type 1 organizations (those with old &amp; overgrown projects), this is what you want to transition your existing dbt project to look like. Type 2 organizations (those new to dbt), this is how you want to design your new dbt project from the ground up.</p> <p>First, use <a href="https://docs.getdbt.com/reference/resource-configs/group">groups</a> to plan out your projects. Before creating one dbt project per domain, create a group per domain, and ensure every model belongs to a group. Planning is cheaper than doing (see: <a href="https://www.penguinrandomhouse.ca/books/672118/how-big-things-get-done-by-bent-flyvbjerg-and-dan-gardner/9780771098437">How Big Things Get Done</a>), and groups provide a cheap (time is money!) way for you to plan out (and iterate!) your domain-oriented projects. Trust me - it is way easier to change what group a dbt model belongs to than the project it belongs to, and it is way way easier to change what groups exist than to change what projects exist. The group config can be assigned by folder in the dbt_project yaml file, and overridden on a model-by-model basis.</p> <p>Next, align your folder structure to your groups. You may already have a well organized dbt project and find that all models in the same folder belong to the same group - in that case, assigning groups and consolidating the folder structure will be very easy! But if you find that you need to assign groups on a model-by-model basis, then you are going to need to spend some time reorganizing your models and folder structure.</p> <p>The typical folder structure for a single dbt project is to organize by pipeline stage first - staging, intermediate, and marts as the top-level folders in the model folder. Then you should organize by source or domain inside those folders. As you get closer to migrating to multiple projects, however, I recommend flipping that structure: your domain (a.k.a all models in a single group, a.k.a all models that will be split out into their own project) should be your top level folders, with the pipeline stage folders (staging, intermediate, marts) replicated inside each top-level domain folder. If you are starting out in a new dbt project that you know will be split out later on, I recommend using this configuration to start with. This structure of top-level domain folders should be replicated across all of the top level dbt project folders (models, seeds, tests, macros, analyses).</p> <p>Once your project is organized by domain folders, you can start enforcing domain ownership within your git workflow by implementing a <a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners">codeowners</a> file. Create teams in your git platform of choice that aligns with the domain-oriented team that will be owning the future domain-oriented dbt project (a.k.a the dbt groups). Any changes to folders belonging to that team must be approved in a pull/merge request by the team that owns it. Similarly, you can now start to slowly shift more and more dbt project ownership activities to these domain teams, like maintaining orchestration jobs, monitoring for test failures, etc.</p> <p>It can be easy to underestimate just how much work it is to manage a dbt project. It requires more advanced skills in git, dbt, and cloud platforms, and by slowly transitioning these responsibilities one at a time, you can provide a safe space for domain teams to advance these skills and figure out for themselves how they want to distribute responsibilities. This is one of the primary reasons I strongly encourage organizations new to dbt to start with a single project: to centralize and thus reduce this burden until enough people have the skillset required.</p> <p>Assigning your dbt models to groups provides another key advantage in drafting out your dbt projects: enforcing model access. By default, models have an access level of “protected” - they can be referenced by any other model in the same project. Once you have implemented dbt groups, you should change the default access level for models in your project to “private”, limiting only models within the same group to reference each other. Then, you will want to decide which models should be referenceable by models in other groups (and eventually other projects), and assign them to have an access level of “public”. These are the models that will turn into “data products” - more on that in the next section.</p> <p>You may encounter some errors in this process, as you realize that models with access level “private” were actually being used by other groups. This is a good thing! You are finding these errors now, while everything is still in the same project and errors are easier to resolve through refactoring or configuration changes, versus after migrating models into many projects (after which it becomes much more difficult to refactor and resolve).</p> <p>At some point, your groups will reach a point of stability, your domain teams will reach a point of self-sufficiency, and your public models will be mature enough that you can finally say: we are ready. We planned, we drafted, we prepared, and now we are ready to migrate into multiple projects. Even if domains do not reach this point on the same timeline, if you have followed my recommendations on your dbt project structure, pulling a single domain out into its own project one-at-a-time should be relatively easy. If you want to do it all at once, I recommend you check out <a href="https://github.com/dbt-labs/dbt-meshify">dbt-meshify</a>.</p> <p>Once you reach the end of this process and all of your domain teams have moved into their own dbt projects, don’t be so fast to deprecate the original project. You may have new domains who need help getting started with their own project, and the original project can continue to serve as an incubator - a project that is already set up, fully functional, and just needs the model files created. Much like when your organization was in the mesh transition phase, the original project provides a safe place for domain teams new to dbt to gradually learn how to manage their own projects. Furthermore, there may be a need for models that don’t fit neatly into one of the domains. The original project is a convenient place to stash them until ownership can be decided.</p> <h3 id="data-as-a-product">Data as a Product</h3> <p>The principle of Data as a Product is the one that has received the most attention and may be considered the most valuable or pressing to adopt. And while there is a lot to say about this principle and how to implement it with dbt Mesh, I will remind everyone that the four principles are designed to work together: if you focus only on this principle and forget the others, you may be producing data products but you will not be following the data mesh approach.</p> <p>According to this second principle, we should apply <em>product thinking</em> to domain-oriented data. Zhamak cites Marty Cagan’s book “<a href="https://www.svpg.com/books/inspired-how-to-create-tech-products-customers-love-2nd-edition/">Inspired</a>”, which describes successful products being feasible, valuable, and useful. Data products should likewise have a common set of characteristics that make them usable, valuable, and feasible - and Zhamak outlines the eight characteristics she believes are fundamental to data products:</p> <ol> <li>Discoverable</li> <li>Addressable</li> <li>Understandable</li> <li>Trustworthy &amp; Useful</li> <li>Natively Accessible</li> <li>Interoperable</li> <li>Valuable on its own</li> <li>Secure</li> </ol> <p>These eight characteristics are inclusive of and expand beyond the older concept of <a href="https://www.go-fair.org/fair-principles/">FAIR data</a> - Findable, Accessible, Interoperable, &amp; Reusable - to include the traits necessary in a distributed system. (However, FAIR is a much easier acronym to remember than DAUTNIVS!).</p> <p>Beyond these characteristics and the technical methods to implement them, Zhamak highlights the cultural shifts needed to truly treat data as a product, vs treating data as an asset. Success should not be measured simply by vanity metrics like how many “data products” exist or the volume of data (measures that assume data is valuable by virtue of its very existence), but rather in how many people are using the data products and their satisfaction with the experience of using the data. Quality data products need continuous care and curation, and furthermore, this care should be administered by the domain-oriented teams that are producing it in the first place, not by downstream centralized data teams that often lack needed context for the data.</p> <p>In other words, you cannot simply declare a table in one of your mart schemas to be a data product by virtue of its existence. That is “data as an asset” thinking, not “data as a product” thinking. Instead, you should carefully consider how (and whether!) you can evolve this mart model to meet each of the eight characteristics of a data product.</p> <h4 id="applied-to-dbt-1">Applied to dbt</h4> <p>Not every model in a dbt project should be considered a data product - nor do you want them to be. So, which models are our candidate data products? Most likely the models living in your marts (or equivalent) layer, which have an access level of “public” (assuming you followed the recommended steps from applying the first principle of Domain Ownership to dbt). These are the models that live at the intersection of projects, the models that other projects will use as cross-project references. These are the models that other teams are going to use - teams that lack the domain knowledge and development context that your team has. All of this means that these models should adhere to a stricter set of rules and documentation standards than models reserved for internal use. Your goal should be for another domain to successfully use your data product without ever needing to contact your team. So, let’s use the eight fundamental characteristics of data products to determine the rules mart models should follow in order to evolve into true data products.</p> <h5 id="discoverable">Discoverable</h5> <p>When a user is browsing a catalog of data products and needs to find one that will suit their needs, a discoverable data product can be easily surfaced during this search. The data product itself should include the necessary and standardized metadata needed to enhance discoverability. In dbt, this metadata is provided in the model yaml file. While dbt does not require any metadata to be provided, there is a standard set of <a href="https://docs.getdbt.com/reference/model-properties">properties</a>, as well as the extremely flexible <a href="https://docs.getdbt.com/reference/resource-configs/meta">meta</a> config. Your team should decide on the minimum acceptable metadata needed for a mart model. Furthermore, the more detailed your descriptions, the more discoverable your model will be. Having a controlled vocabulary can also aid users in discovering the right data products. Try putting your librarian hat on when crafting the metadata for your models ;)</p> <h5 id="addressable">Addressable</h5> <p>Functionally, this means that a data product should have a permanent and unique address (and/or id). You get this by default with dbt, as all model names must be unique within a project. The model name also serves to group together all relevant information about the model in the catalog. One implication of this, however, is that once a mart model is considered a data product, its model name should not change. The relation name in the database can change via the <a href="https://docs.getdbt.com/docs/build/custom-aliases">alias</a> config, but the model name should remain consistent. <a href="https://docs.getdbt.com/docs/mesh/govern/model-contracts">Model contracts</a> can enforce this, among other things, with <a href="https://docs.getdbt.com/docs/mesh/govern/model-versions">model versions</a> providing a way to change the model over time without breaking references. The <a href="https://docs.getdbt.com/reference/resource-properties/deprecation_date">deprecation date</a> should be set for individual versions and the root model prior to the model being removed (and at this point, changing its name is equivalent to removal).</p> <h5 id="understandable">Understandable</h5> <p>Once a data product is found, in order to use it, the user must be able to understand it - its context, its meaning, the entities and relationships involved, and how to relate it to other data products. In other words, it must be well documented! A few sentences in the description field are really not sufficient. This is where <a href="https://docs.getdbt.com/docs/build/documentation#using-docs-blocks">doc blocks</a> can come in handy. Beyond enabling the DRY principle to apply to documentation, doc blocks are a great mechanism to isolate the description into a markdown file, which can be lengthier and include standard markdown formatting. You can even include images and diagrams! Your team should decide on a standard for documentation of data products that will allow users to independently and successfully use the data. Table and column names should follow clearly documented naming conventions. You may find that using doc blocks can help consolidate columns that mean the same thing but have slightly different names! I also highly recommend making writing documentation someone’s entire job - technical documentation is its own specialty, and it requires a different perspective to write documentation for the end users. Hire a documentation specialist!</p> <h5 id="trustworthy--truthful">Trustworthy &amp; Truthful</h5> <p>Zhamak cites Rachel Botsman’s concept of trust, and states that “a data product should close the gap between what data users know confidently about the data, and what they don’t know but need to know to trust it.” One way to do this is to attach service-level objectives &amp; agreements to the data product, where uncertainty is measured and some degree of certainty is guaranteed. Users want to know how often the data is updated (and the gap between the time the data is updated and when business fact happened in reality), how complete the data is (measuring nullness across key fields), basic descriptive statistics, and data lineage. Within dbt, <a href="https://docs.getdbt.com/docs/build/data-tests">data tests</a> and <a href="https://docs.getdbt.com/reference/resource-properties/freshness">freshness checks</a> are a way to measure and enforce some of these SLAs. (In fact, there were some very exciting announcements at the Coalesce 2025 keynote on setting freshness SLAs and smart dbt test execution using column level lineage!).</p> <p>Beyond the basic dbt tests you get out of the box, dbt packages like <a href="https://hub.getdbt.com/dbt-labs/dbt_utils/latest/">dbt-utils</a>, <a href="https://hub.getdbt.com/metaplane/dbt_expectations/latest/">dbt-expectations</a>, and <a href="https://hub.getdbt.com/elementary-data/elementary/latest/">elementary</a> allow analytics engineers to easily add sophisticated tests to models and columns. By carefully adding these tests prior to the mart, deciding when they should error vs warn, and materializing the marts as tables (not views), you can ensure that the data available in a mart meets data quality agreements. With robust observability using packages like <a href="https://hub.getdbt.com/elementary-data/elementary/latest/">elementary</a> or other products like <a href="https://www.metaplane.dev/">Metaplane</a> (now a Datadog company) and established triage processes when issues inevitably arise, you can identify and resolve data quality issues before they ever reach the user. And of course, data lineage features are included out of the box in any dbt project.</p> <h5 id="natively-accessible">Natively accessible</h5> <p>For a data product to be natively accessible, it should be available wherever and in whatever format the user needs it. Since dbt has traditionally operated within a single platform, your data product would only be available as a table/view within a database. However, there are two features that address this limitation: <a href="https://www.getdbt.com/blog/introducing-cross-platform-dbt-mesh">cross-platform Mesh</a> via <a href="https://docs.getdbt.com/docs/mesh/iceberg/apache-iceberg-support">Iceberg support</a> and the dbt <a href="https://docs.getdbt.com/docs/use-dbt-semantic-layer/dbt-sl">Semantic Layer</a>.</p> <p>If you materialize your data product model using an Iceberg materialization - and assuming you are on a platform that supports Iceberg - users can now access that data from their own data platform (assuming their platform also supports Iceberg). Open table formats are changing the game around accessibility and interoperability - at least that is the vision that we seem to be on our way to realizing. Until support is more widespread and reliable, additional infrastructure may still be needed to move data between platforms.</p> <p>If you create a semantic model for your data product model and define the metrics that can be calculated on it, your data product becomes accessible in many different BI tools (again, subject to the BI tool’s support of the semantic layer). This opens up the world of dashboards and even spreadsheets to analysts who want to use the data product.</p> <h5 id="interoperable">Interoperable</h5> <p>While each data product should be self-sufficient and usable on its own, it should also be able to be used together with other data products. The closest that dbt gets to offering a feature to serve this characteristic is the semantic layer - you can identify your entities across your models and how they can be related together, with the semantic layer handling how to query the underlying tables in order to produce the desired metrics sliced and diced any which way you want (as long as these dimensions are defined in the semantic layer).</p> <p>However, the bulk of the work in making data products interoperable relies on you, the analytics engineers, data modelers, and data architects. It is up to you to model your data in a sensical way, to have understandable and consistent naming conventions, and to document your data products well and focus on how they should be related to each other. You should go through the exercises of conceptual data modeling and even master data management to make sure that the entities defined in one domain (project) match up to the same entities defined in another domain (project). I do think that dbt has room to grow in better enabling interoperability: for example, while the data lineage graphs are extremely useful when it comes to tracing the provenance of a data product, they are not so useful when it comes to querying the final product. I’d love to be looking at the documentation page for a mart model and be able to see the star schema diagram as well!</p> <h5 id="valuable-on-its-own">Valuable on its own</h5> <p>This principle seems self-explanatory on the surface, but it has some interesting implications for data product developers. For a data product to be valuable on its own, all components of a data product must be packaged up and considered together: the data, the code, the documentation, and even the infrastructure that produces it all. Zhamak defines a new unit of logical architecture, the “data quantum”, which encapsulates all of the structural components needed to build, publish, and share the data product. Within dbt, this means that your “data product” is not simply your sql model file - it is the combination of the sql file, yaml file, markdown files with any associated doc blocks, the data in the database created by the dbt model, and even the entire upstream lineage of models that produce the final data product model. One interesting implication here is that in a data mesh world you may want to shift from orchestrating dbt projects to orchestrating individual data products. The updates that will be made to dbt core to support state-aware orchestration should be in line with this shift towards model-centric orchestration.</p> <p>Zhamak also uses this principle to caution against lifting and shifting your dimensional data model into data products in a data mesh. While data products should be able to be joined to each other, they should not <em>have</em> to be joined prior to being useful. When you think about your traditional star schema, usually you must join dimensions to your fact before it can really be used for analysis. So, you should not consider your typical “fact” table to be a data product - rather, the entire “star” is the data product. There are some interesting implications here: you may want to have a standard 4th layer between your intermediate and mart layers for these facts and dimensions, leaving the mart layer to follow the “One Big Table” methodology and pre-joining needed dimensions to your facts to form the final data product. Or perhaps you leave your facts and dimensions in your mart layer and introduce a new “product” sub-layer to follow it. Regardless, if you choose the data mesh approach, this must influence your data modeling methodology.</p> <h5 id="secure">Secure</h5> <p>Since we have already established that a pure dbt implementation of data mesh is limited to the medium of relations in a database (see principle 5, natively accessible), this principle is likewise limited to database security capabilities. Zhamak states that access control policies should be defined centrally but enforced at runtime by the data product, taking into account the level of access any individual may have. Fortunately, modern data warehouses can have quite complex and expressive access control policies. However, to truly take advantage of these, you will want to look outside of dbt, and use a tool like Terraform instead. There are some excellent resources on how to use terraform with dbt and data warehouse platforms, such as <a href="https://www.youtube.com/watch?v=6les3dVoYh0">Katie Claiborne’s talk at Coalesce last year</a>. The one dbt feature it is important to be aware of in respect to security is the <a href="https://docs.getdbt.com/reference/resource-configs/grants">grants</a> config. However, at the scale of users participating in a data mesh, you’re likely to quickly outgrow the capabilities of the grants config alone, as you seek to centrally manage access control policies for your data platform in a way that can be “versioned, automatically tested, deployed and observed, and computationally evaluated and enforced” - for this, you need a tool like <a href="https://developer.hashicorp.com/terraform">terraform</a> (or <a href="https://opentofu.org/">OpenTofu</a>).</p> <p>One last side note: while it is not available yet, the future dbt Fusion capability of tracking PII via project-wide column lineage will be highly relevant to this principle (when it is available).</p> <h5 id="score-your-data-product-models">Score your data product models</h5> <p>So, let’s summarize the actions you should take in order to evolve your mart models into data product models:</p> <ol> <li>Data product models should have model access set to “public”.</li> <li>Data product models should have enforced contracts, which means any changes applied needs to go through the model versioning process, and before a model can be removed it must go through the deprecation process.</li> <li>Data product models should be thoroughly documented, at both the model and column level, such that a new user could successfully use the data product as intended based on the available documentation alone. It’s highly recommended that doc blocks be utilized in the documentation process.</li> <li>Data product models must be tested - and these tests should occur upstream of the final data product model, with all assertions that are tested documented with the data product model, and the results of these tests made available. (sidenote for the dbt product team: make it easy to see what upstream tests were run!)</li> <li>Data product models should have a standard set of metadata fields that provide further context and enable discoverability (pro-tip: include data product owners in this metadata)</li> <li>Data product models should be useful on their own, with all needed columns already included (OBT methodology over Kimball). Don’t just call your fact and dimension mart models data products - create new models that join them together.</li> <li>Data product models should have explicit documentation about who has access to the data, and this should be enforced via grant configs.</li> <li>If possible and needed at your organization, materialize the data product model as an iceberg table.</li> </ol> <p>This is a lot! Data products require a lot of careful curation, and this requires time and effort. We want to evolve our mart models into data product models iteratively, not all at once (or it will never happen!). So let me make a further suggestion: develop a scoring system for your data products. Whether it be a 5-star rating, “medallion” score, or low-to-high maturity rank, have an agreed upon methodology for scoring your data product models, and surface this to users. You don’t want a user to expect the same thing from a 1-star model that meets the bare minimum standard of having access = public as they do from a 5-star model that is tested and documented up the wazoo. Utilize a <a href="https://docs.getdbt.com/reference/resource-configs/meta#designate-a-model-owner">meta config to document your model maturity</a>. I’m not going to tell you what your scoring system should be, but I am going to tell you that you need to get together with all of your teams across the mesh and agree on one.</p> <h3 id="self-serve-data-platform">Self-serve Data Platform</h3> <p>The principle of Self-serve Data Platforms is not, as you may initially think, about “self-service analytics”, enabling end users to access and use the produced data products. Instead, it is all about enabling the domain teams to produce those data products in the first place. This third principle is essential to making the first two principles actually work in practice. Without a self-serve data infrastructure as a platform, decentralized is just a synonym for fragmented, and domain ownership over data starts to feel like an untenable burden. Organizations that focus on the first two principles of data mesh will soon find themselves falling back to previous patterns, and return the onus of data management to a centralized data team.</p> <p>What does a self-serve data platform look like? Primarily, it should make it easy for a new domain team to go from 0, to 1, to many published data products. Publishing that first data product should not require going through a central team to set up infrastructure - teams should be able to accomplish this independently, without needing deep specialized technical expertise. There should be a low barrier to adoption, and integration with other systems (namely, operational systems) should be as seamless as possible. And while working with this platform should be as easy as possible for generalists (or rather, those with specialties besides data) to work with, it must not compromise on software engineering best practices of testing, versioning, and modularity.</p> <p>However, the platform must enable all of this… while being decentralized. Zhamak posits that there are very few existing platforms/technologies that meet these requirements, as most platforms currently in vogue are highly centralized. She argues that centralization only introduces bottlenecks that slow down the speed of change &amp; innovation. A self-serve data platform must strike the right balance of enabling decentralized data ownership while only centralizing the most essential tasks and resources so that domain teams can operate at a higher level of abstraction. Furthermore, the self-serve platform must centralize enough of the underlying services that data products developed by different domains remain interoperable.</p> <p>Supporting all of this is a platform team. If you do not already have a platform team in your organization, this may be what your centralized data team evolves into. It is the responsiblity of this platform team to support all of the features and tools needed by all domain teams. It is the platform team who should support the dbt resources I’m about to recommend in this next section.</p> <h4 id="applied-to-dbt-2">Applied to dbt</h4> <p>Before describing how you should apply this principle to dbt, I must first acknowledge something: I think it is highly unlikely that Zhamak would agree that it is even possible to create a self-serve data platform in accordance with her vision using dbt. Zhamak has spent a considerable amount of time and effort creating her vision of a true self-serve data platform with her company’s product, <a href="https://www.nextdata.com/product">Nextdata OS</a>.</p> <p>So, I am not going to even try to recommend you implement this principle with dbt, especially with respect to the decentralization component. Instead, I am going to focus on a sub-principle: that it should be as easy as possible for new domain teams to get started producing their first data products.</p> <p>First, I want to point out some of the excellent product developments out of dbt Labs that have made it easier than ever for non-data specialists to contribute to dbt projects - products that you should be making full use of if you are a dbt Labs customer. Then, I’ll outline some specific suggestions that you can implement in your organization to make it easier for other teams to manage their own dbt projects and start producing data products.</p> <h5 id="dbt-product-shoutouts">dbt Product shoutouts:</h5> <ul> <li>The <a href="https://docs.getdbt.com/docs/cloud/dbt-cloud-ide/develop-in-the-cloud">dbt Studio IDE</a> and <a href="https://docs.getdbt.com/docs/about-dbt-extension">dbt VS Code extension</a> (also, big shoutout to Altimate AI for their work in developing &amp; maintaining the <a href="https://docs.myaltimate.com/">dbt Power User extension</a> for VS Code). These tools make the experience of developing within a dbt project smooth and the Studio IDE in particular walks new developers through the necessary steps in a user-friendly way.</li> <li><a href="https://docs.getdbt.com/docs/cloud/canvas">dbt Canvas</a> for opening up dbt projects to contributions from those who feel uncomfortable with code and prefer a GUI-centric drag &amp; drop interface. Bonus shoutout to <a href="https://docs.getdbt.com/docs/cloud/build-canvas-copilot">dbt Copilot</a> for further enabling these users to still contribute to the underlying codebase.</li> <li><a href="https://docs.getdbt.com/docs/explore/dbt-insights">dbt Insights</a> to more easily enable users to query and explore data - think ad-hoc analysis with superpowers from the additional context of your project’s documentation, metadata, and an assist from AI.</li> <li><a href="https://docs.getdbt.com/docs/use-dbt-semantic-layer/dbt-sl">dbt Semantic Layer</a> for enabling analysts to define and correctly query metrics across different dimensions in their BI tool of choice (so long as it is a supported option)</li> <li><a href="https://www.getdbt.com/product/dbt-catalog">dbt Catalog</a> for providing the best presentation layer of dbt projects, turning version-controlled code repositories into documentation that is easy for users to explore and discover data products they can use - as well as all of the context that goes with it.</li> <li>The <a href="https://learn.getdbt.com/catalog">dbt Learn Course Catalog</a> for providing the best self-serve courses (all free!) to learn every aspect of dbt - both the open source dbt Core and paid products.</li> </ul> <p>Now, I will acknowledge that it is very much in dbt Labs’ self-interest to lower barriers of entry and make it easy for a wider user base to adopt their products. However, I think they have done an excellent job at striking that balance of lowering barriers to entry while not compromising on the software engineering best practices that they brought to data management - a core part of their founding mission. On the other hand, these dbt products all belong to one centralized platform - they certainly do not embody the principle of decentralization that Zhamak emphasizes. Perhaps at some point there will be an integration between dbt and Nextdata OS, and we can see what it looks like to develop data products using dbt on a true decentralized self-serve data platform that fully embodies the original data mesh vision.</p> <p>I’ll also mention that some of these tools are likely to have widely varying adoption across your teams, and can significantly impact the code that gets committed to your project. So you may want to keep this in mind when deciding what your domain projects look like - a team of downstream marketing analysts may want to make heavy use of dbt Canvas for developing their dbt models, and this will influence what the sql in their models looks like. This is not a bad thing - and this team should be able to operate with their tools of choice! The self-serve data platform should enable all of these types of workflows, while the principle of <em>federated</em> computational governance protects the independence of domain teams (within the bounds required for interoperability).</p> <h5 id="3-things-your-platform-should-buildsupport">3 things your platform should build/support</h5> <p>Now, let’s move on to some suggested dbt Mesh best practices that will also help your organization strike the right balance between decentralized domain-oriented dbt projects and centralized support for those projects.</p> <p><strong>Pro-tip #1:</strong> create an internal dbt package. You have used public dbt packages before from the <a href="https://hub.getdbt.com/">dbt Package Hub</a> (I dare you to find someone who hasn’t at least used dbt-utils), but did you know that you can also create your own dbt packages purely for internal use at your organization? And it’s easier than you may think - after all, a dbt package is just a dbt project. There are number of great resources on how to develop a dbt package, including <a href="https://docs.getdbt.com/guides/building-packages?step=1">this official guide</a>, <a href="https://docs.getdbt.com/blog/so-you-want-to-build-a-package">this tutorial</a> from dbt PM extraordinaire Amy Chen, this <a href="https://www.youtube.com/live/Bp6keIz0Jvc?si=sjmGmFzDSL8K623X">past Coalesce talk</a>, and <a href="https://www.phdata.io/blog/how-to-use-a-dbt-package-in-your-project/">this blog post</a> from phData.</p> <p>What does an internal dbt package get you? Code sharing between all of your projects in the mesh. The top use case is macros: there are going to be certain macros that you want available to all projects - and you only want there to be one version of it (imagine the confusion if everyone developed their own version of a macro that performs a common transformation… in slightly different ways!). Macros can go through a promotion process - one domain project may develop a new macro, iterate on it, generalize it, and then add it to the internal dbt package when another team has a use for it as well. Custom generic tests are just macros - you can create standarized custom tests for the whole mesh to use as well. There may also be certain models you want made available to everyone - for example, a common reference table made queryable through a seed + model in the package. It is up to your organization to decide what code would be useful to be centralized and shared across all projects, and decide on a process for what code gets added to this centralized repository.</p> <p><strong>Pro-tip #2:</strong> create a dbt project template. There are a number of ways to do this, ranging in sophistication. It may be as simple as a repo with a standardized dbt project setup that can be forked, or as sophisticated as a terraform template that can set up the new dbt project &amp; all related infrastructure. Katie Claiborne gave an <a href="https://www.getdbt.com/resources/coalesce-on-demand/coalesce-2024-why-analytics-engineering-and-devops-go-hand-in-hand">excellent talk at Coalesce</a> last year about exactly how to do this! Regardless, the goal is to make it fast, easy, and efficient to create new dbt projects that comply with all expectations your organization has of a dbt project in the mesh. With infrastructure-as-code tools like terraform, this setup is reproducible and version controlled.</p> <p><strong>Pro-tip #3:</strong> create a dbt project (or multiple projects) that are only for the purpose of learning dbt. While dbt Learn offers many amazing courses, learners are still meant to follow along by creating their own projects to practice on. By pre-creating these learning projects, you are reducing barriers for new dbt contributors. My recommendation is to do the set-up steps for the lessons, and then ask learners to create their own branches in that dbt project repo in order to complete the course and follow along. They may submit Pull/Merge Requests for their work to be evaluated, but learners should be blocked from merging any of these changes to main - the main branch should remain clean for any new learner to create a branch and progress through the course from the beginning on their own.</p> <p>For the dbt Learn course developers/maintainers: it would be lovely if y’all could consolidate the various course-specific dbt projects in order to minimize the total number of “learning projects” that need to be set up. There are many different versions of the Jaffle Shop project out there!</p> <h3 id="federated-computational-governance">Federated Computational Governance</h3> <p>Finally, we arrive at our fourth and final principle: Zhamak’s modern re-imagining of data governance for the data mesh, Federated Computational Governance. Zhamak’s approach uses systems thinking to govern the mesh, balancing the autonomy of decentralized domains with the harmony of global interoperability. The challenge is in finding equilibrium while the two opposing forces fight for optimization. Zhamak was inspired by Donella Meadows’ “<a href="https://donellameadows.org/systems-thinking-book-sale/">Thinking in Systems</a>”, and describes how to use feedback loops to adjust behavior and leverage points to adjust the strength of those feedback loops and significantly impact the overall system. In a large-scale distributed and dynamic system like a data mesh, it is essential that traditional data governance evolves to become federated and automated.</p> <p>To become automated, policies that were previously enforced by manual intervention must now be enforced automatically by the platform (and this means these policies must be expressed in such a way that they can be automatically enforced - as code). For example, a standard might be that all models should have a primary key test. A policy might be that all PII data must be masked.</p> <p>To become federated, domains must be allowed to create their own policies to some extent (primarily around how data products are created, maintained, and served), while also adhering to some minimum viable set of global policies &amp; standards (primarily those that facilitate interoperability). As a reminder on what “federated” means, think of the governance model outlined by the US Constitution: the federal government sets certain policies/laws that apply across states, but state governments have the power to set policies/laws that apply only to people in their state (so long as compliance with federal laws is maintained. If not… that’s what the Supreme Court is for).</p> <p>Standards and policies are code - administered, enforced, and customized locally after inheriting from global policies - and this code should be subject to the same agile principles as other software: easy to change, iterate, and evolve.</p> <h4 id="applied-to-dbt-3">Applied to dbt</h4> <p>If there is one key leverage point to adjust people’s behavior when contributing to a dbt project, it is during a Pull/Merge Request. The <a href="https://www.getdbt.com/blog/adopting-ci-cd-with-dbt-cloud">CI/CD process</a> as a whole is a feedback loop, and the event of a CI job passing or failing is a leverage point. It is an excellent demonstration on how policies can be defined in code and enforced automatically.</p> <p>A <a href="https://github.com/savannah-dbt-labs/sharing_is_caring">typical setup</a> for a dbt project looks something like this: when a Pull/Merge Request is opened, a <a href="https://docs.getdbt.com/docs/deploy/continuous-integration">CI job</a> is automatically kicked off. This CI job should either build all new/updated models and their downstream models (<a href="https://docs.getdbt.com/best-practices/best-practice-workflows#run-only-modified-models-to-test-changes-slim-ci">slim CI</a>) or the entire project (inefficient, but ok for smaller projects with no way to handle state or deferral yet), in an isolated environment (typically, a new schema is created for the PR, then dropped after the PR is merged). Note that if you have enforced model contracts in your project, any violation of the contract will cause the job to fail. If the dbt job is successful, you can safely merge the code to main (or a develop branch, depending on your <a href="https://docs.getdbt.com/blog/git-branching-strategies-with-dbt">git strategy</a>) - after a code review by a human as well. CI jobs offload some of the burden from human reviewers, by letting computers do what computers are good at (making sure code runs without error).</p> <p>On the surface, requiring a successful CI job prior to merging code to main is a way to protect against bugs that could cause production jobs to fail. However, you can test for more than just the fact that models run successfully - you can also test that the code follows certain agreed upon standards. This is where the <a href="https://hub.getdbt.com/dbt-labs/dbt_project_evaluator/latest/">dbt Project Evaluator package</a> comes in handy.</p> <p>The <a href="https://dbt-labs.github.io/dbt-project-evaluator/latest/">dbt Project Evaluator</a> turns your dbt project(s) into data - and then builds dbt models on that data and tests for certain assertions. While dbt Project Evaluator comes with a standard set of policies it enforces out of the box, you are able to customize which assertions it tests for and even the parameters for some of those tests. For example, let’s say that your team has an expectation that every model should have a primary key test. Typically, it would be the human reviewer’s responsibility to ensure this policy is followed. However, with dbt Project Evaluator enabled and run during the CI job, the <a href="https://dbt-labs.github.io/dbt-project-evaluator/latest/rules/testing/#missing-primary-key-tests">missing primary key test</a> will fail if any model is missing a primary key, thus preventing a merge until the issue is resolved.</p> <p>One useful side effect of using the dbt Project Evaluator package is that your dbt project - the actual code repository - is modeled out and materialized in your database. This means that you can build further models and tests on top of these Project Evaluator models, in order to enforce custom policies that are not included out of the box in <a href="https://dbt-labs.github.io/dbt-project-evaluator/latest/rules/">Project Evaluator’s ruleset</a>.</p> <p>And if you have multiple dbt projects, all with dbt Project Evaluator enabled for staging (or production) jobs, you can put these project-specific tables together to form a cohesive dataset about the entire mesh. And then you can test new and interesting things in this comprehensive set of tables - in other words, you create mesh-wide tests. For example, let’s say that you only want one project in the mesh to use a given table as a source (in order to reduce duplicative and possibly diverging efforts). This mesh-wide test is now possible to run and enforce once you create the model that sources in and consolidates the Project Evaluator core models across all projects.</p> <p>Once your dbt project - the code itself - is data that can be queried and tested, the possibilities are endless. And a great place to put these mesh-wide tests and models? In your internal dbt package, which should be imported by every domain project, enabling these mesh-wide tests to be run during the CI job as well. I would further suggest that you turn your internal dbt project into a full project, with its models materialized in the database in a single permanent location, as this will make it much easier to query and monitor the status of data products across the mesh. (Remember: CI schemas should be dropped after the PR is merged, so you can’t build permanent infrastructure on models built just during CI jobs).</p> <p>Here comes another request for the dbt Labs team: it would be amazing if default project configurations (think: the configs that you set in your dbt_project.yml file) could be inherited from that internal dbt package (which evolves into a central project that underlies the entire mesh). While code can be imported from packages (models, macros), configurations cannot - but it would make setting up and governing the mesh easier if they could be!</p> <h2 id="people-and-process">People and Process</h2> <p>As I’m sure you’ve noticed while reading this blog post, there are a lot of decisions to be made about how, exactly, your organization will decide to implement dbt Mesh. And when you are operating at the scale of a mesh, with governance and ownership distributed across the domains, it is no longer feasible for a single leader or centralized governance team to make these decisions for everyone. While it may be tempting to put the responsibility of crafting data mesh governance policies on the existing (likely highly centralized) governance team (or, conversely, to leave them out completely!), please resist the temptation. Instead, you need a “federated team of domain product owners, subject matter experts, and central facilitators.” Identify a core group of decision makers that allows for every domain to be represented, and charge this group collectively with crafting the policies, making decisions, and steering the development of the mesh. Start to transition data stewards from the centralized governance team into the domains, and pull domain data product owners into the new (federated!) mesh governance team. (Note: this federated mesh governance team is an important component of the fourth principle).</p> <p>In short - you need to convene your mesh! Let’s review some of the decisions this Mesh Governance Council will need to make, one principle at a time.</p> <h3 id="decision-time">Decision time</h3> <h4 id="domain-ownership-1">Domain Ownership</h4> <ul> <li>What qualifications must a domain team meet in order to manage their own dbt project?</li> <li>Who determines when they have met these qualifications?</li> <li>Who manages the creation of the new dbt project and the migration of models into it?</li> </ul> <h4 id="data-as-a-product-1">Data as a Product</h4> <ul> <li>What is the scoring system that will be used to rate data product models?</li> <li>What qualifications does a model need to meet in order to reach the next level?</li> <li>Who is able to make this determination, and how is this score represented in the model metadata?</li> <li>What other metadata is expected for all data product models across all projects?</li> </ul> <h4 id="self-serve-data-platform-1">Self-serve Data Platform</h4> <ul> <li>Who is responsible for maintaining the internal dbt package?</li> <li>Who can contribute to the internal dbt package?</li> <li>When should code be promoted from being managed in domain dbt projects to being generally available in the internal dbt package?</li> <li>Who is responsible for creating and maintaining a dbt project template?</li> <li>Who is responsible for making sure existing dbt projects get any needed updates after changes are made to the template?</li> <li>Who is responsible for supporting new dbt learners, and maintaining the learning projects?</li> </ul> <h4 id="federated-computational-governance-1">Federated Computational Governance</h4> <ul> <li>Who will be monitoring the health of the overall mesh, and making sure that individual domains and projects are adhering to the mesh-wide policies?</li> <li>How will issues be detected, and who is responsible for ensuring those issues get resolved?</li> <li>What will the standard CI job look like across dbt projects, and when can CI jobs diverge from this standard template?</li> <li>What is the standard suite of tests dbt Project Evaluator should run for all projects, and in what tests can individual projects have more flexibility?</li> <li>Who is responsible for setting, maintaining, and monitoring the mesh-wide tests?</li> </ul> <h3 id="map-out-your-process">Map out your process</h3> <p>Hopefully these questions clarified that the transition to a dbt-powered data mesh introduces a lot of new problems and complexities… ones that will take time, iteration, and input from every participating dbt project team to solve. I would highly recommend mapping out the process that the teams should follow in order to craft new policies and solve each problem. (Shameless plug: helping teams figure out what this decision-making process would look like for a dbt Mesh environment was the goal of <a href="https://jennajordan.me/projects/dbt-mesh-council-simulation">my peer exchange at Coalesce in 2024</a>!)<br/> Here is an example of what this process may look like at one organization:</p> <ol> <li>Someone (hereafter the Champion) brings a proposal to the Mesh Governance Council.</li> <li>The Council can either reject the proposal immediately (process stops), or let it go forward.</li> <li>If the process goes forward, the Council evaluates whether this is a major or minor proposal. In essence, a minor proposal can be approved immediately, while a major proposal requires a working group to flesh out the details.</li> <li>If the proposal is minor, the proposal is either approved or rejected by the Council (by a vote or some other decision making function).</li> <li>If the proposal is major, the Champion forms and leads a working group, recruiting interested parties. This working group crafts the proposal, and gathers feedback. When they are ready, they bring this detailed proposal to the Council.</li> <li>Once again, the Council decides whether to reject (process stops), request changes (process loops), or approve the proposal (process moves forward).</li> <li>The proposal may need to go through additional approval processes, depending on the organization’s requirements. The Council determines what further approval processes the proposed policy needs to go through. If the policy is rejected by any other approving body, the Champion and working group can decide whether to stop or loop back through the process again with revisions.</li> <li>Once the policy has all needed approvals, the final version of the policy should be thoroughly documented and stored in a central and accessible location. This final policy should also include the planned (automated if possible) enforcement mechanism. The policy should be communicated out to all projects, along with an expected timeline for compliance. (Pro-tip: the internal dbt package - which may double as a central monitoring project - would be a great initial place to put these policies as markdown files!)</li> <li>Individual domain teams can petition the Council for exceptions to the new policy. The Council decides whether to approve or reject these petitions.</li> <li>New mesh-wide tests (or other automated enforcement mechanism) should be implemented according to the communicated timeline and any approved exceptions. I highly recommend a gradual approach - set tests to warn for a period first, before transitioning the tests to error on failure.</li> </ol> <p>As a reminder, this heavier process should only be needed for policies that must apply across the mesh for the sake of interoperability. Individual domains should be able to set their own policies internal to their projects in a much faster and lighter process. Mesh-wide policies should lean towards being minimal, with individual domains empowered to enforce stricter policies for their own projects.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>I hope that you walk away from this blog post (and accompanying talk) also convinced that dbt Mesh + data mesh is a strong approach for organizations using dbt to manage data at scale. I also hope that I’ve caught you at the right time in your dbt Mesh implementation journey, and you can put into practice some (or all!) of the suggestions I’ve made. Of course, while this is (naturally, for me) an absurdly long blog post, I can only cover so much, and I’m limited by my own experiences. I would love to hear from y’all if there is anything I should add - if you have any pro-tips for the others who have chosen to adopt data mesh by way of dbt - please reach out and let me know!</p> <h2 id="appendix">Appendix</h2> <div class="row"> <div class="col-sm md-auto"> <a href="/assets/pdf/so-you-want-to-build-a-data-mesh.pdf"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_data-mesh-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_data-mesh-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_data-mesh-1400.webp"/> <img src="/assets/img/header_data-mesh.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Coalesce Talk Slides" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> PDF of the slides presented during the talk </div> <div class="embed-responsive embed-responsive-16by9"> <iframe width="560" height="315" src="https://www.youtube.com/embed/yKScVw4gVfo?si=e0el1KQG8u0Xaol2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> Official recording of "So you want to build a data mesh" on YouTube </div>]]></content><author><name>Jenna Jordan</name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[... with your dbt project(s)]]></summary></entry><entry><title type="html">The librarian’s reference interview for data teams</title><link href="https://jennajordan.me/blog/reference-interview-for-data-teams" rel="alternate" type="text/html" title="The librarian’s reference interview for data teams"/><published>2025-06-30T00:00:00+00:00</published><updated>2025-06-30T00:00:00+00:00</updated><id>https://jennajordan.me/blog/reference-interview-for-data-teams</id><content type="html" xml:base="https://jennajordan.me/blog/reference-interview-for-data-teams"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_reference-interview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_reference-interview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_reference-interview-1400.webp"/> <img src="/assets/img/header_reference-interview.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>Note: An abridged version of this article was published Oct 1, 2025, on <a href="https://dataintelligenceplatform.substack.com/p/the-librarians-reference-interview">The Data Intelligence Platform</a></p> <p>Thank you Ole Olesen-Bagneux for featuring my writing as a Guest Expert!</p> </blockquote> <p>If you’re an analyst on a data team, the following scenarios are probably very familiar to you:</p> <ol> <li>A stakeholder you’ve worked with before comes to you with a new request. They saw what kinds of questions could be answered with a dashboard in the last project, and now they have some new questions to try and find answers for</li> <li>You get an email from someone in the organization who wants a dashboard for their data. Right now they do a lot of manual work and one-off visualizations in a spreadsheet, and their friend in another department recently showed them a dashboard that the data team made</li> <li>An executive asks your boss for a set of very specific metrics. Your boss tells you to write some SQL queries to generate numbers for those specific metrics, so they can then email those numbers to the executive</li> </ol> <p>New project requests can come from many sources: an email to a shared inbox, a DM from a former stakeholder, a verbal request from your boss, or even just a jira ticket created by your project manager. No matter how you get the request, once you’ve received it, you will need to choose how to respond.<d-footnote>Not responding at all is still a form of response</d-footnote></p> <h2 id="the-data-team-dilemma-the-ad-hoc-deluge">The data team dilemma (the ad-hoc deluge)</h2> <p>Perhaps your team is already over maximum capacity and you get requests like this every day. You could choose to ignore the request, or mentally bookmark it to respond to later, only to forget about it<d-footnote>hello executive function overload!</d-footnote>. Perhaps you have a project request form, so you redirect all inquiries to filling out a generic form. Maybe you don’t have capacity for this request, but your coworker just wrapped up a project, so you ask them to take it on. If interrupt tasks are common for your team and you are lucky enough to have time allocated to ad-hoc requests, you may try to quickly provide an answer before moving on to the next request or project. If it’s obvious this request is a huge ask, you may reply that you don’t have capacity right now, but they can put a meeting on your calendar for sometime next month. The request may get added to the project backlog, to be re-examined during the next sprint planning phase.</p> <p>You’re on a data team, and in today’s world that means<d-footnote>you've trauma-bonded</d-footnote> you are understaffed, always working at maximum capacity with pre-planned projects, and also expected to drop everything to answer some higher-up’s one-off question. Half of your week is filled with meetings, and you know that if you just had some uninterrupted quiet time you’d be able to actually crank out some real work and make real progress on that dashboard that was supposed to be done last week. Fortune forbid that the data isn’t fresh today or the column you were counting on is a free-text field full of inconsistent garbage. Why did your stakeholder organize their spreadsheet like this, don’t they know that will break the import? Why does your boss think you can just <code class="language-plaintext highlighter-rouge">select * from some_perfect_table_that_does_not_exist</code>? You don’t have enough time to clean up the data going into the dashboard the executives supposedly look at every day (they don’t), you certainly don’t have time to model the data used in 1 project into a star schema!</p> <p>Let’s all stop and take a deep breath. In… out… in… out. If you are very lucky, and you have always worked on a fully staffed, well-managed data team with a reasonable number of projects and minimal interrupts, perhaps the above situations have never happened to you. But for the rest of us (and, I bet, the vast majority of us), these situations are familiar (and perhaps slightly triggering). And I’m about to give you some advice that you really won’t want to hear: you need to talk to your stakeholders more.</p> <p>“What? Didn’t you hear that half my day is already filled with meetings and I can’t find the time to do the work I’ve already been assigned? I already talk to my stakeholders every week and they never tell me anything helpful. They’re just not data people<d-footnote>I'm going to need you to do the big airquotes for me here</d-footnote>, they don’t get it. I can make the dashboard they need without them micromanaging every pixel of it. They’re always going off on tangents unrelated to the project, how is talking to them <em>more</em> going to help?”</p> <p>I hear you. I understand your doubts. I know you don’t have enough time to dive into each project as deeply as you want to. I know how frustrating it can be to talk to “non-data” people who just don’t understand why their seemingly simple ask isn’t reasonable. Trust me - I’ve been there, and I get it. But I want to zoom out for a moment, and look at the bigger picture.</p> <p>What is the role of a data professional? What is the purpose of a data team? What is the function of data in an organization? What even is data?</p> <p>Pause. Take just a few minutes to really think about these questions. What are your answers? I really want to hear! Wherever you may have seen this posted, please let me know in the comments.</p> <p>No really, come up with your own answers before you read what I think - then come back and hear how a librarian would approach this dilemma.</p> <h2 id="the-dikw-pyramid">The DIKW pyramid</h2> <p>My background is in Library &amp; Information Sciences, and I also have a deep fondness for philosophy, which informs my opinion. You may have a totally different answer informed by your own background and expertise, and that is awesome! If we put together all of our perspectives, we can arrive at a more complete picture.</p> <p>What is data?</p> <p>When I think of what data is, I think of it in the context of the DIKW (data-information-knowledge-wisdom) pyramid, which is a foundational concept in the field of Library &amp; Information Sciences. Rather than try to explain the DIKW pyramid myself, I’m going to pull quotes from <a href="https://www.isko.org/cyclo/dikw">this excellent encyclopedia page</a><d-cite key="Frické"></d-cite>, which itself pulls quotes from significant literature on the DIKW pyramid (mostly Ackoff’s “<a href="https://www.researchgate.net/profile/Rob-Keller/post/Original_paper_of_From_data_to_wisdom_by_Ackoff_1989/attachment/63f67d8997e2867d5081d0de/AS%3A11431281121841684%401677098376991/download/Ackoff89.pdf?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InF1ZXN0aW9uIiwicGFnZSI6InF1ZXN0aW9uIn19">From Data to Wisdom</a>”)<d-cite key="Ackoff_1989"></d-cite>.</p> <p>On data:</p> <blockquote> <p>Data are symbols that represent properties of objects, events and their environments. They are products of observation. To observe is to sense. The technology of sensing, instrumentation, is, of course, highly developed (Ackoff 1989, 3)<d-cite key="Ackoff_1989"></d-cite>.</p> </blockquote> <p>On information:</p> <blockquote> <p>Information is relevant, or usable, or significant, or meaningful, or processed, data (Rowley 2007, Section 5.3 “Defining Information”)<d-cite key="Rowley_2007"></d-cite>.</p> </blockquote> <p>On data vs information:</p> <blockquote> <p>The vision is that of a human asking a question beginning with, perhaps, “who”, “what”, “where”, “when”, or “how many” (Ackoff 1989, 3)<d-cite key="Ackoff_1989"></d-cite>; and the data is processed into an answer to an enquiry. When this happens, the data becomes “information”. Data itself is of no value until it is transformed into a relevant form. In consequence, the difference between data and information is functional, not structural (Ackoff 1989, 3)<d-cite key="Ackoff_1989"></d-cite>.</p> </blockquote> <blockquote> <p>Information systems generate, store, retrieve, and process data. In many cases their processing is statistical or arithmetical. In either case, information is inferred from data. (Ackoff 1989, 3)<d-cite key="Ackoff_1989"></d-cite></p> </blockquote> <p>On knowledge:</p> <blockquote> <p>Knowledge is know-how, for example, how a system works. It is what makes possible the transformation of information into instructions. It makes control of a system possible. To control a system is to make it work efficiently. (Ackoff 1989, 4)<d-cite key="Ackoff_1989"></d-cite></p> </blockquote> <p>On wisdom:</p> <blockquote> <p>Wisdom adds value, which requires the mental function we call judgement. […] The value of an act is never independent of the actor [… ethical and aesthetic values] are unique and personal. […] wisdom-generating systems are ones that man will never be able to assign to automata. It may well be that wisdom, which is essential to the effective pursuit of ideals, and the pursuit of ideals itself, are the characteristics that differentiate man from machines. (Ackoff 1989, 9)<d-cite key="Ackoff_1989"></d-cite></p> </blockquote> <p>All organizations collect and store data as a by-product of doing business. However, this data cannot be used beyond its functional purpose of making the widget work until it is processed into information. That information can be used to help the people in the organization gain a better and more accurate understanding of how well the organization is functioning. Knowing how an organization is functioning helps the people serving the organization make decisions, which will then affect how the organization functions. Leaders making these decisions should have a vision of the organization’s ideal state, and we consider these leaders wise when they can effectively move the organization closer to that ideal state.</p> <p>The purpose of a data team is to turn data into information, so that their stakeholders elsewhere in the organization might gain knowledge and make wise decisions. <strong>The role of a data professional is to navigate this process of turning data into information in the most efficient and useful way possible.</strong> An excellent data professional will partner with stakeholders to see through the process of generating knowledge from the processed information, and the most effective data leaders are those empowered to make decisions alongside other leaders.</p> <p>In order to provide information that will increase the knowledge of your stakeholder and help them make wise decisions, you must talk to them in order to find out what the decisions they need to make are, what their current understanding of the knowledge landscape is, and what information they need in order to enhance their understanding. This conversation requires time, empathy, and trust. Most likely, your stakeholder has already thought about this… and then gone the extra step of trying to translate this into a request for a data team.</p> <p>Their idea of a data professional is probably just that - someone who only operates at the level of data. But it is your job to demonstrate that while you have the technical know-how to work at the data level, you also have the business acumen to understand and converse at all levels of the DIKW pyramid. <strong>You can do your best work at the foundation when you have the entire context.</strong> The translation from a business-oriented request into a data-oriented request should be on the data professional, not the stakeholder.</p> <p>This is where I’m going to introduce another concept from the field of Library &amp; Information Sciences… and the primary topic of this blog post: the reference interview.</p> <p>The reference interview is designed to solve a problem common to most information professions - one that you have undoubtedly encountered before and may have already developed techniques to address. My hope is that by learning how librarians approach a reference interview you can add another set of techniques to your toolbox, and perhaps even adopt a new perspective that will help you approach one of the most frustrating aspects of your job.</p> <h2 id="the-reference-interview">The reference interview</h2> <p>Before adapting the reference interview for data teams, let me first explain it in its original context: that of a librarian responding to patron inquiries. Have you ever walked into a library with a research question, but weren’t quite sure where to start? Soon after walking in, you probably saw a circular desk with signs above it proclaiming “Reference Desk”. Behind that desk are one or two librarians, next to computers with the catalog ready for searching, ready to help you with your research quest. You walk up to the desk, ask your first question - and what happens next is the reference interview.</p> <p>My primary source for this section will be the Reference and User Services Association (RUSA) Guidelines for Behavioral Performance of Reference and Information Service Providers<d-cite key="ALA_2008"></d-cite> - as <a href="https://www.ala.org/rusa/resources/guidelines/guidelinesbehavioral">these guidelines</a> are industry-standard and referenced by most institutional guides to the reference interview. I’ll also be drawing from the <a href="https://alastore.ala.org/content/conducting-reference-interview-third-edition">book</a> “Conducting the Reference Interview: A How-To-Do-It Manual for Librarians”<d-cite key="Ross_Nilsen_Radford_2019"></d-cite>, which goes into much more depth than the short RUSA Guidelines.</p> <p>The importance of the reference interview is founded on a simple assumption: that patrons rarely start out asking the librarian for the information they actually need. The goal of the reference interview is to identify what the patron really wants to know, so that the librarian can then more efficiently search the system for resources that will fulfill the patron’s true information need. Librarians should take the time to establish rapport with the patron, engage in active listening, and confirm that the information provided is what they actually needed - in the end, this will save the time of both patron and librarian (compared to jumping straight to the search and repeating through trial and error until an adequate resource is found).</p> <p>While on the surface the reference interview is a structured format for going from initial question to identifying resources that can fulfill the true information need, the real magic is in the empathetic communication style and active listening that underlies these techniques. Excellent interpersonal skills and emotional intelligence are key for the librarian to build trust with the patron, cultivate an inclusive and collaborative learning environment, and work towards a mutual understanding of what the patron really wants to know.</p> <p>Reference interviews generally follow these 6 steps:</p> <ol> <li>Establish contact with the patron and listen to the initial request, without interruption or judgement</li> <li>Paraphrase the initial request, to demonstrate you were listening and verify you correctly understood the question</li> <li>Ask open-ended questions first that encourage the patron to elaborate on their initial request (focusing on the big picture), followed by clarifying close-ended questions to understand the specifics of certain parts of the request</li> <li>Verify the patron’s request by restating the question and asking for confirmation, incorporating what you have learned during the question phase. If the patron verifies this rephrased request is accurate, start the search. If not, repeat the question phase until the patron verifies the restated question is accurate.</li> <li>Conduct a search of the system collaboratively, sharing your screen/resource and explaining each step of the process. Check in with the patron to see if they want to do the search themselves at any point in the process, and if so, continue to offer assistance as needed. This is the phase where you can offer instructions or advice, guiding the patron with your special expertise and knowledge.</li> <li>Follow up with the patron to verify that the patron’s information need was met. You may need to review the resources found and identify different/additional resources if needed, repeating the search phase. It may also be necessary to explain when a request is beyond the scope/capacity of the library, in which case the patron should be referred to an external resource/institution. This final phase is also the appropriate time to ask for feedback on the process.</li> </ol> <p>A reference interview is most successful when the librarian and patron share a common purpose: finding materials that will fulfill the patron’s information need. It is the librarian’s responsibility to find out what the patron wants and guide the interview in order to achieve this goal. The modern reference interview comes from the perspective that libraries are primarily service-oriented, and should provide information in the manner that is most useful to its patrons.</p> <p>Of course, there are plenty of opposing viewpoints in the LIS literature - that the reference interview is not always necessary, that it is only useful for certain types of inquiries, that libraries shouldn’t be service-oriented, that the reference interview can only be performed in person… and the dominant viewpoint in the literature has changed over time. If you’re curious, feel free to explore the literature, but I’m not going to dive any deeper in this post beyond noting the fact that the importance and role of the reference interview has been contested and evolved since it first became a thing over a century ago.</p> <p>It should also be noted that even when librarians are trained to always conduct a reference interview, studies have shown that a majority of the time they do not conduct a full reference interview, or default to the “question-oriented model” (answering the patron’s original question) rather than the “needs-oriented model” (addressing the patron’s information need - the initiating problem). Being empathetic, curious, and patient all of the time for every patron query is hard work, and nobody is perfect - but the reference interview provides an ideal guideline for these engagements.</p> <h3 id="roles-of-the-reference-librarian">Roles of the reference librarian</h3> <p>Before we move on to how this translates to the work data teams do, I want to spend some time on the different roles that a librarian takes on during the reference interview. My primary source for this section is a <a href="http://www.amyvanscoy.net/uploads/5/7/7/9/5779319/vanscoy_inventing_the_future.pdf">chapter</a> in the book “Leading the Reference Renaissance: Today’s Ideas for Tomorrow’s Cutting Edge Services”, called “Inventing the future by examining traditional and emerging roles for reference librarians”<d-cite key="VanScoy"></d-cite>.</p> <p>The most dominant role of the reference librarian is that of the “information provider”: the highest priority for a reference librarian is to provide answers to questions - the more complete the better, whether that be through directly telling the enquirer the answer to their question or helping them find the information resources they need (e.g. books, journal articles, news articles, magazines, etc). After all, the ultimate goal of the reference interview is to fulfill the patron’s information need. But while “information provider” is the most obvious role, it is far from the only (or even, arguably, the most important) role that a reference librarian fulfills.</p> <p>The other major role for a reference librarian is that of the “instructor”. No reference librarian wants to be the sole provider of answers - think of all the questions that would never get answered simply due to time and resource constraints! Instead, reference librarians can instruct users in library and research skills, so that they can be self-sufficient and navigate the library to find the answers and resources they need on their own. While reference librarians can lead classroom-style teaching sessions, instruction can also occur during the reference interview.</p> <p>Read the first sentence of step 5 again: “Conduct a search of the system collaboratively, sharing your screen/resource and explaining each step of the process.” By the end of the reference interview, the patron should feel more comfortable conducting a similar search on their own next time. While this may mean the current reference interview may take slightly longer, it will save time in the long run.</p> <p>There are some further minor roles that are implied by, or overlay, these two major roles, such as:</p> <ul> <li>Communicator: librarian as the human connection between a user and the resources</li> <li>Relationship builder: librarian building a long-term relationship with the user</li> <li>Guide/advisor: librarian guiding the user in selecting good and appropriate resources</li> <li>Counselor: librarian mentoring users in the research process, and treating the user as a holistic person</li> <li>Partner: librarian and user act as a team, each bringing their own knowledge and skillsets</li> </ul> <p>The final role I want to focus on is that of the librarian as an expert: this is the foundation for all of the previously outlined roles. A librarian is an expert at navigating (and cultivating!) the system that organizes the library’s resources. That expertise allows the librarian to quickly find needed information, and some portion of that expertise is what the librarian is attempting to transfer with instruction (though much of the expertise is likely to remain as <a href="https://commoncog.com/tacit-knowledge-is-a-real-thing/">tacit knowledge</a><d-footnote>"What is Tacit Knowledge? Tacit knowledge is knowledge that cannot be captured through words alone." Sidenote: I'm very happy to be linking out to some Cedric Chin content here, his essays are gold!</d-footnote>). This expertise is what the librarian brings to any research partnership, and what allows the librarian to be a guide for users.</p> <h2 id="data-professionals-are-information-professionals">Data professionals are Information professionals</h2> <p>The parallels between data professionals (especially analytics engineers) and librarians have been drawn <a href="https://www.getdbt.com/analytics-engineering/transformation/data-catalog">before</a> (<a href="https://www.getdbt.com/data-careers/adam-stone#describing-analytics-engineering-to-a-friend">many</a> <a href="https://youtu.be/T0Z_ibd3Hx0?t=46">times</a>), albeit usually by those on the data side of the divide. If you’ve read this far, I’m hoping that by now it is obvious that this is because both data professionals and librarians are really information professionals. And while data (engineer/analyst/scientist/etc) is a relatively new profession, librarianship is not, and there is a lot for the data professional to gain by considering themselves part of the wider information profession - and the larger body of knowledge and practice that comes with it.</p> <p>One such practice? The reference interview.</p> <p>Before I give my take on how data teams can learn from and adapt the reference interview, I once again want to ask you, dear reader, to pause and consider for yourself: based on everything you’ve learned so far, how can you borrow from the typical reference interview the next time a stakeholder comes to you with a new request for the data team?<d-footnote>And if you shift how the data team engages with the rest of the organization, how can this change the role of the data team?</d-footnote> Revisit the scenarios from the very start of this blog post: how would you have approached these scenarios before, and how would you approach them now from the perspective of a data-savvy reference librarian who starts every stakeholder engagement with a reference interview?</p> <p>Remember, the difference between data and information is functional - it is only by processing data into information that we give it meaning, purpose, and value. Which level do you want to be operating at? Which level does the rest of your organization think you are operating at? What do you need to do to get to the next level?<d-footnote>(A question for a future post… how can data professionals further elevate their work to be *knowledge* professionals?)</d-footnote></p> <h2 id="the-reference-interview-for-data-teams">The reference interview for data teams</h2> <p>So how can data teams learn from library science, adapt the reference interview, and elevate their practice from being data practitioners to information practitioners? I want to approach this from two perspectives: that of an individual contributor on a data team who has the power to adjust how they interact with stakeholders, and that of a data team leader who has the power to make structural and process changes that affect the environment that the individual contributors on a data team work in.</p> <h3 id="for-the-individual-contributor">For the individual contributor</h3> <p>Let’s start with the individual contributor perspective. Remember the pithy and unwanted advice I gave near the start of this post? You need to talk to your stakeholders more. Beyond that, the attitude and emotional presence you bring to those interactions matters. Your stakeholder has an information need. They know you are busy and have a dozen other projects that need your attention, and they likely have already formed some opinion about what you know and are capable of doing.</p> <p>So in an effort to be helpful and efficient, they have likely already taken that information need and refined it, restricted it, reshaped it in a query or request that they think you can do/answer in order to fulfill their information need. So here is my first and most important request to you: don’t accept that query at face value. It is <em>your</em> job, <em>not theirs</em>, to dig deeper and discover their true information need <em>before</em> you try to fulfill it.</p> <p>Here is an abbreviated and reframed version of the 6 steps to the reference interview, which you can use and adapt in order to uncover their true information need. Think of this as a formula for a structured or guided conversation:</p> <ol> <li>Listen to the initial request without interruption or judgement</li> <li>Paraphrase the request back to them</li> <li>Ask open ended questions first, followed by clarifying close-ended questions</li> <li>Restate the request yet again, now altered by their responses to your questions. Repeat until the rephrased request is confirmed as accurate</li> <li>Collaborate to find an answer to the rephrased information request, using the interviewer’s expertise in navigating the information system and the interviewee’s guidance and background knowledge. Depending on the need/relationship, either can be the “driver”.</li> <li>Verify that the true information need was met. If not, the process can be repeated, or the requestor may need to be directed to further resources if their information need is beyond the capacity of this process. Finally, you can ask for feedback on the process.</li> </ol> <p>Conversations like this (in which you are trying to unearth their true information need) will generally be more successful when you approach them with empathy, non-judgement, and genuine curiosity. Take the time to establish a rapport first. Apply the techniques of active listening. Intentionally cultivate an inclusive and collaborative space. This means that your language <em>matters</em>. Erase words like “simply”, “just”, and “easy” from your vocabulary<d-footnote>Any other words to add to the list?</d-footnote>. When you ask questions, show that you are paying close attention to their response by rephrasing and restating what you heard. Ask yourself first what assumptions you are making, and at the very least verify/clarify those assumptions or don’t bake them in to your first round of (open-ended!) questions.</p> <p>Remember: the interpersonal skills and empathetic mindset that a librarian brings to the reference interview is just as important (if not more!) to the ultimate success of uncovering the user’s true information need as the structured interview process that they follow. I want to acknowledge that this is hard, and it is a skill to develop just as vital as any technical skill, so give yourself grace and time to reflect as you start to adjust your approach. And if you feel like this is already how you engage with stakeholders, that’s fantastic! Make sure you are collecting feedback - from your stakeholders, your peers, and your managers/mentors - so you can continue to improve your approach (everyone has blindspots!).</p> <p>If this sounds preachy, I apologize, but I genuinely think this is both the hardest and most impactful skill you can develop. It is a skill I know I will need to continue to practice and improve on throughout my entire career, and it is a skill that is almost never taught in professional contexts (unless, perhaps, you were a therapist in a previous life). If you are struggling to make the jump to senior, demonstrating this skill will be one of the most important things you can do to show your manager and your team that you are someone who can handle the more nebulous projects - the kinds of projects typically given to seniors.</p> <p>Ok, so what do we have so far?</p> <ul> <li>Talk to your stakeholders more (especially at the start).</li> <li>Never assume that their initial request matches their true information need.</li> <li>Make it your goal to uncover their true information need.</li> <li>Cultivate an empathetic, non-judgemental, and curious mindset.</li> <li>Practice active listening techniques.</li> </ul> <p>What else can we adapt from the reference interview?</p> <h4 id="assuming-the-instructor-role-and-a-path-to-self-service">Assuming the Instructor role… and a path to self-service?</h4> <p>I want to pull your attention to step 5 of the librarian’s reference interview:</p> <blockquote> <p>Conduct a search of the system collaboratively, sharing your screen/resource and explaining each step of the process. Check in with the patron to see if they want to do the search themselves at any point in the process, and if so, continue to offer assistance as needed. This is the phase where you can offer instructions or advice, guiding the patron with your special expertise and knowledge.</p> </blockquote> <p>I also want to remind you of one of the major roles the reference librarian plays: the instructor. Most likely you already assume the other major role of the “information provider” when you receive an ad-hoc request, but how often do you assume the “instructor” role? What would that look like for a data analyst?</p> <p>Let’s revisit the first scenario:</p> <blockquote> <p>A stakeholder you’ve worked with before comes to you with a new request. They saw what kinds of questions could be answered with a dashboard in the last project, and now they have some new questions to try and find answers for</p> </blockquote> <p>After some investigation (read: asking a series of open-ended and then clarifying close-ended questions, rephrasing their ask, and then repeating until you understand their underlying information need), you realize that this stakeholder is trying to do some exploratory data analysis (EDA) for a new project, and they want to gut check whether the trends that they think are happening are actually happening. They don’t actually need a dashboard (or at least, not yet), but that’s how they’ve been doing EDA for the other project, and they figured another dashboard for this new dataset would let them see whether these numbers were actually going up over time.</p> <p>Do you want this stakeholder coming to the data team every time they want to do some EDA? Go that route, and you will soon have a graveyard of unviewed dashboards, not to mention the opportunity cost of devoting valuable analyst time that could’ve been spent on higher value projects. Why is the stakeholder asking for a dashboard when the underlying information need could be fulfilled more efficiently with some SQL queries? Likely, because dashboards and interactive visualizations are familiar. For the sake of this example, let’s say that this stakeholder knows some basic SQL. How could the data analyst approach step 5 of an adapted reference interview?</p> <p>The first step is to invite the stakeholder to be a partner in this EDA. Assuming that this stakeholder is not an executive, if they are asking for the data team to invest time in answering their question, they should be willing to put that time in themselves, as well. Who knows - they may even be excited at this opportunity to learn some analyst skills from an expert! Set up some pairing sessions, and use those sessions to time-box the project - you’ll only work on it during those pairing sessions. During those sessions, you will be sharing your screen and narrating your process.</p> <p>Don’t try to prepare a cleaned up version of the process - let the stakeholder see and participate in every messy step (yes, that includes googling for how that window function works again, and running queries with typos). The ultimate goal here is for the stakeholder to feel comfortable going through this process by themselves, and that includes knowing what to do when something goes wrong<d-footnote>My opinions here have been heavily shaped by The Carpentries pedagogical model - see: https://carpentries.github.io/instructor-training</d-footnote>. You want the stakeholder to lose the perception that the skilled technical data analyst is able to magically produce a dashboard with a few precise keystrokes!</p> <p>Start from the very beginning - the first thing you’ll have to do is identify the right datasets to query. You are an expert at navigating the data warehouse - you know how it is organized, the naming conventions, and how to identify high quality ready-to-use datasets. You know how to search the data catalog (or whatever other documentation there may be), and you are all set up to query the database. <strong>Your stakeholder is also an expert</strong><d-footnote>Let's say it louder for the folks in the back!</d-footnote> - they may or may not know what the data looks like, but they do have a lot of knowledge in this domain. You need to use your expertise to guide the stakeholder in the research process, and you can lean on your stakeholder to know what information they expect and want to find. You might find that by working together, you can fulfill your stakeholder’s information need better and faster than going off on your own… a 1 + 1 &gt; 2 effect.</p> <p>The next time this stakeholder has a question, hopefully they are more comfortable performing some of this process themselves - and maybe in some future request, they will ask to drive in the pairing sessions, with you providing support when needed. By inviting the stakeholder into your workflow and process, you are enabling and empowering them to use the data resources available to them more independently, freeing up your time to focus on more complex analysis.</p> <p>This is one of the foundational building blocks of true self-service - not only making the data and tools available, with the data documented and findable in a curated data catalog, but also providing the training, support, and guidance for users so that they are empowered to navigate the system by themselves or in partnership with a data expert if they so choose. Sorry, but no magic BI tool is going to be able to provide self-service if those core fundamentals aren’t in place.</p> <h4 id="gathering-requirements-with-the-reference-interview">Gathering requirements with the reference interview</h4> <p>Generally, the first step in a new project is to gather requirements. This phase is often responsible for most of the frustration and angst you experience while working on a project. How many times have you thought you reached the end of a project, only to be told that the report or dashboard you delivered does not actually meet the stakeholder’s expectations? This needs to be tweaked, that needs to be reworked… but sometimes that little tweak is a change to a fundamental assumption that requires overhauling the entire product!</p> <p>How does your team gather requirements? When does the requirements gathering start? (Likely during the initial intake phase!) Who typically performs the initial requirements analysis? Does this person have the necessary context and technical skills to gather all requirements? Is there a handoff? What problems typically occur during this handoff? When does the individual contributor who will be doing most of the work for the project get involved? How much of the requirements gathering process do they typically need to re-do? Crazy idea: draw a process map and start identifying waste in your requirements gathering process. How can you improve the experience of gathering requirements for both the data team and the stakeholders?</p> <p>As you may have guessed, I believe the reference interview has something to offer here. First, the individual contributor(s) who will be directly working on the project should be brought into the requirements gathering phase as early as possible. While a project manager or data product manager can direct and guide the process, they should not try to collect all of the requirements in a vacuum and then hand off those requirements to an analyst/engineer. Second, the analysts/engineers/managers involved in the requirements gathering should be conscientious about the emotional energy they are bringing to these stakeholder conversations: adopting an empathetic, non-judgemental, and curious mindset. Third, they should try to follow the key principles of the reference interview in these conversations: that the stakeholder is not likely to start with their true information need, to utilize active listening techniques, and to approach the process as a collaborative exercise in which all parties have special expertise to offer. The data team should iteratively develop a set of requirements and seek consensus on those requirements and the true information need.</p> <p>If you were to make one change to your typical requirements gathering process to align it closer to a reference interview, what would it be?</p> <h3 id="for-the-data-team-leader">For the data team leader</h3> <p>If you’re a data team leader, I hope that you haven’t just skipped ahead to this section - because your whole job is to cultivate an environment that encourages and supports all of the suggestions I made in the previous section for the individual contributor. So let’s review some strategies that will allow your team to start acting more like information professionals, and implement their own version of a reference interview.</p> <p>Let’s start with the most significant environmental change you can make: creating the data team’s equivalent of a reference desk. When you walk into a library with a question, do you ask the first person you see with a name tag? I mean, maybe - but any trained library page should know to then direct you to the reference desk. But most likely you look for the big desk with signs above it and a reference librarian ready and waiting behind said desk. This librarian isn’t (usually) doing other activities like cataloging or purchasing new books - if they are at the reference desk, their sole job is to help any patron who walks up to the reference desk. How can we replicate this experience for stakeholders reaching out to a (e.g. remote) data team?</p> <p>First, someone needs to be “at the reference desk”<d-footnote>I need to caveat here that this really only applies to medium-to-large data teams, as smaller data teams are unlikely to have capacity for this.</d-footnote> - a dedicated analyst/engineer/etc whose only job during that shift is to respond to stakeholder requests. We can borrow from “on-call” best practices for software engineers here as well - while a reference librarian is typically responding to new information requests and an on-call software engineer is typically responding to active issues/outages, the assigned data team member is likely to encounter both of these situations. Regardless, the data team member who is on-call/at-the-desk should not be expected to complete any typical project work during their shift, and it is your job to organize sprints and/or project planning around this.</p> <p>On the flip side, team members who are not currently on-call/at-the-desk should not be expected to respond to ad-hoc requests. The goal here is to minimize overall context switching, account and plan for all types of work (after all, we all know ad-hoc requests are inevitable), and better serve the data needs of the whole organization without sacrificing the needs of the data team itself. What virtual “signs” can you put up to direct stakeholders to the data team’s “reference desk”, to hopefully minimize the number of redirects that are needed? What culture changes are needed to shift and enforce this new social norm?</p> <p>Next, you’ll need to consider training: how can you enable data team members to be successful when it’s their turn to serve at the reference desk? Unless you have a librarian on your team (which, honestly, you should consider!), this is going to be a learning experience for everyone. Consider pairing two team members to be at the reference desk together, so they can observe and learn from each other. Collect and encourage feedback, from each other as well as stakeholders.</p> <p>Here’s a crazy idea: try going to a library with a research topic in mind, and ask a reference librarian for help. Experience the reference interview for yourself as a patron! Your experience may vary depending on the library that you go to, so try visiting a few different libraries (especially if you’re lucky enough to be close to a research university!) - try to pay attention to the overall experience from entering the library with a question to (hopefully) leaving with some resources and/or answers.</p> <p>If you’re willing to try this data team reference desk idea out, I fully encourage setting it up as an experiment. What are you trying to improve? What metrics can you track? How can you test whether this change is actually improving processes for the data team and your stakeholders? What surveys should you send out before, during, and after the experiment?</p> <h4 id="a-different-definition-of-service">A different definition of “service”</h4> <p>For many data teams - and data “thought leaders” on LinkedIn - “service” is a dirty word. You don’t want your data team to be like the typical IT service desk, where data team members are just ticket takers who fulfill stakeholder requests. No, you want your data team to be working on strategic high-value projects that will drive measurable impacts on the business. After all, how can you focus your limited resources on driving the highest impact if your team is churning out dashboards and emailed reports by the dozen?</p> <p>Let’s reframe the word “service” from an IT context to a librarian context. Going back to “Conducting the Reference Interview”, Chapter 1, the authors state unequivocally that “this book is founded upon the principle that libraries must take a service orientation.”<d-cite key="Ross_Nilsen_Radford_2019"></d-cite></p> <blockquote> <p>We are convinced that the institutions that will survive in the twenty-first century are those that serve their clients and give them the help they need. If libraries don’t provide helpful information services, users will turn to other service-providers who are more service-oriented… librarians provide a professional “value added” knowledge component that is lacking in many commercial services. The RUSA (Reference and User Services Association, 2000) “Guidelines for Information Services” state: “Provision of information in the manner most useful to its clients is the ultimate test of all a library does.”<d-cite key="Ross_Nilsen_Radford_2019"></d-cite></p> </blockquote> <p>Some useful context for this quote: there was a significant contingent of LIS researchers convinced that search engines would render the profession obsolete. While the role of librarians and the value they provide may have shifted, librarians are still around and more vital than ever. 20 years later, a similar debate is ongoing… this time focused on generative AI. The conclusion to this debate seems even more obvious than the one over search engines.<d-footnote>The librarian skillset is only going to become more and more important in this "age of AI"</d-footnote></p> <p>A side note in the book elaborates on this idea with a further citation from the literature:</p> <blockquote> <p>Herbert White (1992) says that librarians need to emphasize their strengths. As computers increasingly take over clerical tasks that computers are good at, librarians should focus attention on aspects of service involving human communication that computers can’t do well. Let computers get involved in document identification, document delivery, overdue notices, interlibrary loans and cataloging, White argues, and let librarians take a proactive role in information intermediation.<d-cite key="Ross_Nilsen_Radford_2019"></d-cite></p> </blockquote> <p>Reframed: a service-oriented data team is focused on providing information in the manner most useful to its stakeholders, and is a proactive human intermediary between the organization and its data resources. (I’ll posit that this approach is not opposite of a product-orientation, but rather complementary.)</p> <p>As a data team, you acquire, organize, curate, and retrieve data resources. The team’s collective expertise is vital to interpreting and analyzing these data resources. However, the data team is rarely (if ever) the one making the decisions (or the ones accountable for the results of said decisions). Data teams can be as strategic as possible, working directly with the C-suite on the most important objectives for the organization, but by virtue of their raison d’être, data teams will serve a supporting/helping role.</p> <p>For some, this can grate - you may want to be the “hero” data scientist, crafting machine learning algorithms that make the decisions for the business, rescuing the organization from heuristics-driven (poor) decisions and instead achieving the lofty goal of data-driven (smart) decisions. Rock, meet hard place - given human nature and organizational realities, your hero aspirations are likely a pipe dream, and the sooner you realize that humans will always be in the loop, the faster you can start making a real impact.</p> <p>How does your data team currently operate? How do you want your data team to operate in the future? What does your organization currently need out of the data team? What are your organization’s goals for how to use data in the future? How does the data team need to operate in order to help the organization achieve their data goals?</p> <p>As a data team leader, it’s your responsibility to consider these questions and create a strategic plan to ensure not only that the data team is adding net value, but that the rest of the organization recognizes that value. Do you think that the librarian’s frame for being “service-oriented” should be a part of that strategy? If analysts start new projects or respond to ad-hoc requests with some version of a “reference interview” (vs immediately building out a dashboard based on the description field in a Jira ticket), how do you think that will shift both the actual and perceived value of the data team?</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>At the start of this post, I gave a few examples of scenarios data teams may encounter:</p> <ol> <li>A stakeholder you’ve worked with before comes to you with a new request. They saw what kinds of questions could be answered with a dashboard in the last project, and now they have some new questions to try and find answers to</li> <li>You get an email from someone in the organization who wants a dashboard for their data. Right now they do a lot of manual work and one-off visualizations in a spreadsheet, and their friend in another department recently showed them a dashboard that the data team made</li> <li>An executive asks your boss for a set of very specific metrics. Your boss tells you to write some SQL queries to generate numbers for those specific metrics, so they can then email those numbers to the executive</li> </ol> <p>If you made it all the way to the end of this extremely long blog post, then I hope you can start to brainstorm how you might approach these scenarios as a librarian would. The point of this post is not to say that all data teams should adopt the reference interview for all ad-hoc requests and requirements gathering activities for longer-term planned projects. Rather, I hope that you have a new perspective on how your data team’s processes and procedures might be shifted - that you have a new hat you can wear, a new lens through which to see the world, another tool in your toolbox… whatever metaphor floats your boat.</p> <p>There are 2 key things you should remember about the purpose of the reference interview:</p> <ol> <li>A stakeholder’s first stated information need is rarely their <em>true</em> information need</li> <li>The best way to uncover their true information need is by approaching the conversation with empathy, nonjudgement, and genuine curiosity, as a partner who acknowledges everyone’s respective expertise, and by utilizing the techniques of active listening</li> </ol> <p>The idea that stakeholders rarely ask you for what they actually want is hardly a new one or unique to the reference interview. Just ask your product managers, or anyone with a few years of experience interfacing between the people who <em>want</em> things and the people who have to <em>deliver</em> those things. Hopefully this is not a revelation for you either - though if it is, it’s a valuable <a href="https://youtu.be/g0_P55Y4H_c?si=ef4zQGJl48trsU5x">lesson to learn</a><d-footnote>Shachar Meir has given talks and put out content that is very aligned with the ideas in this post, just from his own experience/perspective - I would highly encourage anyone to follow his work!</d-footnote>.</p> <p><strong>Far more important than realizing that stakeholders rarely state outright what information they actually need, is the perspective switch that it is <em>your</em> role to uncover their true information need.</strong> You will always be handicapped if you blame your stakeholders for not accurately translating their information/business request into the data team’s language.</p> <p>The reference interview provides a structured way to accomplish this vital discovery process, and it really doesn’t take too much translation to go from a librarian-performed reference interview to a data analyst-performed reference interview.</p> <p>To review (in short), this structure is:</p> <ol> <li>Listen to the initial request without interruption or judgement</li> <li>Paraphrase the request back to them</li> <li>Ask open ended questions first, followed by clarifying close-ended questions</li> <li>Restate the request yet again, now altered by their responses to your questions. Repeat until the rephrased request is confirmed as accurate</li> <li>Collaborate to find an answer to the rephrased information request, using the interviewer’s expertise in navigating the information system and the interviewee’s guidance and background knowledge. Depending on the need/relationship, either can be the “driver”.</li> <li>Verify that the true information need was met. If not, the process can be repeated, or the requestor may need to be directed to further resources if their information need is beyond the capacity of this process. Finally, you can ask for feedback on the process.</li> </ol> <p>If you have the time, take a moment to think about each of these 3 scenarios. How would your team approach them today? How would a reference librarian approach them? Role play is an extremely effective way to learn and practice new skills - grab a partner and try conducting a reference interview for each of these scenarios!</p> <p>This post had a lot of questions for you, the reader - many of which were left unanswered, because every reader will have a different answer. I hope that you put some time into considering these questions, and the next time you get one of those annoying ad-hoc requests your memory might be jogged, and something will prompt you to put on your reference librarian hat. (There are certainly worse hats to wear!) I hope you will experiment with the reference interview method - adapt it and make it your own. And I hope that you consider the time that you spent reading this post well spent - thank you for reaching the end!</p> <p>If after reading this post you are intrigued enough to try any of these strategies for yourself or your team, I would love to hear about it! Please reach out via email, on socials, or by leaving a comment on this post. I would love to hear your story.</p> <h3 id="special-thanks">Special thanks</h3> <p>Most students in an MSLIS program take a class called “Reference &amp; Information Services”. It’s a core part of the curriculum, if not required. Many MSLIS graduates go on to work in a library and gain experience in conducting the reference interview. Unfortunately, neither applies to me - I veered hard into the “information management” side of my program, and then kept on veering straight into the data field. However, I do have the great fortune of having a group of friends from my cohort who did take Reference, and who did go on to be librarians who conduct reference interviews. When the idea for this blog post was first taking shape, they were the first people I went to for help and resources. They sent me their class syllabus and readings, and patiently answered all of my questions about the reference interview based on their knowledge and experiences. Thank you for the references and the conversations - I wouldn’t have been able to write this without you. To Yvonne Freebourn, Taylor VanTryon, Adam McConville, and Christine Willson, UIUC iSchool Class of 2020.</p> <p>Also<d-footnote>Thank you to the family, friends, and colleagues who read a draft of this (absurdly long) post in advance and offered feedback/edits/thoughts. I appreciate y'all &lt;3</d-footnote>, special shoutout to Amalia Child for <a href="https://locallyoptimistic.com/post/the-five-laws-of-data-enablement-how-the-father-of-library-science-would-make-his-data-team-indispensable/#a70ebd47-9401-411b-878d-923fe05d6606">citing this post</a><d-footnote>at long last, your footnote can have a link :D</d-footnote> long before it even existed, and fighting the good fight with me to spread the wisdom of Library &amp; Information Science to the data world.</p> <h2 id="appendix">Appendix</h2> <div class="row"> <div class="col-sm md-auto"> <a href="/assets/pdf/Data-Day-2026.pdf"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx26-titleslide-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx26-titleslide-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx26-titleslide-1400.webp"/> <img src="/assets/img/ddtx26-titleslide.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Data Day Texas 2026 Talk Slides" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> PDF of the slides presented during my Data Day Texas 2026 talk </div>]]></content><author><name>Jenna Jordan</name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[What data teams can learn from the reference interview librarians perform to identify and serve true information needs]]></summary></entry><entry><title type="html">Data Day Texas 2025 - Key Takeaways</title><link href="https://jennajordan.me/blog/ddtx25-recap" rel="alternate" type="text/html" title="Data Day Texas 2025 - Key Takeaways"/><published>2025-02-24T00:00:00+00:00</published><updated>2025-02-24T00:00:00+00:00</updated><id>https://jennajordan.me/blog/ddtx-recap</id><content type="html" xml:base="https://jennajordan.me/blog/ddtx25-recap"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_ddtx25-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_ddtx25-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_ddtx25-1400.webp"/> <img src="/assets/img/header_ddtx25.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The data professionals are not okay. We may put on a good face and talk about how excited we are that AI is going to change the world, but behind that mask of professional enthusiasm, we are anxious. Nowhere was this more evident than at the Data Day Texas Town Hall, hosted by Joe Reis and Matt Housley for the 3rd year running. Are executives going to replace us with AI? We knew that AI would likely impact other industries, but will it also reduce job opportunities in the industry responsible for building AI systems? AI is supposed to improve our productivity, to be a tool that we use to do bigger and better things, not a cost-saving mechanism for corporate executives trying to show that this quarter’s numbers are better than expected… right? We talk about how Copilot is just like a super productive intern - but what about the actual interns? If we stop hiring juniors and only hire seniors and supplement them with code generators, what happens in 10 years when suddenly there are no seniors to hire, because they couldn’t get a junior role? Even before that, are students learning the right skills to get hired into an entry-level role? Computer Science programs have been in crisis for a while now, curriculums more and more outdated with every passing year, and students are starting to question the value of a 4-year degree when they can only learn the skills jobs are asking for in bootcamps. Are we, the data professionals who have seized on AI trends as a way to justify our budgets (after all, your AI system is only as good as your data!), in fact building ourselves out of our jobs (not because we should really be replaced, but because the AI is cheaper and seems to be good enough), pulling up the career ladder behind us (why hire juniors if they won’t be productive enough for years), and undercutting the entire higher education system without anything to replace it?</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_reis-housley-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_reis-housley-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_reis-housley-1400.webp"/> <img src="/assets/img/ddtx25_reis-housley.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Yeah, we’re anxious - all of these concerns and more were raised in the Town Hall. Of course, there were also the AI optimists, who debated the AI doomers with cheerful counterpoints that a utopia of sky-high productivity and dramatically more free time was just around the corner (though I’m reminded that Keynes thought we’d be working 15 hour work weeks by now), that AI would solve far more problems than it caused (hopefully it starts with the climate crisis, and codes itself into being carbon neutral), and that human ingenuity and adaptability would always prevail, and jobs will just look different rather than going away (though this tends to work across generations, individuals still get left behind). As Ole pointed out the next day during discussions, it’s ironic that the nation with one of the worst social safety nets is charging ahead with AI so determinedly while other developed nations where you can lose your job without worrying you may also lose your life have been dragging their feet on AI - one would think that paradigm should be flipped.</p> <p>While the Town Hall gave DDTX attendees a chance to give voice to these anxieties in a massive group Data Therapy session (classic Joe Reis), the actual talks offered guidance on how to navigate this next year - the skills data professionals need to focus on, new concepts we need to learn and integrate into our practices, and some old technologies made new again that will be especially relevant in building AI that will actually live up to our hopes (and not our fears). I found the Town Hall comforting - to know that I wasn’t the only one anxious about these things - and the talks inspiring, charting a path forward that I am actually excited for. My experience at Data Day Texas was heavily biased by my interests, given that there were 6 talks happening at once throughout the day (and then 3 discussions at once the next day) and nothing was recorded, but I got a chance to learn about at least some of the talks I couldn’t see live through conversations with the other attendees. (There was one discussion that was recorded due to being live streamed - a town hall style session hosted by Joe Reis and the Super Data Brothers on AI in BI, <a href="https://www.linkedin.com/events/livefromdatadaytexaswithjoereis7289365143958794241/theater/">available here</a>).</p> <p>There were 3 key themes tying the talks together that I noticed:</p> <ol> <li>Learnings from Library &amp; Information Science (and, context is still king)</li> <li>“Human” (soft) skills help us connect with the business (and help them)</li> <li>Data Mesh is a journey, and we’re now in the land of Data Products</li> </ol> <h2 id="learnings-from-library--information-science-and-context-is-still-king">Learnings from Library &amp; Information Science (and, context is still king)</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_talisman-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_talisman-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_talisman-1400.webp"/> <img src="/assets/img/ddtx25_talisman.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If there was one key takeaway that attendees at Data Day Texas walked away with, it was this: there is a lot that data professionals have to learn from Library &amp; Information Science. Don’t believe me? (I am, after all, quite biased on this.) Ask Joe Reis, Matt Housley, Juan Sequeda, and Tony Baer (podcast episode <a href="https://www.linkedin.com/posts/josephreis_if-you-missed-data-day-texas-well-you-missed-activity-7292311431192973330-XUJ6?utm_source=share&amp;utm_medium=member_desktop">here</a>). In this Age of AI, context is king, and that context must be machine-readable. It’s common knowledge that your AI systems are only as useful as the data they are trained on, but DDTX attendees learned that the data in your data warehouse - no matter how carefully curated, modeled, and quality tested - is not enough. You also need the context - the knowledge - that often exists only in people’s heads. In addition to Data Engineers, you now need Knowledge Engineers. You’ve heard of “moving up the stack”? Time to also move up the <a href="https://www.isko.org/cyclo/dikw">DIKW pyramid</a>, and data is at the bottom. What does it mean for context to be machine readable? Think metadata - and then take it one step further and think of metadata in knowledge graphs. Think of text (in all its myriad forms, from books to emails to human speech), the primary method we have for conveying knowledge. We are re-realizing the importance of data modeling in this Age of AI, so how can we model text data? As Bill Inmon explained, with ontologies and taxonomies. Do you know what field has cultivated and expanded the body of knowledge around metadata, knowledge graphs, ontologies, and taxonomies? Library &amp; Information Sciences. As Tony Baer articulated (article <a href="https://siliconangle.com/2025/02/04/data-generative-ai-era-need-knowledge-engineers/">here</a>), the Knowledge Engineer (a role Juan Sequeda introduced in his talk) is an AI-driven evolution of the classical Reference Librarian.</p> <h3 id="highlighted-talks">Highlighted Talks:</h3> <ul> <li><a href="https://datadaytexas.com/2025/sessions#olesen-bagneux">Meta Grid - metadata management as an understanding of what already is and embracing it</a>, by <a href="https://www.linkedin.com/in/ole-olesen-bagneux/">Ole Olesen-Bagneux</a> (<a href="https://www.linkedin.com/posts/ole-olesen-bagneux_metadata-keynote-activity-7290080615155036161-sQxr?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAiK2FABKHqWAaEnq7CdATQwY_GSauRe0Xg">slides</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#talisman">We Are All Librarians: Systems for Organizing in the Age of AI</a>, by <a href="https://www.linkedin.com/in/jmtalisman/">Jessica Talisman</a> (<a href="https://www.linkedin.com/posts/jmtalisman_we-are-all-librarians-systems-for-organizing-activity-7290900261626122240-VzdN?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAiK2FABKHqWAaEnq7CdATQwY_GSauRe0Xg">slides</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#sequeda">How to Start Investing in Semantics and Knowledge: A Practical Guide</a>, by <a href="https://www.linkedin.com/in/juansequeda/">Juan Sequeda</a> (<a href="https://www.linkedin.com/posts/juansequeda_ddtx25-activity-7288967087023144960-Fchp?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAiK2FABKHqWAaEnq7CdATQwY_GSauRe0Xg">summary</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#inmon">How to become a hero - the journey to text</a>, by <a href="https://www.linkedin.com/in/billinmon/">Bill Inmon</a> (<a href="https://www.amazon.com/Turning-Text-into-Gold-Taxonomies/dp/1634621662/ref=sr_1_1">book</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#nguyen">Context Engineering: A Framework for Data Intelligence</a>, by <a href="https://www.linkedin.com/in/andrewnguyensf/">Andrew Nguyen</a> (<a href="https://drive.google.com/file/d/1VmnmxHNgM7Al27TZk4cizpzCuMEzitbI/view?usp=sharing">slides</a>)</li> </ul> <h2 id="human-soft-skills-help-us-connect-with-the-business-and-help-them">“Human” (soft) skills help us connect with the business (and help them)</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_morrow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_morrow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_morrow-1400.webp"/> <img src="/assets/img/ddtx25_morrow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Data teams have spent the last few years struggling to prove ROI and justify their existence on the balance sheet, after costs associated with the “modern data stack” spiraled out of control. We may have fancier tools than ever before, but complaining how the business isn’t “data-driven” enough and throwing data products over the wall for the business to “self-service” isn’t resulting in the ROI we were hoping for. Another takeaway that DDTX attendees walked away with? An acknowledgment that we need to go back to basics and get better at communicating with the business. Yes, talking to people is a skill, and while most people consider it a “soft skill”, it may be among the harder skills you have to learn - but also one of the most important to progressing in your career and growing the impact of your data team. If you don’t want your data team to be treated like an IT service desk - someplace people submit tickets to and expect dashboards in return - but rather a strategic partner that curates the organization’s collective knowledge, then you will need to start valuing “human” skills as much as you do “technical” skills. I’ll note that last year’s opening keynote from Sol Rashidi (“<a href="https://datadaytexas.com/2024/sessions#rashidi">Practitioner turned Executive; what caught me by surprise &amp; lessons I learned about how decisions are really made with data ecosystems</a>”) focused on this takeaway as well, and it is clearly something the data community is still grappling with.</p> <blockquote> <p>“I’ve learned that people will forget what you said, people will forget what you did, but people will never forget how you made them feel.”</p> <p>~ Maya Angelou</p> </blockquote> <p>If you want to communicate and present effectively to executives and key business stakeholders, focus on crafting your story around how you want them to feel, and focus less on specific verbiage. Never underestimate the power of empathy.</p> <h3 id="highlighted-talks-1">Highlighted Talks:</h3> <ul> <li><a href="https://datadaytexas.com/2025/sessions#virtanen">Bridge Skills: The Hardest Problem Tech Still Can’t Solve,</a> by <a href="https://www.linkedin.com/in/eevamaijavirtanen/">Eevamaija Virtanen</a> (<a href="https://open.spotify.com/episode/47H43u3lC1O7T9ETlLiEDY?si=IJvgHCJxTGSXYoqjkkvmRw">podcast episode</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#annie-nelson">The human side of data: Using technical storytelling to drive action</a>, by <a href="https://www.linkedin.com/in/annie-nelson-analyst/">Annie Nelson</a></li> <li><a href="https://datadaytexas.com/2025/sessions#morrow">Elevating Data in the Business - Bring Data and AI Skills to Life</a>, by <a href="https://www.linkedin.com/in/jordanmorrow/">Jordan Morrow</a></li> <li><a href="https://datadaytexas.com/2025/sessions#sullivan">Empowering Change: Building and Sustaining a Data Culture from the Ground Up</a>, by <a href="https://www.linkedin.com/in/dr-clair-sullivan/">Clair Sullivan</a> (<a href="https://github.com/cj2001/ddt_2025/blob/cc4c7729fd22bd48ba3819efbf3848e23f2148fb/data_culture_talk/ddt%202025%20data%20culture%20sullivan.pdf">slides</a>)</li> </ul> <h2 id="data-mesh-is-a-journey-and-were-now-in-the-land-of-data-products">Data Mesh is a journey, and we’re now in the land of Data Products</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_freeman-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_freeman-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_freeman-1400.webp"/> <img src="/assets/img/ddtx25_freeman.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Two years ago Zhamak Dehghani gave the opening keynote for Data Day Texas on Data Mesh, a socio-technical approach to building a decentralized architecture for analytical data. The four pillars of Data Mesh (Domain-oriented ownership, Data as a product, Self-serve data platform, Federated computational governance) are meant to work together and reinforce each other, but when building a Data Mesh from scratch you need to start somewhere. Businesses are already roughly divided into domains (and dotted lines can be drawn until more permanent re-orgs fully align business units with domains), and the next step is to create Data Products (or, treat data as a product). Without Data Products, you can’t really have a self-serve data platform, and there is nothing to govern in a federated computational manner. Data Products seem to be the most visible and lasting legacy of the Data Mesh approach, but I think that is because right now we are in Data Product land (so that is all we see around us). Maybe once we all agree on what a Data Product is and what is needed to make Data Products, we can start seeing actual self-serve data platforms and federated computational governance on the horizon. What does it mean for data to be a product? Zhamak Dehghani has a <a href="https://martinfowler.com/articles/data-mesh-principles.html#DataAsAProduct">definition</a>, DJ Patil had the original <a href="https://www.kdnuggets.com/2012/08/dj-patil-data-jujitsu.html">definition</a>, and perhaps we still don’t know, as JGP gathered us all together on Sunday to discuss and try to arrive at a new <a href="https://medium.com/data-mesh-learning/defining-data-products-a-community-effort-77363611e5c5">definition</a>. One thing is clear, though: we all want to build Data Products, and those doing data engineering and data governance need to start learning how to adopt a product mindset. You don’t need to become a product manager, but you should be able to put on a product manager hat or see what you are building through a product management lens. The first stop through the land of Data Products? Data Contracts - the further upstream the better. Data Contracts should serve as the source of truth for our data’s metadata, and they provide the foundation for any computational data governance implemented further on in our journey.</p> <h3 id="highlighted-talks-2">Highlighted Talks:</h3> <ul> <li><a href="https://datadaytexas.com/2025/sessions#perrin">Data Mesh is the Grail, Bitol is your Journey</a>, by <a href="https://www.linkedin.com/in/jgperrin/">Jean-Georges Perrin</a> (<a href="https://www.linkedin.com/posts/ryandolley_jean-georges-perrin-explains-how-data-contracts-activity-7289034032086986753-u6QT?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAiK2FABKHqWAaEnq7CdATQwY_GSauRe0Xg">summary</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#freeman">Introduction to Data Contracts</a>, by <a href="https://www.linkedin.com/in/mafreeman2/">Mark Freeman</a></li> <li><a href="https://datadaytexas.com/2025/sessions#hawker">Data Governance – It’s Time to Start Over</a>, by <a href="https://www.linkedin.com/in/malhawker/">Malcolm Hawker</a></li> <li><a href="https://datadaytexas.com/2025/sessions#data-death-cycle">Escape the Data &amp; AI Death Cycle, Enter the Data &amp; AI Product Mindset</a>, by <a href="https://www.linkedin.com/in/anneclairefortinbaschet/">Anne-Claire Baschet</a> and <a href="https://www.linkedin.com/in/yoann-benoit/">Yoann Benoit</a> (<a href="https://open.spotify.com/episode/2aKuthOCS6K1mjnCbLkmsk?si=2OehWjdbTGC3gNrH1OT7xg">podcast episode</a>)</li> <li><a href="https://datadaytexas.com/2025/sessions#cao2">Fundamentals of DataOps</a>, by <a href="https://www.linkedin.com/in/lisancao/">Lisa Cao</a></li> </ul> <h2 id="other-themes-of-note">Other themes of note</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_belanger-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_belanger-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_belanger-1400.webp"/> <img src="/assets/img/ddtx25_belanger.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Remember how I said I had to choose just 1 talk of 6 for every time slot? There were a lot of talks I did not get a chance to see. But from what I observed, there are 2 more themes that were prevalent at DDTX: Geeking out over Graphs (of course, this is a tradition at DDTX) and AI in Production (which, not infrequently, overlaps with graphs). A different attendee with different interests may very well say that AI in Production was the most dominant theme, and Geeking out over Graphs was a far more significant theme than those I’ve highlighted, but I can only speak to the talks and discussions I attended, and as much as I may wish I had a time turner (if only for conferences!) … alas, I do not. You can see the full agenda <a href="https://datadaytexas.com/2025/schedule">here</a>. I heard a lot of great things about <a href="https://www.linkedin.com/in/chiphuyen/">Chip Huyen</a>’s talk, <a href="https://datadaytexas.com/2025/sessions#huyen">From ML Engineer to AI Engineer</a>, which was heavily influenced by the book she just wrote, <a href="https://www.oreilly.com/library/view/ai-engineering/9781098166298/">AI Engineering</a>. If you missed the talk (like me), you can at least listen to Joe Reis interview Chip about it on his <a href="https://youtu.be/KODUhWnST_4?si=V7db-dnTgAk4iyOF">podcast</a>. I also heard some great stories coming out of <a href="https://www.linkedin.com/in/michelleyulleyi/">Michelle Yi</a>’s talk <a href="https://datadaytexas.com/2025/sessions#all-your-base">All Your Base Are Belong To Us: Adversarial Attack and Defense</a> and <a href="https://www.linkedin.com/in/hala-nelson/">Hala Nelson</a>’s talk <a href="https://datadaytexas.com/2025/sessions#hala-nelson">Adopting AI in a Large Complex Organization- Aspiration vs Reality</a>. One talk I did catch was <a href="https://www.linkedin.com/in/krbelanger/">Keith Belanger</a>’s on <a href="https://datadaytexas.com/2025/sessions#belanger">Data Modeling in the Age of AI</a> - the conclusion? LLMs are not going to do your data modeling for you, because data modeling is about understanding business context and thinking strategically. The talk was a spinoff from <a href="https://cioinfluence.com/data-management/the-future-of-data-modeling/">this article</a> and argued that data modeling (by humans) is more important than ever, and we can take advantage of AI by automating mundane tasks, operating faster and more accurately, allowing us to spend more time focusing on real business value.</p> <p>Honestly, I could keep going - this is a conference where you could trip and discover a new golden nugget of wisdom. (Side note, if you attended DDTX and have any bootleg recordings of any of the talks, shoot me a DM, because I want to listen to them all). I will say that looking at the agendas for past years, both of these themes show up over and over again, so while the 3 themes I identified may not have been the most dominant (overall), I do think they were the most distinct for this year (compared to previous years).</p> <h2 id="diving-deeper-into-my-favorite-theme-learnings-from-library--information-science">Diving deeper into my favorite theme: Learnings from Library &amp; Information Science</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_ole-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_ole-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_ole-1400.webp"/> <img src="/assets/img/ddtx25_ole.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This year’s keynote (the one talk with no competition for your time) was delivered by Ole Olesen-Bagneux, author of The Enterprise Data Catalog and (upcoming) Fundamentals of Metadata Management. Ole has a BA, MA, and Phd in Library &amp; Information Science, and after listening to his talk the entire audience of over 500 data professionals walked away knowing that Library &amp; Information science, libraries, and reference librarians have an important role to play in how we work with data. Ole presented about an architecture for metadata that he has coined the “Meta Grid” (see the slides <a href="https://www.linkedin.com/posts/ole-olesen-bagneux_metadata-keynote-activity-7290080615155036161-sQxr?utm_source=share&amp;utm_medium=member_desktop">here</a>), and learnings from Library &amp; Information Science were central to his conception of the Meta Grid. Ole presented the Meta Grid as the 3rd wave of decentralization - the first being microservices, and the 2nd being data mesh. The Meta Grid acknowledges that an organization has many metadata repositories scattered throughout, and proposes that the best solution is not to centralize the metadata into one monolithic catalog, but rather to connect these silo’d and uniquely shaped metadata repositories while maintaining their inherent decentralization. The result? An IT landscape that runs more smoothly, at reduced cost, and is better positioned to ensure data privacy, security, and innovation. If the connection to libraries seems unclear, consider this: the vast majority of a librarian’s work boils down to managing, curating, and navigating metadata, while operating within an network of decentralized yet connected library systems. Ole has spent the past year figuring out how to best articulate and explain the Meta Grid (and writing a book about it) to an audience that at first tries to understand what it is, and then wonders what he is trying to sell. And as Ole reiterated during the keynote, he is not trying to sell or productize anything - he is simply trying to alleviate a deep and enduring pain every organization at large enough scale experiences with their IT landscape, by applying learnings from Library &amp; Information Sciences to corporate metadata management.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_talisman2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_talisman2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_talisman2-1400.webp"/> <img src="/assets/img/ddtx25_talisman2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>While Ole opened everyone’s mind to the idea that they might have something to learn from Library &amp; Information Science, Jessica Talisman hammered it home with her talk in the afternoon, titled “We Are All Librarians: Systems for Organizing in the Age of AI” (see the slides <a href="https://www.linkedin.com/posts/jmtalisman_we-are-all-librarians-systems-for-organizing-activity-7290900261626122240-VzdN">here</a>). Jessica opened with the argument that if you organize, research, educate, track provenance (lineage), and retrieve information (as most data professionals do), then you are a librarian. And in the age of AI, you would really benefit when performing these librarian activities if you also had some of the tools in a librarian’s toolbox for building a true semantic information system. This is not the first time Jessica has talked about the importance of bringing Library &amp; Information Science skills into AI work - last year she gave a talk titled “What Data Architects and Engineers can learn from Library Science” (recording available <a href="https://youtu.be/f1J2yJLyEpo?si=UvNtx2TgrS1A9LHW">here</a>). This year’s talk built on last year’s, and focused in on how to actually approach building an ontology (hint: you don’t start at the ontology step) in a series of steps that Ole suggested afterwards be coined the “Ontology Pipeline”. By starting with a controlled vocabulary, associating those terms in thesauri, then further organizing their relationships in a taxonomy, you will be much better prepared to create the actual final ontology. Jessica highlighted OpenRefine as a tool to mine for themes &amp; terms, and dove into a high-level overview of Linked Data to show the true power of ontologies in an open world (Wikidata being a prime example). She concluded with two pointed citations: (1) that AI systems are cultural technologies (and therefore rely on knowledge management) from <a href="https://www.youtube.com/live/k7rPtFLH6yw?si=ShNo1WttoFicGfM5">Alison Gopnik</a>, and (2) “The Semantic Web is not a separate Web but an extension of the current one, in which information is given well-defined meaning, better enabling computers and people to work in cooperation” from <a href="https://www-sop.inria.fr/acacia/cours/essi2006/Scientific%20American_%20Feature%20Article_%20The%20Semantic%20Web_%20May%202001.pdf">Tim Berners-Lee et al.</a></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ddtx25_sequeda-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ddtx25_sequeda-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ddtx25_sequeda-1400.webp"/> <img src="/assets/img/ddtx25_sequeda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In a very similar vein to Jessica’s talk, Juan Sequeda also focused on the “how” for knowlege management and semantic technologies - only briefly reviewing the “why” (which was the focus of his <a href="https://youtu.be/EWe_gasKxlI?si=didH3OggjLSxEK4E">Coalesce 2024 talk</a>). And much like how Jessica opened her talk by listing out activities many data professionals do and arguing that these are foundational activities for librarians (thus, we are all librarians), Juan listed out many such activities (data cleaning, data transformation, standardization, relation extraction, data governance) and argued that this is <em>socio</em>-technical data &amp; <em>knowledge</em> work (thus, we are all knowledge engineers). He defines semantics as a function that gets applied to data, and results in data with knowledge (or, semantics injects knowledge into data). What is the first step to applying semantics? Be more socratic and start asking why - both on a business and personal level (be empathetic &amp; curious… talk to people!). The highest virtue Juan advocates for is reuse: ad-hoc is the enemy of efficiency, and we should focus on incentivizing the quality &amp; reusability of our data products (and the knowledge that is required to use it!). Two acronyms to remember here: DRY (Don’t Repeat Yourself) and FAIR (Findable, Accessible, Interoperable, Reusable). Juan also highlights a methodology that he first started practicing a decade ago around building knowledge graphs, but is more broadly applicable to “knowledge engineering” work. It has three phases, with a business question as the entry point: Start by capturing the knowledge that the business currently has… then implement it through data modeling/transformation and injecting semantics (possibly via knowledge graphs &amp; ontologies)… and finally make that knowledge accessible through data products and catalogs… but focus this entire process around answering a single business question at a time (then do a 2nd, then a 3rd, and maybe even more questions get answered along the way). With reusability comes another virtue Juan emphasizes: that 1+1 should be &gt;2 (aka economies of scale). Why use knowledge graphs? Because everything is connected, and knowledge graphs are the best way to capture all of those relationships, and surfacing these connections in a data catalog is the ultimate way to maximize the reusability of this knowledge. Much like it is the data engineer’s responsibility to steward data through the data engineering lifecycle, organizations should employ knowledge engineers who are responsible for capturing, implementing, and surfacing knowledge.</p>]]></content><author><name></name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[Key Themes and Takeaways to recap my experience at Data Day Texas 2025 (DDTX25)]]></summary></entry><entry><title type="html">dbt Community Spotlight</title><link href="https://jennajordan.me/2024/11/04/dbt-community-spotlight.html" rel="alternate" type="text/html" title="dbt Community Spotlight"/><published>2024-11-04T00:00:00+00:00</published><updated>2024-11-04T00:00:00+00:00</updated><id>https://jennajordan.me/2024/11/04/dbt-community-spotlight</id><content type="html" xml:base="https://jennajordan.me/2024/11/04/dbt-community-spotlight.html"><![CDATA[<p>Redirecting to the dbt Community Spotlight at: https://docs.getdbt.com/community/spotlight/jenna-jordan</p>]]></content><author><name></name></author><summary type="html"><![CDATA[dbt Community Spotlight feature from Nov 2024]]></summary></entry><entry><title type="html">Reflecting on my tenure at the City of Boston</title><link href="https://jennajordan.me/blog/cob-reflection" rel="alternate" type="text/html" title="Reflecting on my tenure at the City of Boston"/><published>2024-04-08T00:00:00+00:00</published><updated>2024-04-08T00:00:00+00:00</updated><id>https://jennajordan.me/blog/cob-reflection</id><content type="html" xml:base="https://jennajordan.me/blog/cob-reflection"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_bostoncityhall-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_bostoncityhall-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_bostoncityhall-1400.webp"/> <img src="/assets/img/header_bostoncityhall.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote class="block-tip"> <h5 id="featured-content">Featured Content</h5> <p>This post was featured in <a href="https://roundup.getdbt.com/p/dispatches-from-the-frontiers">The Analytics Engineering Roundup weekly newsletter</a> on April 28, 2024.</p> <p>Thank you Jason Ganz for the shoutout!</p> </blockquote> <p>March 1st, 2024, was my last day working as a data engineer on the City of Boston Analytics Team. For the past month or so, I’ve been taking some time to reflect, and appreciate just how much I have learned and accomplished during my 2-year tenure at City Hall. This blog post is an attempt to document my growth - and also provide an outlet for the thoughts that have been bouncing around in my brain. So while I do want to let any curious readers know what my journey was like kicking off my career as a mission-driven data engineer, I also reserve the right to meander a bit.</p> <p>While I did a couple shorter-term independent contracting gigs between earning my degree and working for the City, I consider my role as a data engineer on the City of Boston Analytics Team as my first “real” data job - first time working with a data warehouse, first time working collaboratively with other data engineers (and first time creating &amp; reviewing Pull Requests), first time creating production ETL pipelines with an orchestration platform (and maintaining workflows &amp; pipelines other people created)… all of the firsts that come with working as a part of larger data team rather than completing an entire data project by myself. And besides giving me the opportunity to kickstart my career as a data engineer, working for the City also solidified my passion for civic tech. Knowing that my daily work is directly tied to a mission of improving people’s lives is a significant motivator for me - and that will continue to shape the choices I make in my career.</p> <p>Looking back, I can divide my tenure into 2 halves:</p> <p>(1) the first year, during which I learned how to become an effective data engineer on the Analytics team - learning the tools, processes, and standard practices; and</p> <p>(2) the second year, during which I was able to implement improvements to our tools, processes, and standard practices - and ended up re-architecting our data warehouse &amp; ELT pipelines along the way.</p> <p>So, let’s meander in (mostly) chronological order.</p> <h3 id="tldr">tl;dr</h3> <p>Maybe you don’t feel like going on a journey with me at the moment, and you just want the highlights - this section is just for you.</p> <p>What I learned:</p> <ol> <li>How to work with data warehouses (ELT, workflow orchestration, flow of transformations, testing the data, etc)</li> <li>Patterns of collaboration &amp; communication with fellow engineers (e.g. PR flow &amp; etiquette, documenting work), analysts (e.g. division of responsibilities, data modeling), and stakeholders (e.g. requirements gathering, advocating for data engineering best practices)</li> <li>The formal process of process improvement - how to develop and communicate innovative projects that will improve daily processes</li> <li>How to start a dbt project from scratch and iteratively grow its architecture &amp; adoption</li> <li>How to present a fairly technical data engineering process improvement project to teammates, leadership, and a wider audience</li> </ol> <p>What I accomplished:</p> <ol> <li>Starting &amp; growing the City Analytics Exchange, a casual network &amp; meetup group of data analytics practitioners in local government</li> <li>Teaching workshops (from Software Carpentries) on core technical skills like git, SQL, and python to fellow City employees (part of a larger data culture initiative)</li> <li>Integrating dbt into the data engineering team’s tech stack, and re-organizing the data warehouse along the way</li> <li>Presenting about the dbt migration project at Coalesce 2023</li> <li>Collaborating across many teams &amp; individuals in the department to deliver the team’s first data catalog as an official product supported by the department (SSO, boston.gov URL, data governance work initiated to support continued development)</li> </ol> <p>Links to look at next:</p> <ol> <li>Blog post: <a href="/blog/analytical-data-warehouses">Analytical Data Warehouses - an introduction</a></li> <li>Project page: <a href="/projects/dbt-migration-cob">dbt Migration at City of Boston</a></li> </ol> <h2 id="the-first-year-aka-the-year-of-learning">The first year (aka the year of learning)</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/boston_first_day-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/boston_first_day-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/boston_first_day-1400.webp"/> <img src="/assets/img/boston_first_day.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Selfie from my very first day at City Hall (Jan 24, 2022) </div> <p>When I came into this role, I had no prior experience working with a data warehouse, but a decent amount of experience working with databases. For me, learning to work with an analytical data warehouse was a paradigm shift - I had the core technical skills, but it felt like the conceptual foundation I had built on how to work with databases turned to sand beneath my feet.</p> <p>As a student, I had learned how to design for and query normalized transactional databases. It had become second nature for me to understand new data by putting it into 3rd Normal Form, and implementing strong constraints on a database was a given. Have messy data? Design a relational model, then clean and wrangle the data into shape using python to output normalized, tidy tables that could then be visualized and analyzed. This was all fine for independent data projects, but it meant that I didn’t even realize I would need a whole new way of conceptualizing data work once I started working on a data team with a data warehouse. The first few months were a shock to the system!</p> <p>I think there were two pivotal things that helped me adjust and reframe how to work with data in a more collaborative ecosystem: (1) taking the month-long synchronous online course “Analytics Engineering with dbt” from CoRise, taught by Emily Hawkins and Jake Hannan, and (2) staying connected with and learning from the data community online via social media. The combination of actively learning in more of a “classroom” environment, observing and absorbing data professionals talking about their work in a public online forum, and then being able to see how those learnings applied during my daily work at the City, all together had the effect of accelerating and supercharging my learning and re-orienting process. I was able to construct a new conceptual foundation for working with analytical data warehouses within a relatively short time, and then I could continue to iterate and build on that new foundation.</p> <p>I owe a lot of thanks to Emily and Jake for answering my questions and having conversations with me during the CoRise course as I was building my conceptual foundation - those conversations and the resources they pointed me to were really helpful! I took the CoRise course in February/March, less than a month after starting at the City.</p> <p>Before taking the course, all I knew was that dbt was popular in the industry and I should probably learn it. It was a very happy coincidence that I had signed up for the dbt CoRise course a while before even starting at the City - I had been hearing about dbt online but didn’t really understand what it was for. It took my brother convincing me over Thanksgiving vacation that if I wanted to work on a data analytics team I really needed to learn dbt, even if I didn’t know what exactly it was used for. So for the sake of “professional development”, I signed up for the CoRise course in hopes that I could at least learn what was even the point of this new tool everyone in the data community was so enamoured of.</p> <p>After the course, not only did I understand what dbt was, but I understood why it was so valuable when working with a data warehouse… and how it could help improve the data engineering team’s workflows. It took about a year to get to the point of even starting that journey, but the seed was planted pretty early on.</p> <p>While my knowledge about databases and data modeling has grown a lot over the past couple years, I have still held on to a few biases due to my roots in 3NF. First, I still think that the best thing you can do with messy source data is to normalize it. A normalized relational data model provides a strong and flexible foundation that can be queried and transformed into whatever shape you need it to be next. Second, primary keys are essential. Foreign keys and check constraints may be exceedingly rare in a data warehouse, but primary keys are the one contraint that will always be useful. The idea of primary keys being “unenforceable” and “just for documentation” gives me the heebie-jeebies. I understand why (performance), I just don’t like it.</p> <p>On the flip side, there are some biases I started with that I have done a complete 180 on. When I first started as a data engineer, I wanted to do everything in python. I was very comfortable with pandas, and you could do much more complicated data transformations in a python script, so I thought that python was the best choice. Now I know that SQL is king - if you can keep it in the database and do the transformation in a SQL query, don’t even think about using python. Also, there’s no need to jump straight from source data to the final dataset in one long, complicated transformation when you can also get there with a few more steps that are simpler and modular - some of those smaller transformation steps may be re-used for other purposes. Finally, there is no single correct way to format/model your data - there are many different possible ways, and the best choice is the one that will actually get used (which is pretty much never the normalized version).</p> <p>I think my overall point here is this: I was able to go from not really understanding what a data warehouse was to having a good enough understanding of them to re-architect the team’s warehouse and the ELT pipelines powering it inside of 2 years. But that learning journey was rather rocky, and the first few months were especially tough given the lack of formal training. Reflecting on that experience - both how much I had learned by doing &amp; researching on my own, and imagining how that learning process could be smoother for myself and others - was the driving motivator for me writing the <a href="/blog/analytical-data-warehouses">introduction to analytical data warehouses</a> blog post (now <a href="/blog/category/data/">turning into a series</a>). In fact, that blog post started as a page in the team’s internal documentation that I decided to generalize and expand on, in case other folks might find it useful. I spent a lot of time and brain power on truly grokking the data warehouse from scratch, and that is something that I am both proud of and hope others don’t have to go through alone (though many inevitably will).</p> <p>One final reflection on this: I realized that the reason I did not initially understand the point of dbt was because I didn’t understand data warehouses and how to work with them. dbt solves a specific problem that every team working with a data warehouse experiences, and it solves it very well, which is why it is very popular within the specific community of data practitioners who work with data warehouses. Once I understood how to work with a data warehouse (compared to a database), understanding the value proposition of dbt and how to use it was much easier. Figuring out how to use dbt within our specific data warehouse… that was something my brain started noodling on endlessly towards the end of year one.</p> <h3 id="city-analytics-exchange">City Analytics Exchange</h3> <p>I mentioned that another important part of my learning process was being connected to the larger data community online, and this perspective was one of the primary motivations behind my City Analytics Exchange initiative.</p> <p>As a member of the City of Boston Analytics Team, I didn’t just want to learn from my fellow Boston teammates - I also wanted to learn from data analytics practitioners in other city governments. Fortunately I was not alone, and other teammates also wanted to cultivate a network of city analytics professionals! We started by reaching out to former teammates who had moved on to work for other cities, who could then reach out to their new teammates. We also started with a couple of motivating topics - specific tools that we all used or initiatives we were all trying to work on (e.g. knack, open data portals).</p> <p>I created a free Slack workspace so we had a shared space to communicate, and scheduled time to (remotely) meetup with a loose agenda. We had a goal of meeting roughly once a quarter, and adding 1 new city each time we met. With an existing communication and meeting structure, it was much easier to add new folks each time we talked to other city data teams for some unrelated project - and surprise, suprise, other city analytics folks also wanted to talk to professionals doing the same kind of civic tech work! We got to share our accomplishments, work through problems, and generally collaborate, commiserate, and cross-pollinate in a casual environment of individual contributors doing hands-on-keyboard work.</p> <p>I’m happy to say that the City Analytics Exchange is still going strong, and other members have stepped up to help facilitate the community moving forward. I hope that it continues to grow and be a resource for city data workers. If you are on a city analytics team and want to join, you can reach out to <a href="https://www.linkedin.com/in/amy-hood-110479102/">Amy Hood</a> or <a href="https://www.linkedin.com/in/samfirke/">Sam Firke</a>.</p> <h3 id="the-process-of-process-improvement">The process of process improvement</h3> <p>Early in 2023 the Analytics Team collectively took the “Advanced Innovation Training” with <a href="https://brianelms.com">Brian Elms</a> from <a href="https://changeagentstraining.com">Change Agents Training</a>. We learned about techniques like process mapping &amp; identifying waste, standard work and systems of work, batching vs flow, nudging, and how to calculate the value of innovations/improvements. We teamed up and developed some innovations that we could implement for the Analytics Team (although… most of these never actually happened). Many of the techniques seemed simple or obvious, but I think in large part the specific techniques or toy improvement projects were only the delivery mechanism: the real value was in initiating and enabling a culture shift so individual contributors (rather than managers) could feel empowered to make small (or large) changes to how the team got things done (a.k.a improving processes)… and then have a framework for proposing/communicating these innovations to both teammates and leadership.</p> <p>In the 2nd half of 2023 two dedicated process improvement coaches (<a href="https://www.linkedin.com/in/kayleigh-vocca/">Kayleigh</a> and <a href="https://www.linkedin.com/in/nolan-brown-95b95017a/">Nolan</a>) joined the Analytics Team and started to build out training for Boston city workers, and my understanding of the practice was able to advance even further through conversations with them.</p> <p>The primary reason I wanted to highlight this training and what I learned from it, though, is that you could argue that the dbt migration project I kicked off in year 2 was just one very long process improvement project. I was an individual contributor, not a manager, but I was still able to propose and implement an improvement that would substantially change how the team worked with the data warehouse. I had essentially spent a year constructing an internal process map of how the data warehouse functioned, and I could identify many instances of how implementing dbt would reduce waste in that process. I could use the tools and techniques I learned in the Innovation Training course to communicate the value of spending time implementing dbt to leadership and others who weren’t familiar with the daily work and processes the data engineers and analysts followed.</p> <p>What a perfect segue into…</p> <h2 id="the-second-year-aka-the-year-of-doing">The second year (aka the year of doing)</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/boston_my_desk-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/boston_my_desk-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/boston_my_desk-1400.webp"/> <img src="/assets/img/boston_my_desk.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> My desk/cubicle at City Hall, with my knitted snowman next to our "data engineering library" </div> <p>Starting around December 2022, I started to think more seriously about how the data engineering team could start to use dbt. There were three main obstacles I had to overcome: (1) I had to show that dbt would provide a real value-add and would be worth the investment of time and resources so that it could evolve beyond a personal pet project into something the entire team could work on, (2) I needed to get all of the setup figured out so it would be as easy as possible for the second (and third, etc) engineer to start contributing to the project, and (3) I had to decide whether to make dbt fit the team’s current data warehouse architecture, or whether and how to redesign the data warehouse.</p> <p>I also had a couple of theories in how to overcome these obstacles: (1) the dbt-generated docs site with its clean documentation and pretty lineage graphs would be the biggest selling point of dbt, and (2) the easiest path in getting started would be to work outside of the existing data warehouse in a small separate database using open data with a complex lineage graph. I also hadn’t yet decided on the best path for obstacle #3, and pursuing a toy example that could demonstrate the power of the docs site would buy me some time to think through possible solutions.</p> <p>Thus began a solid few months of what felt like banging my head against a wall in hopes that answers would fall out. I’m not going to lie - it was rough. It didn’t help that the open data set with a complicated lineage I had chosen, CityScore, hadn’t been maintained and was a much bigger beast than I was anticipating. It was essentially a practice run that never fully materialized - I struggled through figuring out how to set up the infrastructure needed to develop and run a dbt core project locally, and hacked away at how to refactor the CityScore pipeline into a set of dbt models. Even though I never reached my target shareable end product for that toy project, I did learn a lot along the way. It gave me time and the opportunity to clarify my thinking and approach in how to migrate our transformations to dbt in the real data warehouse.</p> <p>Along the way I decided on how to solve obstacle #3: the best approach would be to redesign the data warehouse with a new set of schemas that were designed with dbt in mind. With the dbt work isolated to an entirely new set of schemas, that new experimental work could proceed without interrupting the existing data &amp; pipelines. We would essentially be slowly duplicating our existing workflows and data, but in a different set of schemas, with all transformations and tests living in dbt, and the import and export tasks re-organized around the dbt transformations.</p> <div class="row"> <div class="col-sm md-auto"> <a href="/assets/pdf/coalesce2023_from-coast-to-coast_slides.pdf"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cob_new_schemas-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cob_new_schemas-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cob_new_schemas-1400.webp"/> <img src="/assets/img/cob_new_schemas.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Slide 26" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> Slide 26 from the Coalesce talk illustrates the old and new sets of schemas </div> <blockquote class="block-tip"> <h5 id="fyi">FYI</h5> <p>If you just want to find the City of Boston dbt project details/links/resources (such as the above presentation), head on over to the <a href="/projects/dbt-migration-cob">project page</a>.</p> <p>If you want to hear some of the back story on how it all happened, you can keep reading this post.</p> </blockquote> <p>Redesigning the data warehouse was not a decision I could make in isolation, of course - this was a decision that would effect the entire Analytics team, as well as anybody else who was using the data warehouse. It was also a change that had to stand on its own merits - while the new set of schemas were designed with dbt in mind, simply facilitating a migration to dbt would not be a good enough argument for making such a substantial change. Fortunately, others had been feeling the pain of our existing schema organization as well, and the new set of schemas solved many of these issues: (1) isolating the source/staging layer from “production” tables &amp; views, (2) substantially fewer schemas, (3) eliminating the need to create new schemas every time a new dataset was added that didn’t fit into any of the existing schemas.</p> <p>The old set of schemas were organized along the 2 dimensions of access level (open, internal, restricted) and data owner (department, team, source system, etc). The new set of schemas preserved the access level distinction, but instead of also organizing by data owner (the reason there were so many schemas), they were organized by data processing stage (staging, development, production). The addition of a “development environment” in the data warehouse was also a huge selling point for the engineers - previously, everything was production.</p> <p>After getting approval from leadership, I presented this proposal for a new set of schemas to the team in March 2023. I wrote up a long document detailing why the old set of schemas were problematic, why the new set of schemas were organized this way, and how this would change some of our ways of working. I also created a form so that the entire team would have a chance to vote and give input on the new set of schema names - prod vs production vs mart? open_prod vs prod_open? staging vs stg vs source? Overall, the feedback was positive and many people gave their input on the new schema names. And while dbt was mentioned as a next step, the decision to adopt the new schemas was made entirely on its own merits.</p> <p>With the team’s approval and a new set of empty schemas, I could move forward with the next step: actually getting the real dbt project up and running. Fortunately, I had learned a lot during my experiments over the past few months, so I already had a rough plan of approach. The biggest hurdle would be getting the first model to successfully run - not only being able to run a dbt command on my local computer to query and create a table, but also creating a workflow in Civis that would run a dbt command within a docker container and successfully connect to the database and create that table. And the local development environment needed to be sufficiently documented and easy to setup so that other engineers could do the same thing. Getting both the development environment (VS Code + dbt power user extension, venv, environment variables + profile for database connection) and the production environment (docker container, Civis credentials + environment variables for database connection, Civis workflow) correctly set up took some trial and error, but eventually we figured it out and got it running. By May 2023 the dbt project repo, local development environment configurations, and production civis workflow were all set up and there were a few models added to serve as examples. None of this would have been possible without the support of <a href="https://www.linkedin.com/in/al-lee-675536226/">Albert Lee</a>, our AWS/Docker/all-things-infrastructure wizard.</p> <p>The next steps were to start populating the staging schemas with the most frequently used core sources (e.g. the 311 database, the permitting/licensing database, the EGIS database) and migrating over transformations that used those data sources, as well as onboarding the engineers most excited to start working on the dbt project. Going from 1 to 2+ people working on the project is always a big but important step, and was a good opportunity to work through any kinks in the process (is the documentation thorough enough? does it work on your machine?). By June 2023 there were 3 of us working on the project, and we were able to ramp up the migration process. I’m so grateful to <a href="https://www.linkedin.com/in/jenna-flanagan-23740713/">Jenna Flanagan</a> and <a href="https://www.linkedin.com/in/david-falta-2b626b86/">David Falta</a> for their strong and early belief in the dbt project (and their continued commitment to it after my departure!).</p> <p>By July most of the engineering team had been onboarded to the dbt project, and we also had a very good idea of what to focus on migrating in the near term. In addition to the new schemas and dbt migration on the data engineering side, the analysts were starting a migration project of their own - transitioning from Tableau to Power BI. The timing for this double migration worked out perfectly, and we decided that all Power BI dashboards would be built exclusively from the new production schemas (and thus the transformations being handled by dbt). Once the set of dashboards being rebuilt first were determined, the engineering team had a definite list of table dependencies to focus on migrating to dbt and the new schemas. We could also go ahead and document those future dashboards and their dependencies as exposures in the dbt project. And given that the dbt generated documentation site was the initial selling point of dbt (those lineage graphs!), we also focused on getting an internal website (hosted via AWS services) that was updated daily ready for the team to reference.</p> <p>By September we had successfully migrated all of the high priority Power BI dashboard dependencies, had a hosted dbt-generated documentation site customized with City of Boston branding, and the new set of ELT workflows (the new import workflows and the dbt build workflows) were running daily in production. Within 6 months we had a functioning (if minimal) redesigned data warehouse and dbt-centric ELT workflows… just in time.</p> <h3 id="presenting-our-work-in-public-at-coalesce">Presenting our work in public at Coalesce</h3> <p>I just sketched out the work we were able to do in 6 months, but let me go back in time to the start. In March, before actually doing any of the work but after getting the go-ahead from the team, I submitted a talk proposal to Coalesce, the yearly dbt user conference hosted by dbt Labs. When I looked for other organizations using dbt, I saw a lot of companies - startups and large corporations - but nothing from government or other public sector adjacent organizations. But I knew that there had to be some other public sector data professionals using dbt, and after reaching out to a few folks in my network I found out that the State of California’s Office of Data &amp; Innovation team was also using dbt. Not only that, but Ian Rose, a senior data engineer on the team, would be willing to join me in proposing a talk for Coalesce! I was so happy and grateful that I would not have to submit a talk alone (public speaking is scary! … and I’d never given a conference talk before!), and I knew that with more examples we could present a stronger case to other government data teams that they, too, should consider using dbt.</p> <p>We submitted the talk proposal in March, not too long after I typed “dbt init” for the official project. Coalesce was happening in October. So I knew that I had 6 months to not only get the dbt project into a reasonably presentable state, learn as much as I could on both the socio- and -technical side of managing this migration, and also figure out what exactly we wanted to say during our 30 minutes on stage. In June we learned that the talk had been accepted, and in July after announcing the talk we were able to add another cospeaker who had been working on a dbt project in the public sector, Laurie Merrell. You can imagine my relief at achieving an MVP (minimum viable project) by September! But while the technical sprint was slowing down, the social sprint was just starting. Presentations have a way of spawning more presentations (not to mention the accompanying practice presentations). So along with working on our official Coalesce talk with Ian and Laurie, I was also crafting presentations for the Analytics Team, the CIO (head of the department), and the department at large. On the one hand, I wanted to make sure everyone at my workplace would be very familiar with the material I would be presenting in public (a presentation that would also be recorded and posted on the internet), and have the chance to provide any feedback/input. On the other hand, I really needed practice presenting this material! The marathon of presentations (and their practice sessions) leading up to Coalesce definitely helped me be more prepared in San Diego.</p> <p>Attending Coalesce was one of my favorite experiences of 2023. The community of data practitioners that gathers at Coalesce is truly amazing - the online friends I had not yet met in person (including my cospeakers!) and the new friends I was able to meet through the talks and social events, the impressive speakers sharing their work, the professional support networks (Data Angels!), even the dbt Labs folks who had been providing me assistance via slack… the people are the reason why I want to go back and attend Coalesce next year (and the year after that!). I’m not going to into too much detail about my experience at Coalesce, because (1) this post is already too long and (2) I was posting my <a href="https://www.linkedin.com/posts/activity-7120931336135028736-EAZJ?utm_source=share&amp;utm_medium=member_desktop">observations</a> and <a href="https://www.linkedin.com/posts/activity-7121255992767459328-l-36?utm_source=share&amp;utm_medium=member_desktop">reflections</a> the week of on LinkedIn, but suffice to say that I was very grateful that <a href="https://youtu.be/6aX7tAfMmIM?si=gXyUBTzKA4tMaCE7">our presentation</a> was on the first day so I could devote the rest of my time to just enjoying the conference. Oh, and I also got a cool certification after passing a test within an hour of arriving at the hotel ;)</p> <div class="row"> <div class="col-sm md-auto d-flex justify-content-center"> <a href="https://credentials.getdbt.com/7c67f8f7-834f-4806-9d34-048323d1804c"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dbt-certified-developer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dbt-certified-developer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dbt-certified-developer-1400.webp"/> <img src="/assets/img/dbt-certified-developer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Slide 26" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </a> </div> </div> <div class="caption"> Officially certified by dbt Labs to develop dbt projects! </div> <p>Fun fact: I had an early flight and had not yet eaten before arriving at the hotel in San Diego, so after finding the exam room I grabbed an acai bowl to eat during the test (the closest place to buy food was a frozen yogurt place and acai bowls are delicious) that may be owed partial credit for me passing and becoming a certified dbt developer. The test itself is mostly a blur but I have vivid memories of that acai bowl.</p> <h3 id="productionizing-the-data-catalog">Product(ion)izing the Data Catalog</h3> <p>Delivering the <a href="/assets/pdf/dbt-lunch-learn.pdf">presentations about dbt to others in the department</a> ended up being extremely useful for the next phase of the dbt migration as well: publishing the dbt-generated documentation site as an officially supported product, with a boston.gov URL and Single Sign-On access. While the engineering team alone was sufficient to migrate the transformations and tests over to the dbt project, many more teams and leaders in the department would need to contribute their time and effort to help turn the documentation site into an officially supported product. This effort was kicked off in November, with the majority of the work allocated for the following quarter (January - March).</p> <p><a href="https://www.linkedin.com/in/ajjaramillo/">Alejandro Jimenez Jaramillo</a>, the department’s inaugural Director of Tech Governance and Policy, immediately saw the utility of an official data catalog in advancing data governance efforts. Aleja and I had worked together on data governance issues when they were a fellow in 2022, and after they rejoined the department in 2023 we were able to continue that data governance work through the data catalog. Without Aleja and others in the department, the data catalog project may have never gotten off the ground. But because of their efforts, and collaboration across many teams in the department, the first phase of the official data catalog was launched and available by my final day. Logging in to datacatalog.boston.gov before losing access to my Boston accounts was the best goodbye gift I could ask for.</p> <h3 id="iteration-is-the-root-of-progress">Iteration is the root of progress</h3> <p>The first six months of the dbt project (March - September) were devoted to getting a minimum viable project up and running, with the first set of tables needed for high priority Power BI dashboards reliably updating in the new set of schemas. The next six months (October - March) were primarily about getting the rest of the workflows (imports, transformations, tests, exports) migrated, but it was also about iteratively improving our implementation of dbt. With all engineers now comfortable with the essential dbt skills, some were able to dive deeper into specific features in order to improve our usage of them and the overall dbt project.</p> <p>When I presented about the City of Boston’s dbt project at Coalesce, I also published a skeleton version of our dbt project repository in a <a href="https://github.com/CityOfBoston/cob_analytics_dbt_skeleton_project">public repo</a>. Before my last day, I updated this repo to be in line with our implementation as of March 2024. So, you can easily <a href="https://github.com/CityOfBoston/cob_analytics_dbt_skeleton_project/compare/6efa3219e3a509692481c95687014dc373e4fe34...a1e5c015ecf22819772ace4bd899780e5c0f621d">compare</a> what changes were made over those 6 months. While this does not represent all of our improvements (so, anything that is implemented only in individual model sql/yaml files is not captured), it does capture most of them. I may write another post in the future outlining all of the key features we added that I think should be part of any dbt project (let me know if there’s any interest in that!).</p> <p>I’m so proud of the progress our team made, and by the time I left in March the migration was about 75% complete and I was fully confident that the team could continue the dbt project work and migration without me - as well as carrying forward the data catalog and data governance work. There were a few key things that I think contributed to the project’s success as a team-wide initiative, rather than just being my personal project (which would not have had a lasting impact!):</p> <ol> <li>We set aside time for a weekly dbt sync, where all of the engineers could meet and discuss the status of the migration, propose features and improvements, ask questions to improve their own understanding, and share their own learnings and discoveries. The meetings were half project management and half knowledge sharing, and I was very intentional in making sure they were a collaborative, safe, and open environment. We were all learning and developing this project together. We also traded off facilitation roles so that there was not just one person in charge of the meeting.</li> <li>We made the right things the easiest things. For example, we wanted to make sure that it would be very easy to add documentation to models at a later point - possibly by those less familiar with dbt. So, every model SQL file also had a YAML file with description fields for every column. Through a combination of the codegen dbt package, some custom macros, and bash scripts, it was as easy as specifying the access level and a model name as arguments and running a bash script to create the base YAML and SQL files. It was fast, easy, and accurate.</li> <li>I put a lot of thought into the initial design, and made sure that the database design and dbt project design worked together cohesively. And once that larger design was decided, we stuck to it. This consistency in the overall design and architecture of the project meant we could focus on the smaller implementation details, and be confident that the ground would not move beneath us as we were trying to build on it.</li> <li>We collaboratively developed a “style guide” and standards for our work in the dbt project. Much of this borrowed from the official dbt Labs recommendations (e.g. keeping the source and ref macros at the top in a “model imports” section, having 1 SQL and 1 YAML file per model), and some of it was customized for our project (e.g. using the generate_geom_column macro for all geometry transforms, and making sure every source had the custom table-level test test_table_not_empty). We held each other to these standards in PRs, and could raise any questions in the weekly syncs.</li> <li>We didn’t try to do everything at once, and perfectly for the first time. Instead, we focused on the next thing that needed to happen given the team’s goals and current priorities. That means that there are some things we had not implemented by March that many data teams might consider essential (like CI/CD checks &amp; pipelines), and others things we implemented relatively early that many teams may not consider as important (like custom styling for the dbt docs site). When we had a relatively small number of models, we didn’t focus on optimizing when those models ran. But when we had many sources and were moving more to be updated at night (and the database was overloaded during the morning run) it became important to implement the freshness selector for the nighttime and morning dbt builds. When a migration happens on top of your regular workload, you quickly learn to prioritize only what is absolutely needed next - and having a vision for the future state can help a lot with that more immediate prioritization.</li> </ol> <p>None of this is to say that our migration went perfectly smoothly - we ran into plenty of hiccups and challenges along the way. For over a week our postgres database ran out of memory and crashed every morning at about 9am (right when it was needed for dashboard refreshes). Analysts suddenly lost the permission to query tables they needed. Some models that should be part of the daily run were not included, so some tables had stale data and we didn’t catch it until a stakeholder raised the issue. We were all learning how to build the airplane as we were building the airplane as we were flying it - and because we had a talented and dedicated team of engineers we did eventually diagnose and solve these issues… and we all learned so much in the process!</p> <p>I’m so excited to see what the team does next with the dbt project. My goal/hope is that the analysts join the engineers in contributing to the dbt project repo. Maybe data owners will start contributing documentation to the data catalog. Analysts may start using dbt metrics (maybe even the semantic layer!) in pursuit of a standardized city-wide metric library. Perhaps in a few years, other data teams throughout the City will develop their own dbt projects, and a multi-project dbt mesh could be established. With the dbt project established, other tools from the modern data stack (those with open source options, of course) could be added to the data engineering team’s toolbox.</p> <p>In order for analysts and other data professionals to contribute to the dbt project, however, there are 2 key skills that they will need: SQL and git. And that leads into the final accomplishment I would like to talk about: bringing my Software Carpentries instructor experience to the City.</p> <h3 id="teaching-computational-skills-workshops-for-city-workers">Teaching computational skills workshops for City workers</h3> <p>In grad school, I got involved with the <a href="blog/about-the-carpentries">Software Carpentries</a> and became a certified instructor. I learned how to <a href="/blog/carpentries-workshops">teach core computational skills</a> (bash, git, SQL, python) to beginner learners in the context of also teaching reproducible research skills. I really liked the Carpentries teaching philsophy of live-coding, conscientious language, and getting learners to the point of using the technical skills as fast as possible (workshops average about 3-4 hours). Also, all of the lesson plans and content are open source, so technically anybody can teach a workshop using Carpentries material.</p> <p>In 2023 I got the chance to teach a couple different Carpentries workshops. When the analysts on the team started expressing interest in having their own github repo for the growing number of projects using python rather than a dashboarding BI tool, I offered to teach the <a href="https://carpentries-incubator.github.io/git-novice-branch-pr/">Carpentries workshop on git</a> with some custom content tailored for the team. This ended up being folded into a larger data culture pilot program during the summer and was opened up to other City workers as well. During the latter half of that data culture pilot, I was also able to teach some material on pandas from the <a href="https://swcarpentry.github.io/python-novice-gapminder/">Carpentries python workshop</a>.</p> <p>In December, I also decided to host a SQL workshop and opened it up to any city worker who might be interested. I knew that SQL skills were going to become more important for analysts across the city who may be expected to query the data warehouse in order to rebuild a dashboard in Power BI, and I also just really enjoy teaching the <a href="https://swcarpentry.github.io/sql-novice-survey/">Carpentries SQL workshop</a>.</p> <div class="row"> <div class="col-sm md-auto d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sql_workshop_feedback-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sql_workshop_feedback-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sql_workshop_feedback-1400.webp"/> <img src="/assets/img/sql_workshop_feedback.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Slack message" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A positive review of my SQL workshop! </div> <p>One amazing outcome of these workshops? Another team at the city who used the Analytics team data warehouse and had specific subject matter expertise used the opportunity to upskill their analysts and started using a GitHub repo to store and collaboratively develop SQL queries to create tables/views that could build on each other. This is a significant and useful step forward from saving long, convoluted SQL in the BI tool, as well as making it much easier on the data engineering team to later incorporate these queries into the dbt project.</p> <h2 id="wrapping-up">Wrapping up</h2> <p>This post is already longer than it needs to be, so I’m not going to spend too much time wrapping up. I will just say this: during my 2+ years at the City of Boston I was able to learn a lot by doing, I accomplished a few key things that I hope will have a lasting impact, and I met some really amazing people whom I hope to stay connected with for the long haul. There are also a lot of projects I worked on that I did not talk about in this blog post - this was primarily about the internal data engineering work I did. However, the vast majority of my time was spent on projects completed for stakeholders, who came to the Analytics team for data project help. It is not really my place to talk in detail about those projects, but I will say that I had a fantastic time contributing my expertise to their work - especially those longer, more complex projects that had a direct impact on constituent lives. I look forward to seeing what all of my former colleagues accomplish in the years to come!</p> <p>~</p> <p>Did you scroll directly to the bottom just get to the conclusion, and skip right over the TL;DR? Don’t worry, here it is again:</p> <h3 id="tldr-1">tl;dr</h3> <p>Maybe you don’t feel like going on a journey with me at the moment, and you just want the highlights - this section is just for you.</p> <p>What I learned:</p> <ol> <li>How to work with data warehouses (ELT, workflow orchestration, flow of transformations, testing the data, etc)</li> <li>Patterns of collaboration &amp; communication with fellow engineers (e.g. PR flow &amp; etiquette, documenting work), analysts (e.g. division of responsibilities, data modeling), and stakeholders (e.g. requirements gathering, advocating for data engineering best practices)</li> <li>The formal process of process improvement - how to develop and communicate innovative projects that will improve daily processes</li> <li>How to start a dbt project from scratch and iteratively grow its architecture &amp; adoption</li> <li>How to present a fairly technical data engineering process improvement project to teammates, leadership, and a wider audience</li> </ol> <p>What I accomplished:</p> <ol> <li>Starting &amp; growing the City Analytics Exchange, a casual network &amp; meetup group of data analytics practitioners in local government</li> <li>Teaching workshops (from Software Carpentries) on core technical skills like git, SQL, and python to fellow City employees (part of a larger data culture initiative)</li> <li>Integrating dbt into the data engineering team’s tech stack, and re-organizing the data warehouse along the way</li> <li>Presenting about the dbt migration project at Coalesce 2023</li> <li>Collaborating across many teams &amp; individuals in the department to deliver the team’s first data catalog as an official product supported by the department (SSO, boston.gov URL, data governance work initiated to support continued development)</li> </ol> <p>Links to look at next:</p> <ol> <li>Blog post: <a href="/blog/analytical-data-warehouses">Analytical Data Warehouses - an introduction</a></li> <li>Project page: <a href="/projects/dbt-migration-cob">dbt Migration at City of Boston</a></li> </ol>]]></content><author><name></name></author><category term="life-updates"/><summary type="html"><![CDATA[what I've learned and accomplished during my 2+ years as a data engineer on the Analytics team]]></summary></entry><entry><title type="html">The Transformation Flow, Part 1 - building intuition</title><link href="https://jennajordan.me/blog/transformation-flow-building-intuition" rel="alternate" type="text/html" title="The Transformation Flow, Part 1 - building intuition"/><published>2024-01-21T00:00:00+00:00</published><updated>2024-01-21T00:00:00+00:00</updated><id>https://jennajordan.me/blog/transformation-flow-building-intuition</id><content type="html" xml:base="https://jennajordan.me/blog/transformation-flow-building-intuition"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_transformation_flow_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_transformation_flow_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_transformation_flow_1-1400.webp"/> <img src="/assets/img/header_transformation_flow_1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Many months ago I posted an <a href="/blog/analytical-data-warehouses">introductory primer to analytical data warehouses</a>, specifically targeted to folks who were familiar with data work but did not have a lot of experience working with data warehouses. In that post I made reference to a follow-up blog post that would focus more on the transformation flow within a data warehouse:</p> <blockquote> <p>There may be many, many downstream transformations for a particular data source, and the structure of these can vary widely. There are many different design philosophies around this area of the data warehouse, but I will leave that for another post. Suffice to say, a large portion of the “thinking” work in a data warehouse is in writing these downstream transforms.</p> </blockquote> <p>Well, this is part 1 of that promised follow-up post (finally!). There is so much to say about the transformation flow that I’ve had to break it up into 3 posts: Part 1 will attempt to demonstrate that you are likely already familiar with the transformation flow in other tools, and relate your experience back to how it would work within data warehouses. Part 2 will go over the major data modeling design paradigms, which provide a common structure for these transformations. And part 3 will focus on how these transformations and data models can be put together and implemented in the data warehouse (this is the one that will focus on dbt).</p> <p>These posts will continue to be in the same spirit with the same target audience as the initial one: this is written for those of you who work with data but have not yet had a chance (or perhaps have recently begun) to work on a team that uses a data warehouse. I’m going to assume that you’ve started to develop intuitions about how to clean, wrangle, organize, and analyze data, and you want some help jumpstarting your ability to work professionally on a data team that has a data warehouse. And while the previous post introduced some broad principles for what data warehouses are, why you would want to use them, and how they tend to be used, this post will start to dive deeper into the transformation and modeling flow of data within the warehouse - the bread and butter of the “analytics engineer”.</p> <h2 id="a-philosophical-aside">A philosophical aside</h2> <p>Throughout this post, I want you to pay attention to how I talk about the transformation flow. I will use words like “intuition”, “design”, “philosophies”, “choices”, etc. The underlying point is this: data modeling - the thing you are doing when you implement specific transformations - is a creative act. There are no “right” or “wrong” choices (though there are certainly “better” and “worse” choices for your specific context and use case), and with time and practice you can grow your data modeling craft as a creative knowledge worker.</p> <p>Please note that when I say “creative”, I mean it in the sense of thinking about a concept or solving a problem in a new/different way, not in the imaginative or fantastical sense. Think of data modeling in the same way that you would think of mapmaking: you are creating a simplified and useful representation of the landscape that captures enough detail that it can serve a variety of use cases, while not including so much detail that the map becomes a mess of illegible scribbles. “The map is not the territory” - in other words, the model should not be confused with the reality it is representing. Models are imperfect, limited abstractions that must be interpreted. A model that perfectly and completely describes its reality has no utility at all (besides being impossible) - we simplify and choose what to emphasize because our minds have limits. To go back to maps: would you rather navigate while driving with your map application in “map” view or “satellite” view?</p> <p>So remember: the work of translating perceived reality into a limited respresentation that can be described in a machine-readable format, and then further transforming, integrating, and analyzing that data into a human-digestable format that can then be used to influence how others perceive reality (and thus their actions, which may effect reality)… is deeply philosophical, creative, and intellectually rewarding work (and hopefully impactful!).</p> <p>Furthermore, it is important to remember that all of your more abstract data work should be grounded in the reality that is your source truth and the impact that you want your data work to have. So it helps to have both knowledge and interest in that reality you are trying to model and influence. If you are still learning and looking for practice projects, or at the start of your career and looking for jobs, don’t make the mistake of thinking all data work is the same. Focus on the subjects that interest you the most and you have specialized knowledge in - and the parts of the world where you most want to make an impact. For me, that domain is (generally speaking) government services and international development. My favorite data sources (and the focus of my upcoming personal project) are about peace/conflict (a.k.a wars, and preventing/resolving them) and comparing political systems. What domains and data sources are you most interested in?</p> <p>All of that to say - it is impossible for any one person, blog post, book, or methodology to tell you how to transform and organize your data. What I hope to accomplish in this post is to build upon the intuitions you likely already have, and provide some structure and guiderails for how to go further. In the following posts, I will point to some resources you can use to dive even deeper into the many rabbit holes that exist in the realm of data modeling. But in the end, it is up to you to become the expert on your data and make creative data modeling choices to craft your transformations - and hopefully inform and influence the decisions and actions people take in a positive, productive way.</p> <h2 id="whats-in-a-table-and-whats-in-a-cte">What’s in a table? (and what’s in a CTE?)</h2> <p>If you read <a href="/blog/analytical-data-warehouses">the previous post</a>, then you know that data flows through many different tables and views in the data warehouse, from source to final data/analytical product. However, beyond a few basic rules like making sure your first transform is simple and has a 1:1 relationship with its source table, I did not go into just how these transforms get divided up into tables and organized in the warehouse. Figuring out when a particular transform should be materialized as a table/view, or just broken out into a CTE in your larger query, is often a matter of intuition (intuition founded on learned and internalized guidelines). This can be frustrating for beginners, but in this section I want to show that you probably already have developed some of that intuition - you just developed it in a different context.</p> <p>Before trying to bridge from intuition you developed either by writing code in notebooks or applying formulas in spreadsheets, let’s define a few key terms and note some caveats.</p> <p><strong>Transformations</strong>. I’ve used this term quite a few times already, and if you’re not sure what that means you’re probably already confused. A transformation simply refers to any type of change you are applying to dataset. Changing the <em>shape</em> of data by joining two tables or aggregating a table is a substantial transformation, while applying functions to columns like trimming whitespace or changing a text field into a datetime are simpler transformations. Adding data by appending rows or updating values is <em>not</em> a transformation. Essentially, any SQL select query that is persisted in the database (the result creates a new table/view) is a transformation.</p> <p><strong>Tables, views, and CTEs</strong>. A table in a database contains physical data structured by rows &amp; columns. A view is a saved query, and doesn’t actually contain data. A “Common Table Expression” (CTE) is similar to a subquery in that it is a self-contained SQL select query inside of a larger SQL query, but while subqueries can exist within a clause of the main query (e.g. the <code class="language-plaintext highlighter-rouge">select</code> or <code class="language-plaintext highlighter-rouge">from</code> clause), CTEs are pulled out, ordered, and the first CTE is preceded by a <code class="language-plaintext highlighter-rouge">with</code> keyword. CTEs are more readable, maintainable, and DRY (a CTE can be referenced multiple times in later queries, while a subquery must be repeated). If CTEs are a new concept for you, <a href="https://learnsql.com/blog/what-is-cte/">here</a> is a good introductory article.</p> <p><strong>You should already know SQL</strong>. If you are unfamiliar with SQL, then I’d suggest you revisit this post (and <a href="/blog/analytical-data-warehouses">the previous one</a>) after you have learned the basics of SQL and databases. If there is interest, I can write a separate blog post focused on how to learn SQL and the many free online resources available (as well as excellent books to borrow/buy). For now, if you’re still reading, I’m going to assume you are comfortable with writing SQL select queries.</p> <p><strong>Bring your own examples</strong>. This post does not really have any specific examples to illustrate the transformation flow. Instead, I’m going to be relying on you, the reader, to think back to past projects and fit the data &amp; code you are intimately familiar with to approaches I’m describing. The goal of this post is not to teach you how to do a specific technical thing, but rather to help develop a general mindset and approach to transforming data within a data warehouse. Plus, I’d much rather demonstrate the entire process by blogging my way through a personal project than coming up with a few trite examples - so there may not be any short examples right now, but there will be one very long example in the future. (Please don’t hate - I’m mostly writing these posts for the fun of it!)</p> <h2 id="building-intuition">Building intuition</h2> <p>Burgeoning data practitioners typically come to data warehouses having already developed some expertise in manipulating data in one of two paradigms: dataframes or spreadsheets. Pick which of the following sections to read depending on which best describes you.</p> <h3 id="for-the-pandas-or-polarsdplyretc-practitioner">For the pandas (or polars/dplyr/etc) practitioner</h3> <p>I first want to revisit a paragraph from the <a href="/blog/analytical-data-warehouses">previous blog post</a>:</p> <blockquote> <p>If you have worked on enough code-based analytics projects, you have probably learned the value of maintaining a proper data transformation flow. This may show up in how you organize your data folder: you have your “raw” data that is never to be altered, your cleaned/processed data that is an intermediate output, and your final data. You learn the value of using code to transform data - it means all transformations are documented, replicable, and easy to alter. Your notebooks/scripts are a mix of exploration, small cleaning tasks, and big transformations. You may have some functions in there for transformations that need to be performed repeatedly or to encapsulate more complicated code. You are displaying chunks of dataframes in order to visually examine the results and decide what needs to be done next. You are also saving dataframes to CSVs to be used elsewhere. All of these elements show up in the analytical data warehouse as well - they just look a bit different.</p> </blockquote> <p>Let’s dive deeper into this comparison: the analytical data flow of pandas code in jupyter notebooks and data in files vs the analytical data flow of SQL code in a dbt project and data in database tables.</p> <p>I’m going to assume that if you chose this section you are already familiar with manipulating data using pandas (or polars, or dplyr) in jupyter (or R markdown) notebooks in a git-tracked repo, and it is how this process works in data warehouses that you are not sure about. If that is not the case, then this section is probably not useful for you, as there is nothing to create a conceptual bridge from. Feel free to skip ahead to the next section or come back later.</p> <p>It may be useful for you to pause reading and go back to look at your last project. Scan through your notebooks with a critical eye. Don’t focus on the individual lines of code, but rather the overall structure. How did you organize your data files? How did you organize your notebooks? What code did you chunk together into one cell, and what code did you pull out so you could run it separately? When did you display your data, and what were you looking for? What types of transformations get saved to the same dataframe variable name (overwriting the dataframe), and what types of transformations get saved to a new variable name? Why did you choose to use a new variable name, and what naming schema organically developed for your variable names? When did you save your data to a file, and why? What code has been organized into functions, and why did you decide to do that? At what point in the development process did you decide to make that change? What notes/documentation did you include, in code comments or markdown cells? How different is your final completed notebook from how it looked during the development process? Did you pull out any exploratory notebook code and formalize it in a python script? How does the code in the python file compare to the code (that presumably accomplishes the same or a similar goal) in the notebook?</p> <p>Seriously, take a few minutes to pause reading, pull up GitHub or your local repo, examine your project, and try to answer these questions before continuing. It will help, and this blog post isn’t going to disappear.</p> <p>Caveats and code review concluded, let’s proceed.</p> <p>Your notebook likely follows a pattern like this: first, you import the data files you need and save it to a dataframe. Then, you apply some set of transformations, saving it to some new variable name. Then, you might apply a different set of transformations, and save those to a new variable name. Finally, you will save your transformed data to a file - perhaps at multiple points in the process.</p> <p>It’s useful to save sets of transformations to different dataframes because each time you tweak your code you have to re-run all of the transformations relevant for that dataframe variable name (or risk being unable to re-run your notebook and get the same result). You don’t want a new variable name for every single transformation, because that gets unwieldy, and you don’t want to use the same variable name throughout the notebook, because some blocks of code take longer to run and it becomes tiresome to re-run the whole notebook every time you make a tweak. So, you naturally develop some conventions for when to save a dataframe to a new variable name. Those groups of transformations that get applied to the same variable name - those are the same groups of transformations that correspond to CTEs. CTEs serve two purposes: (1) they allow you to order and chain many transformations, allowing the final overall query to accomplish more than one simple query alone; and (2) they make complex SQL code more readable, by logically grouping together transformations and assigning the result a meaningful name. So, think of your CTE aliases as being analagous to your dataframe variable names, and CTEs as accomplishing the same conceptual chunking that each successive dataframe iteration does.</p> <p>Why save a dataframe to a CSV file? You may have reached a point where you want to share the resulting transformed data with someone else. Or, you may want to use that data in another notebook. Or, you may be ready to do something else with that data - make a visualization, run a statistical analysis, etc. Regardless, you have identified that the data has been sufficiently transformed that it needs to be preserved in that state in a more permanent and reusable way than just an ephemeral dataframe that will disappear when the notebook is shut down. It is at this same point that you should conclude a transformation being performed by a SQL query, and save the resulting data to the database as a table/view. This will allow others to query the transformed data - for their own subsequent queries, for visualization, for exploratory analysis, for productionized ML pipelines, etc. So, think of saving a dataframe to a CSV as being analagous to saving a query’s output to a table/view.</p> <p>In short, data goes through many transformations. These transformations tend to be chunked together, due to necessity or conceptual similarity. At some point the transformation is substantial enough that the resulting data has its own value, and that state of the transformed data should be preserved for multiple future uses. This is true both in the dataframe flow of pandas code in jupyter notebooks, and the database flow of SQL queries making tables and views.</p> <h3 id="for-the-excel-or-sheets-enthusiast">For the Excel (or Sheets) Enthusiast</h3> <p>Spreadsheets remain the most common way most people interact with data. They are intuitive, easy to use, and extremely flexible and powerful. For the vast majority of use cases, spreadsheets are sufficient. But if you are reading this, then you have probably already bumped up against some constraints and know that databases can solve your spreadsheet woes.</p> <p>Have you ever accidently deleted the content of a cell without realizing it? Deleted some cells, only to realize that now the rest of your data has been shifted out of order as well? Realized that your dataset has exactly 1,048,576 rows… and some data is missing? Maybe you have manually, laboriously, transformed your raw data into something analyzable… only to learn that new data is available that needs to go through the exact same process? Or you get handed a spreadsheet to analyze, and the organization of the sheets make you want to question why you decided to work with data in the first place? Do you have a folder full of the exact same data, with some kind of versioning note in the file name (_v2, _FINAL, _FINALFINAL, _2022, _2020-2021, _thisyear, _forCDO, _forJane, etc)? At some point in your data journey, you’re going to look at all of your spreadsheets and think… there has to be a better way.</p> <p>Congratulations - there is! But if you have worked only with spreadsheets for your whole career, even the word “database” may be intimidating (or, you may be thinking, I already made a database with my spreadsheets!). Big, complex, analytical data warehouses may not initially seem like a viable solution for you, but I promise that if you have already started learning SQL then you are ready to start making that transition. Let’s talk about how the work you are currently doing translates over to the process you will follow when transforming data in the data warehouse, and the advantages this shift will offer.</p> <p>First, rather than getting emailed a spreadsheet to transform and analyze, that data should already be present in the database as a source table - you just need to know what the table name is. (If the data is not already in the database, that’s probably someone else’s job to get it in there and make sure it gets updated on a regular schedule). Next, you will need to write a SQL query in order to transform it. Think about the transformation process you would follow in your spreadsheet: you may do some data cleaning by making sure that a categorical column has only the set of values that should exist (correcting misspellings, removing whitespace, etc). You may turn a text column into a date column (if Excel hasn’t already tried to do it for you - good luck). You may do some math on your numerical values by creating a new column and using a formula. You may then need to do further math by subtracting that new column from another column. You may need to create a pivot table after these cleaned and transformed columns are created. You may need to create a new sheet with only a subset of the original data, or a new sheet that combines data together from two other sheets. Some of these transformations are small/intermediate - they are needed in order to get the data to its final form, and are typically done by modifying data inplace or creating a new column. Some transformations are more substantial - the result gets its own sheet. Usually transformations have to be done in a specific order. At various points you may save and share the resulting data by emailing the modified spreadsheet to a colleague, or start creating inline visualizations based on this transformed data.</p> <p>Think about at what point during the transformation process the data reaches a “final” state (which may happen multiple times in the overall process). You want to share out or visualize this transformed data. In a SQL transformation flow, the group of transformations you need to get to that point will compose your overall query, and result in a table or view being created in the database. Rather than emailing the spreadsheet, you can just let your colleague know the new table name, and they can then query that table. You can query that table with a BI tool in order to visualize it. And remember - all of those transformations were done with a SQL query, and the original source table remains unchanged. This means that if the source table gets updated with new data, you can simply re-run the SQL query, all of your transformations will get applied, and your transformed table will have the new data. If you made a mistake or need to modify your query, you can simply update the SQL and re-run the query. This is only possible because the source table remains unchanged.</p> <p>Now consider all of the transformations that would make up that overall query, and group them into stages. First, this group of transformations must happen. Then, this next group of transformations must happen - but they can only be done after the first group of transformations is done. Maybe you want to break those stages into smaller groups because certain transformations seem to form natural groups, and it makes more narrative sense for them to be separate. These groups of transformations, whether due to the necessity of the order they must be performed or the nature of the transformations, would be equivalent to the CTEs that make up your overall query. CTEs are your query building blocks - each one is complete and sufficient on its own, but stack them together and you can build a more complex overall structure. Chunking transformations together conceptually also helps your overall query to be more readable - after all, you don’t just want the transformation to happen, you also want other people to understand what the transformation is doing.</p> <p>In short, data goes through many transformations. These transformations tend to be chunked together, due to necessity or conceptual similarity. At some point the transformation is substantial enough that the resulting data has its own value, and that state of the transformed data should be preserved for multiple future uses. This is true both in the spreadsheet flow of functions and pivot tables, and the database flow of SQL queries making tables/views.</p> <h3 id="did-the-conceptual-bridges-hold">Did the conceptual bridges hold?</h3> <p>I hope that these comparisons made sense and you can more easily see how the data transformation work you’ve already been performing translates over to the SQL transformations you will craft to build out your data warehouse. However, so far these bridges are only built out of theoretical foundations - you’ll have to finish the rest of the job by practicing using SQL to transform raw input data into actionable output datasets, and all of the intermediate tables along the way. Most of this will only make sense after you’ve had experience building data models in your own data warehouse. Only with practice can you turn abstract theoretical knowledge into internalized tacit knowledge.</p> <p>If that still sounds intimidating, and you’re not sure how to get started with your own personal data warehouse project, you are in luck! I have my own personal project that I’ve been meaning to work on for quite a while now, and I’ve decided to write a series of blog posts along the way documenting my process (and motivating my progress). Stay tuned for that series in the coming months.</p> <h2 id="purpose-driven-design">Purpose-driven design</h2> <p>I want to note that so far, we have been focused on scenarios where we know where we are and where we want to go. We know what the raw source data looks like, and we know what the transformed data needs to look like in order to accomplish a specific purpose (such as creating a visualization, calculating a metric, or sharing summarized data). This is the most fundamental and impactful type of data design. Shifting transformations that you may have performed in a manual, isolated, undocumented way into an data warehouse paradigm where the transformation process can be automated, transparent, and documented is inherently valuable and a necessary first step. You know your data, you are analyzing your data, and you know how to get your data from point A to point B (and point C, D, etc). You know exactly what this data will be used for, what the purposes of the transformations are, and you can see your project through the entire data lifecycle.</p> <p>However, one drawback of this purpose-driven design paradigm is that it does not scale very well. On large data teams, there may be different people assigned to each task: ingesting, transforming, documenting, visualizing, and analyzing the data. Those tasked with transforming the data may not know all of the many purposes the data may need to be designed for. Furthermore, the data warehouse will need to house many different datasets, for many different purposes, transformed by different people at different times for different projects. If every dataset is only transformed for the initial known purpose, over time the design of the data warehouse will become rather chaotic (this is how you get a spaghetti DAG). Plus, it’s very likely that most of the data will no longer be in use, data transformations will have been duplicated as slightly different use cases arise, and users may not even realize that the data they need is already in the warehouse (though perhaps not in the form they need). This is not necessarily a bad problem to have, as it means that your data warehouse has survived for a few years and has been actively used by many different people. However, at some point it really does become a problem, and someone is going to look at the data spaghetti and think, “there must be a better way.”</p> <p>And of course, there is. In fact, there are quite a few you can choose from or combine in whatever way suits the larger purpose of your data warehouse. In part 2 (coming soon!) I will cover a few of the most important data modeling design paradigms in broad strokes and link to resources where you can learn more about each one.</p>]]></content><author><name></name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[preparing to dive deeper into the flow of transformations in an analytical data warehouse]]></summary></entry><entry><title type="html">Analytical Data Warehouses - an introduction</title><link href="https://jennajordan.me/blog/analytical-data-warehouses" rel="alternate" type="text/html" title="Analytical Data Warehouses - an introduction"/><published>2023-08-13T00:00:00+00:00</published><updated>2023-08-13T00:00:00+00:00</updated><id>https://jennajordan.me/blog/analytical-data-warehouses</id><content type="html" xml:base="https://jennajordan.me/blog/analytical-data-warehouses"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_data_warehouses-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_data_warehouses-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_data_warehouses-1400.webp"/> <img src="/assets/img/header_data_warehouses.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When I first started working on the City of Boston Analytics Team, I had never really worked with an analytical data warehouse before. I had experience with databases, and with the data flow in analytics projects (mostly using pandas, in jupyter notebooks), and I had used databases as a starting point for analytics projects, but I had not yet put all of those pieces together. Then I started working as a data engineer whose primary role was to operate within an analytical data warehouse, and I took a 4-week crash course in dbt… and my brain went into overdrive trying to sythesize everything I already knew and was learning in order to come up with a mental model of what an analytical data warehouse was, how it should operate, and why. Now, almost two years later, I’m pretty sure I get it - and I want to share how I conceptualize a data warehouse in hopes that it makes the journey easier for anyone else who may be in the same position I was.</p> <p>But first, a couple caveats:</p> <ol> <li>I’m still learning, and I know my understanding will continue to evolve as I continue to learn</li> <li>This is largely influenced by how my team uses our data warehouse, and it may be different for other teams</li> </ol> <h2 id="databases-vs-data-warehouses-transactional-vs-analytical-the-what-and-why">Databases vs data warehouses, transactional vs analytical (the what, and why)</h2> <p>An analytical data warehouse is the data source that powers analytical outputs - dashboards, reports, one-off statistical analyses, machine learning models, etc. Having a data warehouse makes sense when you are working in a team - you often need to share data, pick up other people’s projects, and know that an analytical output will continue to function even after a project is wrapped up. Having a reliable, managed, and shared infrastructure is a huge productivity boost. Also, pretty much every BI tool is built to at least connect to a database.</p> <p>But what makes a data warehouse different from a (relational) database? In large part, this is simply a matter of semantics. Any database can be a data warehouse, it all comes down to how you use it. If you are using a database for analytics, you are likely doing a lot more and more complicated queries to read from the database. If you are using a database to power an application, you are doing a lot more (and need faster) writes to the database. Transactional databases are optimized for writes, and analytical databases are optimized for reads. Some DBMSs are balanced and can be used for both (i.e. PostgreSQL). Others optimize heavily for one use case (Snowflake is pretty much only used for analytics). How you are using a database also dictates how you organize it. If you are using a transactional database to power an application, there is a very good chance your database is normalized (most typically into 3rd or Boyce-Codd Normal Form).</p> <p>Normalizing your data is the best way to organize your data when you want to reduce the duplication of data, avoid data anomolies for inserts/deletes/updates, and ensure relational integrity. But the normalized structure can make it difficult to answer seemingly simple questions - you often have to write complex queries with many joins. And analytics is all about asking questions of your data.</p> <p>If you are using your database to answer many different analytical questions, you probably have many different data sources - and many of those data sources may be completely different from each other and never intended to be joined together. This is when a database starts to become a data warehouse - rather than one cohesive place to store data for one purpose (and therefore the data is all related), the data warehouse is more like a data repository. It is stored in a single place because of the convenience, not because the data is necessarily related (though some if it certainly will be).</p> <p>In a well-designed relational database, it is usually pretty easy to understand how data is organized and linked through primary and foreign keys. You don’t necessarily need to know the data to query it so long as you know the structure. But an analytical data warehouse is not so easy to comprehend. There are almost never foreign keys, because they introduce complications to the ETL processes that update the data. If you are lucky, there are primary keys on some of the tables. To understand how an analytical data warehouse is organized, you instead need to look at the transformation flow.</p> <h2 id="the-transformation-flow-the-how-and-why">The transformation flow (the how, and why)</h2> <p>If you have worked on enough code-based analytics projects, you have probably learned the value of maintaining a proper data transformation flow. This may show up in how you organize your data folder: you have your “raw” data that is never to be altered, your cleaned/processed data that is an intermediate output, and your final data. You learn the value of using code to transform data - it means all transformations are documented, replicable, and easy to alter. Your notebooks/scripts are a mix of exploration, small cleaning tasks, and big transformations. You may have some functions in there for transformations that need to be performed repeatedly or to encapsulate more complicated code. You are displaying chunks of dataframes in order to visually examine the results and decide what needs to be done next. You are also saving dataframes to CSVs to be used elsewhere.</p> <p>All of these elements show up in the analytical data warehouse as well - they just look a bit different. No matter where the data lives, analytics data work should follow a set of core principles that cement data management best practices in the analytical output.</p> <h3 id="principles-of-data-management">Principles of data management</h3> <ol> <li>Preserved raw data: raw data should be a direct copy of the source as much as possible, and should not be altered (besides being updated with new data)</li> <li>Replicable transformations: all transformations of the raw data should be replicable/documented (the SQL/python code is saved, preferably in a git-tracked central repository) and automated if needed (transformations performed on a schedule in line with data imports)</li> <li>Tested production data: all data considered “production” (ready for analysis) should be able to pass tests of data quality. These tests should be documented and automated if needed, like the transforms</li> </ol> <h3 id="preserved-raw-data">Preserved raw data</h3> <p>When raw data is loaded into a data warehouse via an ETL process, it is loaded in a “source” table. Different teams have different names for this set of tables - “base”, “staging”, etc - but I will use “source” for the sake of simplicity/consistency. These source tables are often isolated or otherwise identifiable in the data warehouse. Some teams have a separate database only for source data, others have a separate schema or set of schemas for source tables, and some may use a naming convention to identify these tables (e.g. a suffix of “_stg”).</p> <p>These source tables reflect the actual source data as closely as possible. This means that they may have the wrong datatypes, need to be cleaned (e.g. trimming whitespace), split out into multiple columns (especially if it is JSON text), the column names may need to be changed, or some other transformation is needed before it can be considered “production ready”. But it is important that these raw source tables remain exactly the same as they were when first loaded into the database - you want to avoid <code class="language-plaintext highlighter-rouge">alter table</code> statements.</p> <p>Sometimes, these raw sources may come from a relational database. In this case, you may want to implement primary keys on the source tables - because if the primary key constraint is violated, that means something went wrong in the ETL process. However, implementing foreign keys is generally not recommended, as it can severely complicate the ETL process. Instead, document these foreign keys (in tests, in text, etc) and use them to inform downstream transformations.</p> <h3 id="replicable-transformations">Replicable transformations</h3> <p>The next step is to transform these source tables into “production” tables. There may be many transformations before producing the table that will be used for analysis, but there should always be at least one. In a data warehouse, a “transform” usually means a SQL select query that is used to create either a table or a view. For the purpose of this post, “production” refers to tables or views in the database that are ready to be used for any downstream analytical purposes (dashboards, analysis, further downstream views, etc), have been through a development and approval process, and have been designated as being “production” based on its name, schema, database, or some other indication.</p> <p>The first transform is the simplist - it should select from the source table and apply any basic cleanup that is needed. Let’s call the result of this first transformation the “base production” table. It should correspond to the source table closely, and should have a 1:1 relationship to the source table (so, no joins). It should also always be materalized as a table (not as a view). The base production tables - and all downstream transforms - should live in a separate schema/database from the source tables, or otherwise be easily distinguished from the source tables.</p> <p>Even if no cleanup or alteration from the base table is needed, this first transformation is still a necessary step. Why? Sometimes there is an error in the ETL process, and the source table may be wiped. If you only have one table (the source table), then you have lost that data, and it will effect any downstream uses on that table. However, if you have a base table (which selects from the source table), and the source table is wiped, then you can stop the transformation before the corresponding base production table is wiped. So, the base production table still has data (even if it is outdated - which is preferable to no data at all). Note: this is only possible if the base production table is materialized as a table, not a view.</p> <p>Even if a source is static (not being updated by an ETL process), you still want to have this source -&gt; base production table transform, and preserve both this separation and the tranformation applied. Why? You may discover at a later point that the base production table needs to be altered in some way - a column should be renamed, a data type needs to be corrected, whitespace needs to be removed, etc. Or, you may discover that a transform you thought you needed is actually incorrect, and you need to alter the transform. Once you alter the source table, there is no going back (besides dropping the table and re-creating it from the source). Whereas if you have a transformation query, you can simply re-run the query to re-create the altered production table. This is only possible if the raw data in the source table remains the same - and if the SQL used to do the transform is saved (preferably in a git-tracked repo).</p> <h4 id="further-transformations">Further transformations</h4> <p>While base production tables should be materialized as a table and should have a 1:1 relationship with its source table, all further downstream transformations can be materialized as views and query multiple tables. The primary reason that a downstream transformation would be materialized as a table instead of a view is for performance reasons - if the query is computationally expensive and takes a long time to run. Otherwise, views are preferred.</p> <p>Note: further downstream transformations should never query a source table - only the production tables/views. These tables/views generally live in the same schemas/databases as the base production tables.</p> <p>There may be many, many downstream transformations for a particular data source, and the structure of these can vary widely. There are many different design philosophies around this area of the data warehouse, but I will leave that for another post. Suffice to say, a large portion of the “thinking” work in a data warehouse is in writing these downstream transforms.</p> <h3 id="tested-production-data">Tested production data</h3> <p>When new data is imported and transformed on a regular cadence, it is important to have Data Unit Tests (DUTs) to automatically check that previous assumptions about the data still hold true. DUTs can be performed on any table/view in the data warehouse - source, production, or downstream views. For example, you might want to check that the source table contains some minimum number of rows before doing the transform to production - and if the test fails, then the transform will not happen. You may want to check that a field you assume is your primary key is actually unique and has no null values. You may want to check that a categorical field only has a limited list of values - and if it fails the test, you only want it to warn you, but not prevent the transformation. The most important tests (the ones you want to stop downstream transforms if they fail) should be performed as early in the transformation flow as possible (ideally, the source tables), while the nice-to-have tests that should only warn you about failures (and not prevent transformations) can be implemented further downstream.</p> <p>DUTs are a way to productionize data testing, but can also be useful during the exploratory phase when first designing transformations. You can test your data through a formal DUT structure, or through a series of queries. For example, you may want to <code class="language-plaintext highlighter-rouge">select distinct</code> on a field to see all possible values, or <code class="language-plaintext highlighter-rouge">select count(distinct __)</code> on a field compared to <code class="language-plaintext highlighter-rouge">select count(__)</code> to see whether there are duplicates. Based on this exploratory data testing, you may wish to revise your transform. And while it is important to document and preserve any SQL transform queries so they can be re-run, it is less essential to preserve these ad-hoc data tests - their primary purpose for static data is to inform your SQL query for transforming source data into the production table. However, if the data source may be updated, then formalizing them into DUTs is a good idea.</p> <h2 id="the-implications-or-why-dbt">The implications (or, why dbt)</h2> <p>Imagine you are an analyst, staring at this data warehouse and all of its many tables, trying to figure out which tables are going to have the answer to your question. Or, imagine you are an engineer, informed that something has gone wrong in a dashboard and you need to figure out what (and where) in the data flow something went wrong. What is the one thing that you are really going to want (need!) in order to do your job effectively and efficiently?</p> <p>Documentation.</p> <p>You are going to want documentation for your tables, the columns in those tables, the relationships and dependencies between those tables, the tests performed on those tables, and any external dependencies (dashboards relying on those tables, ETL processes powering the source tables). But documentation has a dirty secret - (almost) nobody wants to or has time to write documentation. Unless you make documentation fast and easy to produce (or you pay for it by making documentation someone’s job or primary responsiblity), it isn’t going to happen (at least not consistently).</p> <p>dbt may be advertised as a tool to transform (and test!) your data, but its real superpower is in the documentation that is produced as a side effect of how those transformations and test are implemented. If you do not currently have an infrastructure to support transformations and data unit tests within your data warehouse, then implementing dbt is a no brainer because it is the easiest out-of-the-box way to accomplish those fundamental tasks. But even if you do have a (likely custom/house built) infrastructure to do those transforms &amp; tests, making the switch to dbt is worth it, because of the documentation that is produced (and the culture of documentation that is encouraged/supported). Once you see a DAG (directed acyclic graph of table nodes) that visualizes every step from source to dashboard, there is no going back.</p> <p>Why? Because given enough time, your team will accumulate enough data sources (and the people most knowledgable about those sources will leave at some point), and enough complicated transforms and table structures, that working within the data warehouse will become a snakes’ nest of tangled dependencies, table rot, and dangerous assumptions. And if you can’t rely on your data warehouse, then your stakeholders can’t rely on your analytical outputs. Documentation is not just a nice to have - it is essential. Document your data, your transforms, your tests, your analytical outputs, everything - and then put that documentation in one easily accessible and searchable place. dbt makes this easy, and that is the real reason it has become the darling of the data community - and the reason I’ve wanted to implement it for our team ever since I learned enough about the tool and our current data infrastructure.</p> <p>Which bring us back to our principles of good data management for analytics. There was, in fact, one principle missing from the list, so let’s round it out.</p> <h3 id="principles-of-data-management-final-version">Principles of data management (final version)</h3> <ol> <li>Preserved raw data: raw data should be a direct copy of the source as much as possible, and should not be altered (besides being updated with new data)</li> <li>Replicable transformations: all transformations of the raw data should be replicable/documented (the SQL/python code is saved, preferably in a git-tracked central repository) and automated if needed (transformations performed on a schedule in line with data imports)</li> <li>Tested production data: all data considered “production” (ready for analysis) should be able to pass tests of data quality. These tests should be documented and automated if needed, like the transforms</li> <li>Documented data: the data (each column, each table) should be documented, as well as the data lineage (how the final data was produced from the raw source data)</li> </ol> <h2 id="bonus-the-development-cycle">Bonus: the development cycle</h2> <p>Now that we’ve brought dbt into the picture, we can talk about one final piece of the (modern) analytical data warehouse: the development cycle. In the principles above (and the blocks of text further above), note the use of the term “production”. The presence of “production” data implies the presence of data that is not “production” - something distinct from the raw source data already described. This missing something is the “development” version of the data - basically, the data/transform that has not yet been finalized.</p> <p>Tables that are still in development should be distinguished from production tables in the data warehouse. Dev tables may live in a separate database or schema, or be indicated through a table naming convention. Teams may also have less formal (read: social) ways of indicating that a table is in development, though this introduces opportunity for error. A table in development is not finalized - column names, order, and content may change, the table name itself may change, and the logic has not yet been reviewed and approved by the team. Essentially, it’s very risky (and certainly inefficient) to build downstream dependencies on dev tables, though there will always be cases where speedy delivery is a necessity and this rule has to be broken.</p> <p>One great feature of dbt is that this development phase of tables/views (dbt refers to them all as “models”) is built into how dbt works through the use of profiles. It’s very easy to designate a specific schema or database as a dev schema/database, and to build models agnostic of the dev/prod schema. It’s also why git is a core component of any dbt project - the “main” branch should have models in “production”, while feature branches should have models in “development”. You have to be familiar with the use of version control in a software development cycle in order to use dbt effectively, and the fact that dbt brings these software engineering practices into the modern analytical workflow is considered a major selling point of dbt.</p> <h2 id="in-conclusion">In conclusion</h2> <p>Unless you are already part of a data team that uses a data warehouse, it can be hard to understand what an analytical data warehouse is, how to use it properly, and why it is such an important part of working on a data team. If you’re in school or otherwise studying for a career in data, knowing how to work within a data warehouse will give you a big leg up over other entry level candidates. My hope is that this blog post can (1) convince you that you already know the most important pieces of the puzzle, (2) fill in some of the missing pieces of the puzzle, and (3) help you synthesize and put all of the pieces together to form a cohesive picture.</p> <p>In future posts I would like to continue to flesh out how to work in a data warehouse, but this concludes the introduction. If you have any questions or future topics you’d like me to focus on, let me know!</p>]]></content><author><name></name></author><category term="data"/><category term="essays"/><summary type="html"><![CDATA[what? how? why? data warehousing for analytics, explained (mostly)]]></summary></entry><entry><title type="html">Life Since Grad School</title><link href="https://jennajordan.me/blog/life-since-grad-school" rel="alternate" type="text/html" title="Life Since Grad School"/><published>2023-07-30T00:00:00+00:00</published><updated>2023-07-30T00:00:00+00:00</updated><id>https://jennajordan.me/blog/life-since-grad-school</id><content type="html" xml:base="https://jennajordan.me/blog/life-since-grad-school"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_lifesincegradschool-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_lifesincegradschool-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_lifesincegradschool-1400.webp"/> <img src="/assets/img/header_lifesincegradschool.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This website, and therefore this blog, has fallen by the wayside since I graduated from library school with my MSLIS in the spring of 2020… in the midst of a global pandemic. I wasn’t really sure what to do with the site, beyond letting it exist and adding some projects here and there, but I recently decided that it was time for a revamp (and rebrand/redesign). I wanted my personal website to say “working professional”, not “overly enthusiastic grad student” - though I am keeping all the old posts so the “overly enthusiastic grad student” is not going away. But it has been 3 years (wow!) since I was in grad school, and I have managed to get some professional experience under my belt - though with the pandemic raging as I entered the work force I had a bit of a rocky start.</p> <h2 id="2020">2020</h2> <p>My last semester of grad school was not the best. There was before spring break, and there was after spring break. Before spring break, everything was continuing on as usual, with some masks starting to appear. After spring break, everything shut down. All classes and work went remote. There was no graduation ceremony. I had no idea that the last time I saw my professors, classmates, coworkers, and firends, it would be the last time I saw them ever. While in undergrad I could not care less about the graduation ceremony, for grad school - a place where I really found my academic home - I was looking forward to the chance to see everyone one last time and say goodbye. Instead, there was just endless isolation.</p> <p>After graduating into a pandemic with no job offer yet in hand, I did what everyone seemed to be doing - I moved back home. My mom was living alone, I couldn’t afford to pay rent without a job, and after the isolation of the previous months I was looking forward to forming a little family bubble. I put most of my stuff in storage, sure that I would be moving soon to wherever I found a job. And sure enough, within a few months of graduating I got a job offer… with one big catch. I would need to get my security clearance. I was so excited to get a job offer, I ignored the perfectly reasonable advice every entry level professional is given (and ignores): don’t accept the first offer you get. I was told that the process would take about 6-12 months, and I hoped that my previous security clearance (for my internship with the State Department in undergrad) would help speed things up. In the meantime, I could find some short-term contracting gigs to start advancing my data skills and developing experience. So that’s exactly what I did.</p> <p>Pretty much as soon as I had some income coming in, I kept a promise that I had made to myself years ago at the end of undergrad (when I knew I was going to be doing a lot of traveling). I finally adopted a puppy. I adopted my sweet girl from Saving Grace, a rescue in Raleigh. At the rescue, the name on her tag was “Genuine Risk”. I named her <a href="/bella/">Bella</a>, after the loyal pony Bela in Wheel of Time. She was about 14 weeks old, the last of her litter, and shy as could be, but after 5 minutes of me sitting cross legged in her pen she crawled into my lap and fell asleep. That’s when I knew - and my mom really knew - that she would be coming home with us.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bella_blanket-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bella_blanket-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bella_blanket-1400.webp"/> <img src="/assets/img/bella_blanket.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I also took up knitting (I know, such a classic pandemic hobby). The very first thing I knit was a blanket for Bella. My reasoning was: (1) she won’t care what it looks like, so it’s fine if it turns out awful and ugly, and (2) it’s getting cold, she’d probably appreciate a blanket that smells like me. (Actually I lie, the first 4 blanket squares got combined into a blanket for Molly, our much smaller and much older family dog). Sadly, that blanket no longer exists - it got slowly torn apart and consumed because… I gave it to a puppy. C’est la vie. But while the blanket slowly got smaller, I kept on <a href="/knitting/">knitting</a>, and eventually started knitting shawls as wedding presents. (6 shawls and 5 weddings later, I’m finally now starting on a shawl for myself)</p> <h2 id="2021">2021</h2> <p>A year came… and went. Still no word on the security clearance. I had done all of the steps (including going in person for the drug test, polygraph, etc) and I got no updates, no estimated timeline, nothing. I knew that I wanted to kick off my career properly - in a full-time position doing data work as a part of a larger team focused on public service - and the contracting gigs had only been meant as a temporary stopgap until I could join such a team. I had been applying for data librarian jobs and even got to the final round of interviews a couple times, but no dice (actually, <a href="/projects/carpentries-dataviz-workshop">my Carpentries lesson</a> was a side product of one of those interviews!).</p> <p>Finally, in late fall of 2021, I hit gold. A mutual on twitter pointed me towards a posting for a data engineer at the City of Boston’s Analytics Team. It was perfect - data work, in public service, and a genuine way to really start my career. I applied, made a <a href="/projects/streamlit-apps">streamlit app</a> for extra credit, and by the end of the year I had accepted a job offer.</p> <p>I kid you not, a week after accepting that job offer, my security clearance came through. But I knew that the data engineer position in Boston was a better fit for me, so I stuck with it and moved to Boston in January of 2022. Almost a year and a half of waiting on that security clearance… well, sometimes, timing really is everything.</p> <h2 id="2022">2022</h2> <p>My first year as a Data Engineer on the Analytics Team was a true learning experience - exactly what I was hoping for from my first job to kick off this new career. In grad school, I had learned SQL and how to design databases in 3rd Normal Form. At my job, I learned about analytical data warehouses and how SQL was used in production code, and I leveled up my SQL skills substantially (can you believe we learned about subqueries but not CTEs?). In grad school, I had learned about automated workflows as related to improving data quality. At my job, I learned about data orchestration (hello, YAML files and YAQL) and the pros and cons of different workflow design paradigms. In grad school, I mostly wrote my own python code from scratch to finish. At my job, I learned how to work with a team of engineers and contribute to an existing codebase. I learned how to work and contribute within a larger analytics team - rather than doing every part of the analytics flow myself.</p> <p>I also made sure to focus on my professional development in other ways. In March I took CoRise’s course <a href="https://corise.com/course/analytics-engineering-with-dbt">“Analytics Engineering with dbt”</a>, taught by Emily Hawkins. In August I took GovEx’s course on Data Governance. Both were valuable and what I learned in them I could immediately apply in my work.</p> <p>During my first year, I focused on learning how the Analytics Team, and the Engineering Team especially, worked, and how I could best contribute within the existing framework. I learned so much from my coworkers, and I found the experience of having fellow engineers that I could lean on and collaborate with invaluable. But in addition to learning how to contribute to what was already there, I was also starting to identify new ideas that I wanted to add to the mix. Primarily: <a href="https://docs.getdbt.com/docs/introduction">dbt</a>.</p> <h2 id="2023">2023</h2> <p>For my second year on the Analytics Team, I wanted to make a substantial impact to improve how the engineering team worked. This involved a lot of thinking and iterating through designs and strategies before I brought my proposals to the team. I wanted to make sure that when it came to the actual implementation of these plans, that it would go as quickly and smoothly as possible. It also meant communicating with my team and building interest and agreement in my proposals. I had been talking about dbt almost since I joined the team, but in a “this is a cool tool” way, not in a “let’s do it” way. If I was going to ask everyone to learn and use this new tool, I wanted to make sure they believed it was worth it too.</p> <p>I don’t want to go into too much detail about what dbt is and how we are implementing it - I’ll save that for a future blog post. But suffice to say that the project got started early in 2023, and after 6 months we are finally in a sprint of building models and adding to the project, with almost all engineers onboarded and starting to onboard analysts. I’m very excited to see what further progress we can make for the rest of the year - and I’m especially excited to be able to share this experience of implementing dbt for a city analytics team in <a href="https://coalesce.getdbt.com/agenda/from-coast-to-coast-implementing-dbt-in-the-public-sector">a talk at this year’s Coalesce</a> (dbt Lab’s annual conference).</p> <p>Besides the dbt project, this year I’ve also been working on larger and more complex projects that have involved a lot of data architecture/design work. Without going into too much detail, the city has a lot of legacy systems that don’t talk to each other, when city workers really need for information to be passed between those systems in order to do their work well and efficiently. I have been involved in a couple projects this year that have been focused on how to integrate these systems (or at least have all of the data in one place and able to be joined together), particularly for housing and development work. I love building ETL pipelines, but I know a project will be special when I get to do system design and data architecture work before building the pipeline. These projects are always bigger, longer, and trickier, involve working with more stakeholders and teams, and are just so rewarding because you can see the difference your contribution is making and you can build relationships outside of your team.</p> <p>Another high point for this year on the team is that I’ve had the opportunity to start teaching Carpentries workshops again. I taught a <a href="https://carpentries-incubator.github.io/git-novice-branch-pr/">workshop on git</a> because the analysts on the team have been writing more code and wanted to preserve and collaborate on their code in a GitHub repository, and I taught some <a href="https://swcarpentry.github.io/python-novice-gapminder/07-reading-tabular.html">sessions on python and pandas</a> as a part of a Data Culture pilot program focused on python. Later this year I’m planning on teaching a [SQL workshop](https://swcarpentry.github.io/sql-novice-survey/ and a session on querying APIs with python. Teaching computational skills is something I really enjoy, and it was the one thing I was really missing in this job last year.</p> <p>Finally, if you think that working on a city analytics team sounds interesting, I will plug the fact that we are <a href="https://www.boston.gov/news/join-city-bostons-department-innovation-and-technology">currently hiring for several positions</a> - if you live in Boston or want to move to Boston, <a href="https://www.boston.gov/career-center">apply</a>!</p>]]></content><author><name></name></author><category term="life-updates"/><category term="musings"/><summary type="html"><![CDATA[from grad student to working data professional]]></summary></entry><entry><title type="html">Carpentries Workshops</title><link href="https://jennajordan.me/blog/carpentries-workshops" rel="alternate" type="text/html" title="Carpentries Workshops"/><published>2021-09-28T00:00:00+00:00</published><updated>2021-09-28T00:00:00+00:00</updated><id>https://jennajordan.me/blog/carpentries-workshops</id><content type="html" xml:base="https://jennajordan.me/blog/carpentries-workshops"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_carpentries-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_carpentries-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_carpentries-1400.webp"/> <img src="/assets/img/header_carpentries.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My Carpentries teaching journey started out slow - I taught a couple of times that first year, and stuck to the lesson I was most comfortable with (SQL &amp; Databases). However, at the start of the summer in 2021 my work put me in charge (or I volunteered to be in charge) of a group of undergraduate interns who wanted to learn how to use computational methods for open source intelligence analysis. So I put together a curriculum of Carpentries lessons to take my interns from zero-assumed knowledge to the ability to complete a computational analysis in Python. I’m teaching the same curriculum in the Fall (slightly expanded and refined) to another group of undergraduate interns. This allowed me to gain experience in teaching more of the core Carpentries lessons… and also inspired me to develop <a href="/projects/carpentries-dataviz-workshop">my own lesson focused on interactive data visualizations</a>!</p> <hr/> <h1 id="a-semester-long-curriculum-of-carpentries-workshops">A semester-long curriculum of Carpentries workshops</h1> <p>Each week I teach for 2 hours, and then learners can practice what they’ve learned and give feedback on the workshop in an assignment (delivered via Google Forms). I’ve included the curriculum schedule below:</p> <h2 id="week-0-self-assessment">Week 0: Self Assessment</h2> <p><strong>Goals:</strong></p> <ul> <li>Learners assess their current skill level</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/QsB6fFpn9VWiVjRz9">Computational Skills Self Assessment form</a></li> </ul> <h2 id="week-1-github">Week 1: GitHub</h2> <p><strong>Goals:</strong></p> <ul> <li>Instructor &amp; learners introduce themselves</li> <li>Overview of the curriculum</li> <li>All learners have a GitHub account and know the basics of creating and modifying a personal repository on GitHub (with GitHub Desktop).</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/SPUiKxH5gtc1q5o47">Assignment 1: GitHub</a></li> </ul> <p><strong>Resources:</strong></p> <ul> <li><a href="https://github.com/elliewix/github-training-brain-dumps/blob/master/github_directions.md">Getting Started with GitHub - Training by Elizabeth Wickes</a></li> <li><a href="https://docs.github.com/en/github/getting-started-with-github/signing-up-for-github">GitHub’s docs on creating a new account</a></li> <li><a href="https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/overview/getting-started-with-github-desktop#introduction">GitHub’s docs on GitHub Desktop</a></li> <li><a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/creating-a-new-repository">GitHub’s docs on creating a new repo</a></li> <li><a href="https://www.makeareadme.com/">Writing a good README</a></li> <li><a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github">GitHub’s docs on cloning a repo</a></li> </ul> <h2 id="week-2-the-shell-part-1">Week 2: The Shell (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/shell-novice/">Shell workshop lesson plan from Software Carpentries</a> (Lessons 1-5)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>become comfortable navigating your computer via the command line</li> <li>learn the basic set of Bash commands (pwd, ls, man, cd, mkdir, nano, touch, mv, cp, rm, wc, cat, less, sort, head, tail, echo)</li> <li>learn how to chain commands using the pipe, redirect, and append</li> <li>learn how to use loops</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/CqZ1zkhdAmdBju8c9">Assignment 2: The Shell</a></li> <li><a href="https://docs.google.com/document/d/1uvkMieUHK2JM4XPx8mAeEmqvfdNiDSAQc2mUHshrAvE/edit?usp=sharing">Answer Key for Assignment 2</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="http://linuxcommand.org/lc3_learning_the_shell.php#contents">The Linux Command Line - web tutorials</a></li> <li><a href="https://phoenixnap.dl.sourceforge.net/project/linuxcommand/TLCL/19.01/TLCL-19.01.pdf">The Linux Command Line - full free textbook</a></li> <li><a href="https://explainshell.com/">Explain Shell</a></li> </ul> <h2 id="week-3-the-shell-part-2">Week 3: The Shell (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/shell-novice/">Shell workshop lesson plan from Software Carpentries</a> (Lessons 6 &amp; 7)</li> <li><a href="http://carpentries-incubator.github.io/shell-extras/">Shell Extras workshop lesson plan from Software Carpentries</a> (Lesson 2)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to save a series of commands as a shell script</li> <li>Learn how to search files from the command line <ul> <li>grep, find</li> </ul> </li> <li>Learn how to generate and use your own SSH key pair</li> <li>Learn how to access a computer remotely <ul> <li>ssh, scp</li> </ul> </li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/hVq8VgEBWPyVXC4S8">Assignment 3: The Shell cont</a></li> <li><a href="https://docs.google.com/document/d/1B0qwQVUyNudAQ7jGPIAIONmU32FBedkNo4wkyPEsX74/edit?usp=sharing">Answer Key for Assignment 3</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://linuxize.com/post/using-the-ssh-config-file/">Setting up and using the SSH config file</a></li> </ul> <h2 id="week-4-git">Week 4: Git</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/git-novice/">Git workshop lesson plan from Software Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to use git from the command line</li> <li>Set up git on the remote sandbox server <ul> <li>git config</li> </ul> </li> <li>Learn how to create repositories <ul> <li>git init</li> </ul> </li> <li>Learn how to track changes <ul> <li>git add, git commit, git status, git log, git diff</li> </ul> </li> <li>Learn how to use the history <ul> <li>HEAD, git show, git checkout, git revert</li> </ul> </li> <li>Learn how to not track things <ul> <li>.gitignore</li> </ul> </li> <li>Set up a remote repository <ul> <li>git remote, git push, git pull</li> </ul> </li> <li>Learn how to collaborate &amp; resolve conflicts <ul> <li>git clone</li> </ul> </li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/XyEa9RmgznnRuLPx7">Assignment 4: Git</a></li> <li><a href="https://docs.google.com/document/d/1VUVdRvK9YPIEqiVrWX_3pL1DeSr9e-brajbPsx0XgeY/edit?usp=sharing">Answer Key for Assignment 4</a></li> </ul> <h2 id="week-5-python-part-1">Week 5: Python (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 1-7)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Everyone has Anaconda (and thus Python &amp; Jupyter Lab) installed and is familiar with how to work within Jupyter Lab and use a Jupyter Notebook</li> <li>Learn about data types and type conversion in python</li> <li>Learn how to use built in functions</li> <li>Learn how to get help, read the built-in docs, and read errors</li> <li>Learn how to import and use libraries</li> <li>Learn how to read data into a dataframe and interact with the dataframe (pandas)</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/oJYrV2aPVtnVPHJ56">Assignment 5: Python</a></li> <li><a href="https://docs.google.com/document/d/1FGN-ZHiPPu_dW73kC7o3ks9kWwXP8VtvXcPXe--2B2E/edit?usp=sharing">Answer Key for Assignment 5</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://carpentries-incubator.github.io/introduction-to-conda-for-data-scientists/">Intro to Anaconda from Software Carpentries</a></li> </ul> <h2 id="week-6-python-part-2">Week 6: Python (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 8-12)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to select data from a dataframe (pandas)</li> <li>Learn how to plot data (matplotlib.pyplot)</li> <li>Learn how to work with lists</li> <li>Learn about for loops and the accumulator pattern</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/XPxxRaBubMaDaptD9">Assignment 6: Python &amp; pandas</a></li> <li><a href="https://github.com/NCRI-io/nclabs_pandaspractice">GitHub repo for the pandas practice</a></li> </ul> <h2 id="week-7-python-part-3--requests--rest-apis">Week 7: Python (part 3) + requests &amp; REST APIs</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/python-novice-gapminder/">Plotting &amp; Programming in Python workshop lesson plan from Software Carpentries</a> (Lessons 13-18)</li> <li><a href="https://cac-staff.github.io/summer-school-2016-Python/11-dicts.html">Storing Information with Dictionaries from Software Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn about conditionals (if, else, elif)</li> <li>Looping over files (glob)</li> <li>Learn how to write functions</li> <li>Learn about the dictionary data type</li> <li>Learn how use python’s requests library to query a REST API</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/GGDeBWAmg5Ck7WRA7">Assignment 7: Python</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://realpython.com/python-api/">Python &amp; APIs: A Winning Combo for Reading Public Data from Real Python</a></li> <li><a href="https://www.dataquest.io/blog/python-api-tutorial/">Python API Tutorial: Getting Started with APIs from DataQuest</a></li> </ul> <h2 id="week-8-interactive-data-visualizations">Week 8: Interactive Data Visualizations</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="https://carpentries-incubator.github.io/python-interactive-data-visualizations/">Interactive Data Visualizations in Python workshop lesson plan</a></li> </ul> <p><strong>Goals</strong></p> <ul> <li>Learn how to create a new conda environment</li> <li>Learn how to wrangle data into a tidy shape</li> <li>Learn how to create line plots with plotly</li> <li>Learn how to create and run a streamlit app</li> <li>Learn how to add widgets to the streamlit app</li> </ul> <h2 id="week-9-databases--sql-part-1">Week 9: Databases &amp; SQL (part 1)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/sql-novice-survey/">SQL workshop lesson plan from Software Carpentries</a> (Lessons 1-5)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to select data from a table</li> <li>Learn how to sort results and remove duplicates</li> <li>Learn how to filter, or select subsets of data, based on boolean conditions</li> <li>Learn how to calculate new values to be returned in the results</li> <li>Learn about missing data, and incorporating NULL into queries</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/wWQFYSREEXLkQQMA6">Assignment 8: SQL</a></li> </ul> <h2 id="week-10-databases--sql-part-2">Week 10: Databases &amp; SQL (part 2)</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="http://swcarpentry.github.io/sql-novice-survey/">SQL workshop lesson plan from Software Carpentries</a> (Lessons 6-10)</li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to aggregate data in queries (calculate sums, averages, etc)</li> <li>Learn how to combine tables in a query</li> <li>Learn about data hygiene, and principles of database design like primary &amp; foreign key constraints</li> <li>Learn how to create, modify, and delete data</li> <li>Learn how to use SQLite databases within a python script</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li><a href="https://forms.gle/hdgDb7KsQU2DhVCw9">Assignment 9: SQL</a></li> <li>Review <a href="https://swcarpentry.github.io/sql-novice-survey/10-prog/index.html">Lesson 10</a> (Programming with Databases - Python) on your own</li> </ul> <h2 id="week-11-regular-expressions">Week 11: Regular Expressions</h2> <p><strong>Lesson Plan:</strong></p> <ul> <li><a href="https://librarycarpentry.org/lc-data-intro/">Regular Expressions workshop lesson plan from Library Carpentries</a></li> </ul> <p><strong>Goals:</strong></p> <ul> <li>Learn how to use Regular Expressions (regex)</li> </ul> <p><strong>Assignment:</strong></p> <ul> <li>Practice using regular expressions by completing some <a href="https://regexcrossword.com/">Regex Crosswords</a></li> </ul> <p><strong>Further Resources:</strong></p> <ul> <li><a href="https://youtube.com/playlist?list=PL7C1EB31127AB8A0B">Regular Expressions workshop videos from Software Carpentries</a></li> </ul>]]></content><author><name></name></author><category term="software-carpentries"/><summary type="html"><![CDATA[An overview of the workshops I've taught, including a semester-long syllabus.]]></summary></entry><entry><title type="html">About The Carpentries</title><link href="https://jennajordan.me/blog/about-the-carpentries" rel="alternate" type="text/html" title="About The Carpentries"/><published>2021-09-27T00:00:00+00:00</published><updated>2021-09-27T00:00:00+00:00</updated><id>https://jennajordan.me/blog/about-the-carpentries</id><content type="html" xml:base="https://jennajordan.me/blog/about-the-carpentries"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/header_carpentries-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/header_carpentries-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/header_carpentries-1400.webp"/> <img src="/assets/img/header_carpentries.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If you’ve never heard of <a href="https://carpentries.org">The Carpentries</a> before, you may be confused - have I gotten into woodworking?! (No… but I have gotten into knitting - the real kind, not the RMarkdown kind).</p> <p>From the horse’s mouth, here’s what The Carpentries is all about:</p> <blockquote> <p><strong>Vision:</strong> Our vision is to be the leading inclusive community teaching data and coding skills.</p> <p><strong>Mission:</strong> The Carpentries builds global capacity in essential data and computational skills for conducting efficient, open, and reproducible research. We train and foster an active, inclusive, diverse community of learners and instructors that promotes and models the importance of software and data in research. We collaboratively develop openly-available lessons and deliver these lessons using evidence-based teaching practices. We focus on people conducting and supporting research.</p> <p>-– <a href="https://carpentries.org/about/">The Carpentries - About Us</a></p> </blockquote> <h2 id="how-i-got-involved-with-the-carpentries">How I got involved with The Carpentries</h2> <p>My first semester of my MSLIS, I took an Intro to Python class with Elizabeth Wickes. After completing the class, Elizabeth (an Instructor Trainer with The Carpentries and an elected member of the Executive Council since 2018) encouraged me to take the Carpentries instructor training class and get involved with the local UIUC chapter of The Carpentries… but it took me another year before I felt comfortable enough with my technical skills to feel like I could teach them. In December 2019, I finally took the instructor training class with Elizabeth Wickes &amp; Neal Davis. It was a 2-day class that prepared new instructors to teach Carpentries workshop - with a special emphasis on how to teach while live-coding. As with all things Carpentries, the <a href="https://carpentries.github.io/instructor-training/">Instructor Training course</a> is also freely available.</p> <p>I had the chance to be a helper for one in-person workshop in early 2020 before… the pandemic happened. Then, everything went remote. I was a helper for another workshop series online, before I finally felt comfortable enough to teach a workshop for myself in Summer of 2020 - the SQL lesson, which remains my favorite lesson to teach!</p> <h2 id="why-i-love-the-carpentries">Why I love The Carpentries</h2> <p>The Carpentries is something special. For one, all of the workshop lessons are freely available - so anyone can use the lesson plans to teach or learn for themselves. The lessons live in GitHub repos, so they can be collaboratively developed. Anyone can develop their own Carpentries lesson using the provided template and submit the lesson to the Carpentries Incubator, so there are a ton of lessons out there beyond the core set. All of the lessons follow a similar pedagogical philosophy live coding throughout the workshop, getting learners to a place of being able to <em>use</em> their new skills as quickly as possible, and being as inclusive and supportive as possible. <a href="https://carpentries.org/blog/2019/07/alex-ttt-reflection/">This reflection blog post</a> details some of the pedagogy that is the foundation of all Carpentries lessons.</p> <p>The Carpentries is also all about community. Some instructors are based at a university and have the support of their local chapter, but even if instructors don’t have a local chapter, they can be supported by the global online community. There are community discussions via Zoom and a Slack workspace so instructors can stay connected and learn from each other, and many instructors are active on social media, like Twitter. There’s also <a href="https://carpentrycon.org">CarpentryCon</a>!</p> <p>So if you’re interested in getting involved with The Carpentries, <a href="https://carpentries.org/volunteer/">go for it!</a> I cannot emphasize enough how much I have learned and benefited from being a part of The Carpentries community.</p>]]></content><author><name></name></author><category term="software-carpentries"/><summary type="html"><![CDATA[All about The Carpentries, how I got involved, and what I love about the organization.]]></summary></entry></feed>